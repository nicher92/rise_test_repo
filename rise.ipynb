{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac54ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import string\n",
    "from transformers import (LukeTokenizer, LukeModel, LukeForEntityPairClassification, \n",
    "                          AutoModel, AutoTokenizer, LukeForEntitySpanClassification,\n",
    "                          BertForTokenClassification, get_linear_schedule_with_warmup, AdamW, get_scheduler)\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import Counter\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "\n",
    "\n",
    "from ray import tune, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1231bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset_name = \"Babelscape/multinerd\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e333d3d",
   "metadata": {},
   "source": [
    "# Load data & Remove non-english items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa5b986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/NicHer/.cache/huggingface/datasets/Babelscape___json/Babelscape--multinerd-f822e910a4f604c0/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95721573a8ab4f33b008ecc580abbdc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\NicHer\\.cache\\huggingface\\datasets\\Babelscape___json\\Babelscape--multinerd-f822e910a4f604c0\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-eb9ab4cbc9b9233f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\NicHer\\.cache\\huggingface\\datasets\\Babelscape___json\\Babelscape--multinerd-f822e910a4f604c0\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-eed54e3e6216fb0e.arrow\n",
      "Loading cached processed dataset at C:\\Users\\NicHer\\.cache\\huggingface\\datasets\\Babelscape___json\\Babelscape--multinerd-f822e910a4f604c0\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-fe66576485de1b0f.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Loads data, only keeps english, removes language column\n",
    "dataset_eng = load_dataset(dataset_name).filter(lambda x: x[\"lang\"] == \"en\").remove_columns(\"lang\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53467bb9",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996f9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Uneven class distribution\n",
    "#nercounts = dict(Counter([item for sublist in dataset_eng[\"train\"][\"ner_tags\"] for item in sublist]))\n",
    "#nercounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a50a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#most texts are short, can get away with low max token length\n",
    "#plt.hist([len(x) for x in dataset_eng[\"train\"][\"tokens\"]], bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf019c4",
   "metadata": {},
   "source": [
    "## Itos & Stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24ee0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping from integer labels to strings (from HF dataset repo) and vice versa\n",
    "\n",
    "stoi = {\n",
    "    \"O\": 0,\n",
    "    \"B-PER\": 1,\n",
    "    \"I-PER\": 2,\n",
    "    \"B-ORG\": 3,\n",
    "    \"I-ORG\": 4,\n",
    "    \"B-LOC\": 5,\n",
    "    \"I-LOC\": 6,\n",
    "    \"B-ANIM\": 7,\n",
    "    \"I-ANIM\": 8,\n",
    "    \"B-BIO\": 9,\n",
    "    \"I-BIO\": 10,\n",
    "    \"B-CEL\": 11,\n",
    "    \"I-CEL\": 12,\n",
    "    \"B-DIS\": 13,\n",
    "    \"I-DIS\": 14,\n",
    "    \"B-EVE\": 15,\n",
    "    \"I-EVE\": 16,\n",
    "    \"B-FOOD\": 17,\n",
    "    \"I-FOOD\": 18,\n",
    "    \"B-INST\": 19,\n",
    "    \"I-INST\": 20,\n",
    "    \"B-MEDIA\": 21,\n",
    "    \"I-MEDIA\": 22,\n",
    "    \"B-MYTH\": 23,\n",
    "    \"I-MYTH\": 24,\n",
    "    \"B-PLANT\": 25,\n",
    "    \"I-PLANT\": 26,\n",
    "    \"B-TIME\": 27,\n",
    "    \"I-TIME\": 28,\n",
    "    \"B-VEHI\": 29,\n",
    "    \"I-VEHI\": 30,\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "itos = {value:key for key,value in stoi.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007952fa",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fb242a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic bert model - cased since the text is cased and it probably helps label names like \"Jessica Alba\"\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "\n",
    "# Tokenizer is to be initialized here, used for processing\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37470f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Modularize this, dont hardocode a bunch of stuff\n",
    "\n",
    "\n",
    "#Taken from HF, to align the ner labels with tokenized words\n",
    "def align_labels_with_tokens(labels, word_ids, max_length):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    \n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # New word\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]   #-100 to not take into account during loss function\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)                                #-100 for special tokens\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels[:max_length]    ##truncates labels match\n",
    "\n",
    "\n",
    "\n",
    "def to_tensor_dataset(data, labels):\n",
    "    # Effectively zips the data and labels\n",
    "    \n",
    "    inp_ids = data[\"input_ids\"]\n",
    "    atmask = data[\"attention_mask\"]\n",
    "    return TensorDataset(inp_ids, atmask, labels)\n",
    "\n",
    "\n",
    "\n",
    "def label_fix(labels):\n",
    "    # Input is a nested list of labels\n",
    "    \n",
    "    return [[x if x in [1,2,3,4,5,6,7,8,13,14] else 0 for x in sublist] for sublist in labels]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(dataset, splitname, all_labels, max_length=60):\n",
    "    ## dataset, split, all_labels or subset => Returns torch tensors\n",
    "    \n",
    "    splitname = splitname\n",
    "    sents = dataset[splitname][\"tokens\"]\n",
    "    labels = dataset[splitname][\"ner_tags\"]\n",
    "    \n",
    "    # Change labels if all_labels = False\n",
    "    # A bit slow, would be faster with tensor operations\n",
    "    if not all_labels:\n",
    "        labels = label_fix(labels) \n",
    "    \n",
    "    assert len(sents) == len(labels)\n",
    "    \n",
    "    # Tokenize and align labels, currently pad everything to the same length\n",
    "    tokenized_sents = tokenizer(sents, is_split_into_words=True, add_special_tokens=True, padding=\"max_length\", truncation=True, max_length=max_length,  return_tensors=\"pt\")\n",
    "    \n",
    "    # Stack to turn list of torch tensors into one tensor\n",
    "    aligned_labels = torch.stack([torch.tensor(align_labels_with_tokens(labels[i], tokenized_sents[i].word_ids, max_length)) for i in range(len(labels))])\n",
    "    \n",
    "    dataset = to_tensor_dataset(tokenized_sents, aligned_labels)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_labels = True\n",
    "\n",
    "training_dataset = prepare_data(dataset_eng, \"train\", all_labels)\n",
    "evaluation_dataset = prepare_data(dataset_eng, \"validation\", all_labels)\n",
    "testing_dataset = prepare_data(dataset_eng, \"test\", all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09eebb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262560"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc51fa0",
   "metadata": {},
   "source": [
    "\n",
    "for i in range(0,3):\n",
    "    print(tokenizer.tokenize(\" \".join(dataset_eng[\"train\"][\"tokens\"][i])))\n",
    "    print(dataset_eng[\"train\"][\"tokens\"][i])\n",
    "    print(dataset_eng[\"train\"][\"ner_tags\"][i])\n",
    "    print([itos[int(x)] for x in train_labels[i] if x != -100])\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d505471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If not all labels, num labels = 11, 2*5 for B and I, + 1 for O\n",
    "# Else its the length of the label dataset\n",
    "\n",
    "if not all_labels:\n",
    "    num_labels = 11\n",
    "else:\n",
    "    num_labels = len(itos)\n",
    "    \n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "219d53fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cuda\"\n",
    "#batch_size = 256\n",
    "\n",
    "\n",
    "#test_loader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=False) #False for reproducibility for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24f35e5",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9aa605b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The model probably doesnt need to be more complicated than this, the context is usually very short\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", \n",
    "                                                   num_labels=num_labels, \n",
    "                                                   vocab_size=tokenizer.vocab_size, \n",
    "                                                   ignore_mismatched_sizes=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d4c8751",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters ###\n",
    "\n",
    "#epochs = 1\n",
    "#lr = 3e-5,\n",
    "\n",
    "\n",
    "#num_batches = train_data[\"input_ids\"].shape[0] // batch_size\n",
    "#total_steps = num_batches * epochs\n",
    "\n",
    "#num_warmup_steps = total_steps // 10\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "\n",
    "### Hyperparameters ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Keep track for plotting &  Check difference between training and eval loss to prevent overfitting\n",
    "t_lossi, e_lossi, total_training_loss, total_eval_loss = [], [], [], []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1aabc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([32, 60])\n",
      "torch.Size([32, 60])\n",
      "torch.Size([32, 60])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(training_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for j,batch, in enumerate(train_loader):\n",
    "    print(j)\n",
    "    _ids, at, lab = batch\n",
    "    print(_ids.shape)\n",
    "    print(at.shape)\n",
    "    print(lab.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb9165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 21:01:26,988\tINFO tune.py:586 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-12-12 21:52:17</td></tr>\n",
       "<tr><td>Running for: </td><td>00:50:50.75        </td></tr>\n",
       "<tr><td>Memory:      </td><td>24.3/255.7 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/16 CPUs, 1.0/1 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  step</th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">  training_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_3bbc1_00000</td><td>RUNNING </td><td>127.0.0.1:9260 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         3025.31</td><td style=\"text-align: right;\">  3600</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">       0.123966</td></tr>\n",
       "<tr><td>train_model_3bbc1_00001</td><td>RUNNING </td><td>127.0.0.1:14048</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">3e-05</td><td style=\"text-align: right;\">   361</td><td style=\"text-align: right;\">         3036.93</td><td style=\"text-align: right;\">  9025</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">       0.13892 </td></tr>\n",
       "<tr><td>train_model_3bbc1_00002</td><td>RUNNING </td><td>127.0.0.1:16260</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         3025.09</td><td style=\"text-align: right;\">  3600</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">       0.12388 </td></tr>\n",
       "<tr><td>train_model_3bbc1_00003</td><td>RUNNING </td><td>127.0.0.1:13576</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">3e-05</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         3025.25</td><td style=\"text-align: right;\">  3600</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">       0.142874</td></tr>\n",
       "<tr><td>train_model_3bbc1_00004</td><td>RUNNING </td><td>127.0.0.1:4436 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         3025.41</td><td style=\"text-align: right;\">  3600</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">       0.123451</td></tr>\n",
       "<tr><td>train_model_3bbc1_00005</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">2e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>train_model_3bbc1_00006</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>train_model_3bbc1_00007</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">2e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>train_model_3bbc1_00008</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">3e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>train_model_3bbc1_00009</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">3e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">               </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=16260)\u001b[0m C:\\Users\\NicHer\\AppData\\Local\\anaconda3\\envs\\envname2\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th style=\"text-align: right;\">  epoch</th><th>loss  </th><th style=\"text-align: right;\">  step</th><th style=\"text-align: right;\">  training_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_3bbc1_00000</td><td style=\"text-align: right;\">      1</td><td>      </td><td style=\"text-align: right;\">  3625</td><td style=\"text-align: right;\">       0.123434</td></tr>\n",
       "<tr><td>train_model_3bbc1_00001</td><td style=\"text-align: right;\">      1</td><td>      </td><td style=\"text-align: right;\">  9050</td><td style=\"text-align: right;\">       0.138649</td></tr>\n",
       "<tr><td>train_model_3bbc1_00002</td><td style=\"text-align: right;\">      1</td><td>      </td><td style=\"text-align: right;\">  3625</td><td style=\"text-align: right;\">       0.123358</td></tr>\n",
       "<tr><td>train_model_3bbc1_00003</td><td style=\"text-align: right;\">      1</td><td>      </td><td style=\"text-align: right;\">  3625</td><td style=\"text-align: right;\">       0.142264</td></tr>\n",
       "<tr><td>train_model_3bbc1_00004</td><td style=\"text-align: right;\">      1</td><td>      </td><td style=\"text-align: right;\">  3625</td><td style=\"text-align: right;\">       0.122919</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 25: 3.5628770446777343\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 50: 3.450376024246216\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 25: 3.4625025844573973\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 100: 3.1990654945373533\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 50: 2.9885331797599792\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 125: 3.0087327051162718\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 150: 2.750977282524109\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 175: 2.5171081709861753\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 75: 2.345032615661621\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 200: 2.3262782740592955\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 225: 2.152107840379079\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 100: 1.9711295801401139\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 275: 1.8870659518241881\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 300: 1.782145651181539\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 125: 1.693210730791092\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 325: 1.6931370868132665\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 350: 1.6065903336661203\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 150: 1.4707403476039569\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 400: 1.4643019242584705\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 425: 1.3979988341471727\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 175: 1.2997914410488947\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 450: 1.3353897636466556\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 475: 1.279697267224914\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 200: 1.1598670004308225\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 525: 1.1764689223823093\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 550: 1.1309593191607432\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 225: 1.0498165951834784\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 575: 1.0883124059795037\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 600: 1.0518562438556303\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 250: 0.9614884372353554\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 650: 0.9866196432681038\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 675: 0.9550723059999722\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 275: 0.8893089567260308\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 700: 0.9265155388122158\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 725: 0.8995623607034313\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 300: 0.8280376403530438\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 775: 0.8499314833119992\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 800: 0.8263547357497737\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 325: 0.7744150662880678\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 825: 0.8049438616475373\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 850: 0.7848003809921005\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 350: 0.7284249790864331\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 900: 0.747699609109097\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 925: 0.7307278696830208\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 375: 0.6898288207451503\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 950: 0.7151082596967094\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 975: 0.6997086252386754\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 400: 0.6548899586498738\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1025: 0.6727843523443472\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1050: 0.6599587284826807\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 425: 0.6224060957396732\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1075: 0.6474865863654156\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1100: 0.6360556299078532\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 450: 0.5946763799008395\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1150: 0.6138911107044829\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1175: 0.603536831525729\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 475: 0.5680411769763419\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1200: 0.593605139493011\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1225: 0.5834708664376212\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 500: 0.5440258952751755\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1275: 0.5649374939289455\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1300: 0.5565375935852241\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 525: 0.5210924752730699\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1325: 0.5480708177942993\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1350: 0.5403646732169997\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 550: 0.5006866127354177\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1400: 0.5249663462699391\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1425: 0.5180566802251626\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 575: 0.48188152156122355\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1450: 0.5113414432503411\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1475: 0.505114869747816\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 600: 0.46463256382383405\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1525: 0.49230162179341813\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1550: 0.4863978803707587\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 625: 0.44867655812054874\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1575: 0.4805924961724806\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1600: 0.4746690263247001\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 650: 0.4337964109087793\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1650: 0.4633725388717809\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1675: 0.45769185352959296\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 675: 0.41987878556742714\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1700: 0.45219688478227266\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1725: 0.447193996011401\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 700: 0.4064908784454955\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1775: 0.43798454887666544\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1800: 0.43324725544054266\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 725: 0.3944014304298265\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1825: 0.4287617031589457\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1850: 0.42388775823939895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 750: 0.38390364866331217\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1900: 0.4149329481933168\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1925: 0.4104510240360804\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 775: 0.37412310683318684\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1950: 0.406065712909047\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 1975: 0.4026529175097335\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 800: 0.364622739237966\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2025: 0.3948349411141725\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2050: 0.39016939612167956\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 825: 0.3556865559484471\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2075: 0.3865236032891637\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2100: 0.38277949670845246\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 850: 0.3472590330867645\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2150: 0.3754188590074976\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2175: 0.3718200713119352\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 875: 0.339057246170938\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2200: 0.3681966660332612\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2225: 0.3647008953257621\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 900: 0.3314370600848148\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2275: 0.3577946420220126\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2300: 0.3546987389620778\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 925: 0.3241204449423664\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2325: 0.3516205824683771\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2350: 0.34875549553596275\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 950: 0.3172455122978672\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2400: 0.3425828062867125\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2425: 0.33970591525825644\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 975: 0.31112746452578366\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2450: 0.336759299092679\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2475: 0.3339605306484972\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1000: 0.3057134727118537\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2525: 0.3286544387324685\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2550: 0.3261911222911166\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1025: 0.29962842198861084\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2575: 0.3236789831684595\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2600: 0.32113715219589023\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1050: 0.2939639083411367\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2650: 0.3160754915923407\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2675: 0.31373188992970946\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1075: 0.2884805760212069\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2700: 0.31139423665788923\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2725: 0.3090479609938424\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1100: 0.28308482035147875\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2775: 0.3042683172019551\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2800: 0.3019311795013657\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1125: 0.27812248926030264\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2825: 0.29970015929311317\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2850: 0.29752619744572595\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1150: 0.27338248141109944\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2900: 0.29331762922979926\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2925: 0.2914457232075242\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1175: 0.2686670435830317\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2950: 0.28960773418357566\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 2975: 0.28766579426895483\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1200: 0.2639008831442334\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3025: 0.2840981620724875\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3050: 0.2824079051614571\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1225: 0.2602291085517832\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3075: 0.28091172667432246\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3100: 0.2791464606623706\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1250: 0.25654296252951025\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3150: 0.27559134616926756\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3175: 0.2739374645623034\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1275: 0.25294505577680526\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3200: 0.2722173475662203\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3225: 0.2704292066748319\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1300: 0.24946378683814635\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3275: 0.26727777916717677\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3300: 0.2657265072421058\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1325: 0.24583578977101253\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3325: 0.2642569589355778\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3350: 0.26260768351871383\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1350: 0.24230100759477527\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3400: 0.2597816408093786\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3425: 0.2582431517867371\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1375: 0.2393959967019883\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3450: 0.25677273396486955\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3475: 0.2553359490712408\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1400: 0.2364972066280565\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3525: 0.252583022815215\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3550: 0.25121779399245164\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1425: 0.23383453653961944\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3575: 0.24984478080957814\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3600: 0.24856494823561257\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1450: 0.23115685813514322\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3650: 0.24596283551528794\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3675: 0.2446268410175456\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1475: 0.22839105198065104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3700: 0.24335722851044675\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3725: 0.24210437796616163\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1500: 0.22576009724661708\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3775: 0.2397706780149077\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3800: 0.23857288632005771\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1525: 0.22332248643284938\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3825: 0.23749897281033613\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3850: 0.23643100856026247\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1550: 0.22092994966574253\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3900: 0.23443155304179528\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3925: 0.23332568329305145\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1575: 0.21878131882065818\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3950: 0.23235789860259812\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 3975: 0.231390425429164\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1600: 0.2164915697928518\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4025: 0.22951910475023113\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4050: 0.22841583521418665\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1625: 0.214371304795146\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4075: 0.2273692937529263\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4100: 0.22631935417030302\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1650: 0.21229163677516308\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4150: 0.22420953296632673\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4175: 0.223199347423384\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1675: 0.21022957120479932\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4200: 0.22227247062047745\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4225: 0.2212249378969386\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1700: 0.20827802977250778\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4275: 0.2193442518057367\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4300: 0.2183269564257285\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1725: 0.20620491600123006\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4325: 0.21734156651500441\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4350: 0.2164584625707056\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1750: 0.20431790155598095\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4400: 0.21444221684919532\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4425: 0.21352127592477915\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1775: 0.20222596118882508\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4450: 0.21262235110859765\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4475: 0.2117850420223263\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1800: 0.20032891417853535\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4525: 0.2099902212953977\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4550: 0.20910337074324592\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1825: 0.19873200845738798\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4575: 0.20831990401555728\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4600: 0.207550836413132\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1850: 0.19701000592897872\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4650: 0.20586052253767248\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4675: 0.2050702000410859\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1875: 0.19554247187276683\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4700: 0.204213566531878\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4725: 0.20327784199111174\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1900: 0.19402315562002753\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4775: 0.20152000121646732\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4800: 0.20068782582795394\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1925: 0.19269791903724143\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4825: 0.19987485677362155\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4850: 0.19922325408192995\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1950: 0.19121879821499954\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4900: 0.1981572185426461\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4925: 0.19759233891823313\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 1975: 0.1896757931281117\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4950: 0.19700091141107906\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 4975: 0.1963294800491793\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2000: 0.1882593057155609\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5025: 0.19497735687894097\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5050: 0.19443346944624917\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2025: 0.18675975087432214\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5075: 0.19379199227938668\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5100: 0.19321231896433222\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2050: 0.18562093524852905\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5150: 0.19192849007700752\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5175: 0.1912883838737532\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2075: 0.18404039578624518\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5200: 0.19073828235016732\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5225: 0.19010296196879542\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2100: 0.18248654907835382\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5275: 0.18886256372787819\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5300: 0.18820585229396733\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2125: 0.18114840891080744\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5325: 0.18752977891513606\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5350: 0.18699697398153028\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2150: 0.1800087240364316\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5400: 0.18574578578735884\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5425: 0.1852773649457194\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2175: 0.1784962558146866\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5450: 0.18478729402737204\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5475: 0.18425132339780928\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2200: 0.17696419347966597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5525: 0.18336805814429402\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5550: 0.1828138553466845\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2225: 0.17561509629774294\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5575: 0.18242184732801536\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5600: 0.18190602649472046\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2250: 0.1741513715961741\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5650: 0.18101667471933672\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5675: 0.18057091642871687\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2275: 0.17276965843820638\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5700: 0.18015533045721607\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5725: 0.17968312639176773\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2300: 0.1715055378611483\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5775: 0.17869853772530414\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5800: 0.1782900167063204\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2325: 0.17030856731237584\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5825: 0.17779805277691066\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5850: 0.17730946903416497\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2350: 0.1691163396355795\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5900: 0.17642391022544363\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5925: 0.17597572224325686\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2375: 0.167887654303328\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5950: 0.17549969505116603\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 5975: 0.17506818241736888\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2400: 0.16666183098879023\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6025: 0.1741855458095689\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6050: 0.17374311949696472\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2425: 0.16569490953703825\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6075: 0.17331279793969326\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6100: 0.17297601826109668\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2450: 0.16462734268446053\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6150: 0.1722073219088332\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6175: 0.17181391636266463\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2475: 0.16343725490901206\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6200: 0.17136761699309816\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6225: 0.1710175092481161\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2500: 0.16244693263694643\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6275: 0.17028454554414255\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6300: 0.16989290301827067\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2525: 0.16130621807409984\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6325: 0.16945596948080593\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6350: 0.1690409286656668\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2550: 0.16019123917482062\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6400: 0.16832775409151965\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6425: 0.16793640669029847\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2575: 0.15891715650494184\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6450: 0.1676528177429541\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6475: 0.16725781981718663\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2600: 0.1577001848996196\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6525: 0.16654879821646848\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6550: 0.1662024487042133\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2625: 0.1564469681098126\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6575: 0.16589818528616543\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6600: 0.16552396905122088\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2650: 0.1552728102672274\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6650: 0.16487467896942964\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6675: 0.16455291699402275\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2675: 0.15413816148978787\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6700: 0.1641416583405573\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6725: 0.16379510737492073\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2700: 0.15302649366934526\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6775: 0.16315764263444546\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6800: 0.16279137843528588\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2725: 0.15191809751116883\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6825: 0.16243961135458096\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6850: 0.16207705072946862\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2750: 0.15074264148908498\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6900: 0.1613236441200749\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6925: 0.1609854378706619\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2775: 0.14965013357380674\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6950: 0.1606765127514628\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 6975: 0.1602975169046136\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2800: 0.1486173127787645\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7025: 0.15962985387291956\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7050: 0.15925761999940113\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2825: 0.14766150744217738\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7075: 0.15885932357120822\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7100: 0.15849182356043612\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2850: 0.14666595792420323\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7150: 0.1579203116853186\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7175: 0.15759233720171856\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2875: 0.14567809957118058\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7200: 0.15722685659083963\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7225: 0.15695560556961705\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2900: 0.14475534908294854\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7275: 0.1564341314340809\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7300: 0.1561637534942655\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2925: 0.1437949307442595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7325: 0.15591548642019246\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7350: 0.15559518377015882\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2950: 0.14286229991573066\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7400: 0.1549882614740754\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7425: 0.1547170813282856\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 2975: 0.14190616570833772\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7450: 0.15445646807150962\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7475: 0.15425030280782587\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3000: 0.14103750680198815\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7525: 0.15373769295064227\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7550: 0.15351768429871782\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3025: 0.14018927134356024\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7575: 0.15329081818922086\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7600: 0.1530318291800665\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3050: 0.13940873395619136\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7650: 0.15267987733928629\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7675: 0.15237537530113954\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3075: 0.138558277920233\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7700: 0.15216771332873258\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7725: 0.15188600113860656\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3100: 0.13772724904268305\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7775: 0.15148431336631102\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7800: 0.15125439819066042\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3125: 0.13688498870415614\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7825: 0.15097777088555453\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7850: 0.1507314729575953\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3150: 0.13600500529334467\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7900: 0.15014754461491836\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7925: 0.14991245173611562\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3175: 0.13520244767740883\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7950: 0.1496745080323985\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 7975: 0.14942068065414102\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3200: 0.13436962613091963\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8025: 0.14889953700164435\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8050: 0.14861103717112256\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3225: 0.13352179043801762\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8075: 0.1484144708921523\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8100: 0.14812172975533794\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3250: 0.13267273639481014\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8150: 0.14778829340444824\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8175: 0.1475990244731861\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3275: 0.13197537000626966\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8200: 0.14737486041030667\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8225: 0.14713828870397144\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3300: 0.13131343310517188\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8275: 0.14656233773147195\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8300: 0.1462581615086823\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3325: 0.13062237082597192\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8325: 0.14593428748500023\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8350: 0.14566707469904613\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3350: 0.1299584608589153\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8400: 0.14509598958951495\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8425: 0.14490166081620992\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3375: 0.12922900097515366\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8450: 0.14463551843486183\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8475: 0.1444132288969788\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3400: 0.1285093273943708\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8525: 0.14409029989765174\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8550: 0.14393195537879874\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3425: 0.12791279508638786\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8575: 0.1437070200237094\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8600: 0.14347720780975368\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3450: 0.1273428966262398\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8650: 0.14294103792846685\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8675: 0.14266272515970718\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3475: 0.1267857360125104\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8700: 0.1423655002598481\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8725: 0.14206004146086573\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3500: 0.12618933363879167\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8775: 0.14146799514099673\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8800: 0.14117985206542719\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3525: 0.1255971106916752\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8825: 0.1409835257614724\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8850: 0.14075076262783956\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3550: 0.12499247806351697\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8900: 0.14020065404999585\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8925: 0.13996076211254035\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3575: 0.12444233115497078\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8950: 0.13970818432255105\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 8975: 0.13941171359373283\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3600: 0.12387985539165028\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 9025: 0.13892005009321273\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14048)\u001b[0m Average T loss at step 9050: 0.13864930168095235\n",
      "\u001b[36m(train_model pid=16260)\u001b[0m Average T loss at step 3625: 0.12335814664726434\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train_model(params, model, training_dataset, evaluation_dataset):\n",
    "\n",
    "    #Reset loaders with batch size parameter\n",
    "    train_loader = DataLoader(training_dataset, batch_size=params[\"batch_size\"], shuffle=False) #False for reproducibility for now\n",
    "    eval_loader = DataLoader(evaluation_dataset, batch_size=params[\"batch_size\"], shuffle=False) #False for reproducibility for now\n",
    "    \n",
    "    \n",
    "    optimizer = AdamW(params = model.parameters(), lr=params[\"lr\"])\n",
    "    \n",
    "    total_steps = (len(training_dataset) // params[\"batch_size\"]) * params[\"epochs\"]\n",
    "    warmup_steps = int(total_steps * 0.1) #standard 10th\n",
    "    \n",
    "    scheduler = get_scheduler(\"linear\", \n",
    "                              optimizer=optimizer, \n",
    "                              num_warmup_steps= warmup_steps, ##set warmup steps to 0.1 * total num steps \n",
    "                              num_training_steps=total_steps)\n",
    "    \n",
    "    \n",
    "    for i in range(1, params[\"epochs\"] + 1):\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "        for j,batch in enumerate(train_loader):\n",
    "            _ids, at, lab = batch #_ids, attention_mask, labels = [b.to(model.device) for b in batch] - looks nicer\n",
    "            \n",
    "            out = model(input_ids=_ids.to(device), attention_mask=at.to(device), labels=lab.to(device)) #   logits = [256, 40, 31] B,T,C, loss is NLL\n",
    "            train_loss += out.loss.item()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm) #gradient clipping - safety net\n",
    "\n",
    "            optimizer.zero_grad() # Zero gradients between each update\n",
    "            out.loss.backward()   # Calculate gradients\n",
    "            optimizer.step()      # Step\n",
    "\n",
    "            if scheduler:  # Update learning rate\n",
    "                scheduler.step()\n",
    "\n",
    "            if j % 25 == 0 and j > 0:\n",
    "                print(\"Average T loss at step {}: {}\".format(j, train_loss / j ))\n",
    "                train.report({\"step\": j, \"epoch\": i, \"training_loss\": train_loss / j, \"loss\": None}) ##loss needs to be 0 here otherwise error\n",
    "                t_lossi.append(train_loss / j)\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for j,batch in enumerate(eval_loader):\n",
    "                _ids, at, lab = batch\n",
    "                out = model(input_ids=_ids.to(device), attention_mask=at.to(device), labels=lab.to(device))\n",
    "\n",
    "                eval_loss += out.loss.item()\n",
    "\n",
    "                if j % 25 == 0 and j > 0:\n",
    "                    print(\"Average E loss at step {}: {}\".format(j, eval_loss / j ))\n",
    "                    train.report({\"step\": j, \"epoch\": i, \"evaluation_loss\": train_loss / j, \"loss\": None }) ##loss needs to be 0 here otherwise error\n",
    "                    e_lossi.append(eval_loss / j )\n",
    "        \n",
    "        \n",
    "        #report average evaluation loss to tune\n",
    "        avg_eval_loss = eval_loss / len(eval_loader)\n",
    "        train.report({\"loss\" : avg_eval_loss})\n",
    "\n",
    "        print(\"epoch: \", i)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "## Random search of hyperparameters, not optimal but its something\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(\n",
    "        train_model,                           #name of function - normally trainer.train\n",
    "        model=model,                          #model input\n",
    "        training_dataset=training_dataset,    #this otherwise errors when running\n",
    "        evaluation_dataset=evaluation_dataset  #this otherwise errors when running\n",
    "    ),\n",
    "    resources_per_trial={\n",
    "        \"gpu\": 0.2  # only gpu, set to 0.2 for 5 simultaneous jobs\n",
    "    },\n",
    "    config={\n",
    "        \"lr\": tune.choice([2e-5, 3e-5, 5e-5]),    #per original bert paper\n",
    "        \"batch_size\": tune.choice([16, 32, 64]),    #batch sizes to test\n",
    "        \"epochs\": tune.choice([1]),           #epoch choices\n",
    "    }, \n",
    "    num_samples=10,                                   #total number of combinations\n",
    "    metric=\"loss\",                                 # The metric to optimize\n",
    "    mode=\"min\"    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b0768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = analysis.get_best_config(metric=\"loss\", mode=\"min\")\n",
    "print(\"Best hyperparameters found were: \", best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for overfitting by comparing performance on training data and evaluation data\n",
    "plt.plot(t_lossi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf0d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(e_lossi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb836c2",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf89e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def test(test_loader):\n",
    "    model.eval()\n",
    "    predictions, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            _ids, at, lab = batch\n",
    "            out = model(input_ids=_ids, attention_mask=at)\n",
    "\n",
    "            #print(out.logits.shape)  #dim 2 are predictions for each input\n",
    "            preds = torch.argmax(out.logits, dim=2)\n",
    "\n",
    "\n",
    "            # Only compare indicies where there is not padding or special tokens\n",
    "            mask = (lab != -100) #& (lab != 0)\n",
    "\n",
    "            # Also test with mask = lab != 0, since the class is so large\n",
    "\n",
    "            # Retrieve correct indices\n",
    "            preds = preds[mask]\n",
    "            labs = lab[mask]\n",
    "\n",
    "            # Tolist for comparison\n",
    "            predictions.extend(preds.tolist())\n",
    "            labels.extend(labs.tolist())\n",
    "    \n",
    "    return predictions, labels\n",
    "\n",
    "\n",
    "#accuracy is high, because most are just 0\n",
    "sum([1 for x,y in zip(predictions, labels) if x==y]) / len(labels)\n",
    "\n",
    "preds, labels = test(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7312ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cm = metrics.confusion_matrix([itos[x] for x in labels], [itos[x] for x in predictions])\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(30, 30))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041cde54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envname2",
   "language": "python",
   "name": "envname2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
