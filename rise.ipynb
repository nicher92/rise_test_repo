{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac54ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (AutoModel, AutoTokenizer,\n",
    "                          BertForTokenClassification, get_linear_schedule_with_warmup, \n",
    "                          AdamW, get_scheduler)\n",
    "\n",
    "import string\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "from ray import tune, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1231bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset_name = \"Babelscape/multinerd\"\n",
    "model_name = \"bert-base-cased\" # Basic bert model - cased since the text is cased and it probably helps label entities like \"Johnny Depp\"\n",
    "\n",
    "# Set device, also used for hyperparameter search\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    gpu_per_trial = 1 / num_gpus\n",
    "    cpu_per_trial = 0\n",
    "else:\n",
    "    gpu_per_trial = 0 \n",
    "    cpu_per_trial = 2  # Something low\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e333d3d",
   "metadata": {},
   "source": [
    "# Load data & Remove non-english items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaa5b986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/NicHer/.cache/huggingface/datasets/Babelscape___json/Babelscape--multinerd-f822e910a4f604c0/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05bd5e0c039f4c088e97ac4d743c0066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\NicHer\\.cache\\huggingface\\datasets\\Babelscape___json\\Babelscape--multinerd-f822e910a4f604c0\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-eb9ab4cbc9b9233f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\NicHer\\.cache\\huggingface\\datasets\\Babelscape___json\\Babelscape--multinerd-f822e910a4f604c0\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-eed54e3e6216fb0e.arrow\n",
      "Loading cached processed dataset at C:\\Users\\NicHer\\.cache\\huggingface\\datasets\\Babelscape___json\\Babelscape--multinerd-f822e910a4f604c0\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-fe66576485de1b0f.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loads data, only keeps english, removes language column\n",
    "dataset_eng = load_dataset(dataset_name).filter(lambda x: x[\"lang\"] == \"en\").remove_columns(\"lang\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da548d",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d920ff75",
   "metadata": {},
   "source": [
    "### Itos & Stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f200a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping from integer labels to strings (from HF dataset repo) and vice versa\n",
    "\n",
    "stoi = {\n",
    "    \"O\": 0,\n",
    "    \"B-PER\": 1,\n",
    "    \"I-PER\": 2,\n",
    "    \"B-ORG\": 3,\n",
    "    \"I-ORG\": 4,\n",
    "    \"B-LOC\": 5,\n",
    "    \"I-LOC\": 6,\n",
    "    \"B-ANIM\": 7,\n",
    "    \"I-ANIM\": 8,\n",
    "    \"B-BIO\": 9,\n",
    "    \"I-BIO\": 10,\n",
    "    \"B-CEL\": 11,\n",
    "    \"I-CEL\": 12,\n",
    "    \"B-DIS\": 13,\n",
    "    \"I-DIS\": 14,\n",
    "    \"B-EVE\": 15,\n",
    "    \"I-EVE\": 16,\n",
    "    \"B-FOOD\": 17,\n",
    "    \"I-FOOD\": 18,\n",
    "    \"B-INST\": 19,\n",
    "    \"I-INST\": 20,\n",
    "    \"B-MEDIA\": 21,\n",
    "    \"I-MEDIA\": 22,\n",
    "    \"B-MYTH\": 23,\n",
    "    \"I-MYTH\": 24,\n",
    "    \"B-PLANT\": 25,\n",
    "    \"I-PLANT\": 26,\n",
    "    \"B-TIME\": 27,\n",
    "    \"I-TIME\": 28,\n",
    "    \"B-VEHI\": 29,\n",
    "    \"I-VEHI\": 30,\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "itos = {value:key for key,value in stoi.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0db2c5",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aecabe80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4957198,\n",
       " 5: 117330,\n",
       " 25: 14872,\n",
       " 26: 4702,\n",
       " 1: 125974,\n",
       " 2: 132376,\n",
       " 13: 17404,\n",
       " 14: 11608,\n",
       " 9: 280,\n",
       " 21: 12162,\n",
       " 22: 20070,\n",
       " 6: 48800,\n",
       " 3: 55282,\n",
       " 4: 71998,\n",
       " 27: 5080,\n",
       " 7: 25472,\n",
       " 8: 10614,\n",
       " 15: 5050,\n",
       " 16: 8406,\n",
       " 28: 3942,\n",
       " 11: 5370,\n",
       " 12: 2972,\n",
       " 17: 16558,\n",
       " 18: 6060,\n",
       " 29: 808,\n",
       " 30: 956,\n",
       " 23: 1138,\n",
       " 24: 202,\n",
       " 19: 758,\n",
       " 20: 726,\n",
       " 10: 70}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Uneven class distribution\n",
    "nercounts = dict(Counter([item for sublist in dataset_eng[\"train\"][\"ner_tags\"] for item in sublist]))\n",
    "nercounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "071384be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter 0 for plotting\n",
    "nercounts = {k:v for k,v in nercounts.items() if k != 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05b1940d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Counts')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABPAAAAMaCAYAAAAIu7hpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAACsHklEQVR4nOzdd7gV5dU34HUQKQIHsAASUVARQYlEUQRLLCgKUYmYYG+osWAjsSuSxFiwd9I176uJMW9irChqlNhQERQQUCMqmgCJCkdQqev7w+/ssAWN5ZQx3Pd1nQv2zLP3rN1mz/zmmWcqMjMDAAAAACikBvVdAAAAAADwyQR4AAAAAFBgAjwAAAAAKDABHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIE1rO8CViXLli2Lv//979GiRYuoqKio73IAAAAAqCeZGe+99160b98+GjT49D52Arw69Pe//z06dOhQ32UAAAAAUBAzZ86M9dZb71PbCPDqUIsWLSLiozemsrKynqsBAAAAoL5UVVVFhw4dSnnRpxHg1aHq02YrKysFeAAAAAB8pmHWXMQCAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIEJ8AAAAACgwAR4AAAAAFBgAjwAAAAAKDABHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIEJ8AAAAACgwAR4AAAAAFBgAjwAAAAAKDABHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIEJ8AAAAACgwAR4AAAAAFBgAjwAAAAAKDABHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIEJ8AAAAACgwAR4AAAAAFBgDeu7AGDV1PHMe+p0ea9dPKBOlwcAAAA1RQ88AAAAACgwAR4AAAAAFJgADwAAAAAKTIAHAAAAAAUmwAMAAACAAhPgAQAAAECBCfAAAAAAoMAEeAAAAABQYAI8AAAAACgwAR4AAAAAFJgADwAAAAAKTIAHAAAAAAUmwAMAAACAAhPgAQAAAECBCfAAAAAAoMAEeAAAAABQYAI8AAAAACgwAR4AAAAAFJgADwAAAAAKTIAHAAAAAAUmwAMAAACAAhPgAQAAAECBCfAAAAAAoMAEeAAAAABQYAI8AAAAACgwAR4AAAAAFJgADwAAAAAKTIAHAAAAAAUmwAMAAACAAhPgAQAAAECBCfAAAAAAoMAa1ncBQN3oeOY9db7M1y4eUOfLBAAAgP82euABAAAAQIEJ8AAAAACgwAR4AAAAAFBgAjwAAAAAKDABHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIEJ8AAAAACgwAR4AAAAAFBgAjwAAAAAKDABHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIEJ8AAAAACgwAR4AAAAAFBgAjwAAAAAKDABHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACq9cAb+zYsbHXXntF+/bto6KiIu64447SvMWLF8cZZ5wR3bt3j2bNmkX79u3j0EMPjb///e9lj/HOO+/EQQcdFJWVldGqVasYMmRIzJ8/v6zNCy+8EDvssEM0adIkOnToECNHjlyhlttvvz023XTTaNKkSXTv3j3uvffesvmZGcOHD4911103mjZtGn379o2XX3655l4MAAAAAFiJeg3wFixYEFtssUVcf/31K8x7//3347nnnovzzjsvnnvuufjjH/8Y06dPj7333rus3UEHHRRTpkyJMWPGxN133x1jx46NY445pjS/qqoqdt9999hggw1i/Pjxcemll8aIESPiZz/7WanNE088EQcccEAMGTIkJkyYEAMHDoyBAwfG5MmTS21GjhwZ11xzTYwaNSrGjRsXzZo1i379+sWHH35YC68MAAAAAHykIjOzvouIiKioqIg//elPMXDgwE9s88wzz8Q222wTr7/+eqy//voxderU6NatWzzzzDPRs2fPiIgYPXp09O/fP958881o37593HjjjXHOOefErFmzolGjRhERceaZZ8Ydd9wR06ZNi4iIwYMHx4IFC+Luu+8uLWvbbbeNHj16xKhRoyIzo3379vH9738/fvCDH0RExLx586Jt27Zx0003xf777/+ZnmNVVVW0bNky5s2bF5WVlV/kZYIvrOOZ99T5Ml+7eMAnzqvrej6tFgAAAKhrnycn+kqNgTdv3ryoqKiIVq1aRUTEk08+Ga1atSqFdxERffv2jQYNGsS4ceNKbXbcccdSeBcR0a9fv5g+fXq8++67pTZ9+/YtW1a/fv3iySefjIiIGTNmxKxZs8ratGzZMnr16lVqszILFy6Mqqqqsj8AAAAA+Dy+MgHehx9+GGeccUYccMABpVRy1qxZ0aZNm7J2DRs2jDXXXDNmzZpVatO2bduyNtW3/1Ob5ecvf7+VtVmZiy66KFq2bFn669Chw+d6zgAAAADwlQjwFi9eHN/97ncjM+PGG2+s73I+s7POOivmzZtX+ps5c2Z9lwQAAADAV0zD+i7gP6kO715//fV4+OGHy84JbteuXcyZM6es/ZIlS+Kdd96Jdu3aldrMnj27rE317f/UZvn51dPWXXfdsjY9evT4xNobN24cjRs3/jxPFwAAAADKFLoHXnV49/LLL8eDDz4Ya621Vtn83r17x9y5c2P8+PGlaQ8//HAsW7YsevXqVWozduzYWLx4canNmDFjokuXLtG6detSm4ceeqjssceMGRO9e/eOiIhOnTpFu3btytpUVVXFuHHjSm0AAAAAoDbUa4A3f/78mDhxYkycODEiPrpYxMSJE+ONN96IxYsXx3777RfPPvts3HLLLbF06dKYNWtWzJo1KxYtWhQREV27do099tgjjj766Hj66afj8ccfj6FDh8b+++8f7du3j4iIAw88MBo1ahRDhgyJKVOmxG233RZXX311DBs2rFTHySefHKNHj47LL788pk2bFiNGjIhnn302hg4dGhEfXSH3lFNOiQsuuCDuvPPOmDRpUhx66KHRvn37T71qLgAAAAB8WfV6Cu2zzz4bO++8c+l2dah22GGHxYgRI+LOO++MiFjhNNW//OUvsdNOO0VExC233BJDhw6NXXfdNRo0aBCDBg2Ka665ptS2ZcuW8cADD8QJJ5wQW221Vay99toxfPjwOOaYY0pt+vTpE7feemuce+65cfbZZ0fnzp3jjjvuiM0337zU5vTTT48FCxbEMcccE3Pnzo3tt98+Ro8eHU2aNKnplwUAAAAASioyM+u7iFVFVVVVtGzZMubNm1c2lh/UhY5n3lPny3zt4gGfOK+u6/m0WgAAAKCufZ6cqNBj4AEAAADAqk6ABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAosIb1XQBffR3PvKdOl/faxQPqdHkAAAAA9UkPPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAosHoN8MaOHRt77bVXtG/fPioqKuKOO+4om5+ZMXz48Fh33XWjadOm0bdv33j55ZfL2rzzzjtx0EEHRWVlZbRq1SqGDBkS8+fPL2vzwgsvxA477BBNmjSJDh06xMiRI1eo5fbbb49NN900mjRpEt27d4977733c9cCAAAAADWtXgO8BQsWxBZbbBHXX3/9SuePHDkyrrnmmhg1alSMGzcumjVrFv369YsPP/yw1Oaggw6KKVOmxJgxY+Luu++OsWPHxjHHHFOaX1VVFbvvvntssMEGMX78+Lj00ktjxIgR8bOf/azU5oknnogDDjgghgwZEhMmTIiBAwfGwIEDY/LkyZ+rFgAAAACoaRWZmfVdRERERUVF/OlPf4qBAwdGxEc93tq3bx/f//734wc/+EFERMybNy/atm0bN910U+y///4xderU6NatWzzzzDPRs2fPiIgYPXp09O/fP958881o37593HjjjXHOOefErFmzolGjRhERceaZZ8Ydd9wR06ZNi4iIwYMHx4IFC+Luu+8u1bPttttGjx49YtSoUZ+pls+iqqoqWrZsGfPmzYvKysoaed2KoOOZ99Tp8l67eECdLu+/RV2/TxGf/l753AAAALAq+zw5UWHHwJsxY0bMmjUr+vbtW5rWsmXL6NWrVzz55JMREfHkk09Gq1atSuFdRETfvn2jQYMGMW7cuFKbHXfcsRTeRUT069cvpk+fHu+++26pzfLLqW5TvZzPUsvKLFy4MKqqqsr+AAAAAODzKGyAN2vWrIiIaNu2bdn0tm3blubNmjUr2rRpUza/YcOGseaaa5a1WdljLL+MT2qz/Pz/VMvKXHTRRdGyZcvSX4cOHf7DswYAAACAcoUN8P4bnHXWWTFv3rzS38yZM+u7JAAAAAC+Ygob4LVr1y4iImbPnl02ffbs2aV57dq1izlz5pTNX7JkSbzzzjtlbVb2GMsv45PaLD//P9WyMo0bN47KysqyPwAAAAD4PAob4HXq1CnatWsXDz30UGlaVVVVjBs3Lnr37h0REb179465c+fG+PHjS20efvjhWLZsWfTq1avUZuzYsbF48eJSmzFjxkSXLl2idevWpTbLL6e6TfVyPkstAAAAAFAb6jXAmz9/fkycODEmTpwYER9dLGLixInxxhtvREVFRZxyyilxwQUXxJ133hmTJk2KQw89NNq3b1+6Um3Xrl1jjz32iKOPPjqefvrpePzxx2Po0KGx//77R/v27SMi4sADD4xGjRrFkCFDYsqUKXHbbbfF1VdfHcOGDSvVcfLJJ8fo0aPj8ssvj2nTpsWIESPi2WefjaFDh0ZEfKZaAAAAAKA2NKzPhT/77LOx8847l25Xh2qHHXZY3HTTTXH66afHggUL4phjjom5c+fG9ttvH6NHj44mTZqU7nPLLbfE0KFDY9ddd40GDRrEoEGD4pprrinNb9myZTzwwANxwgknxFZbbRVrr712DB8+PI455phSmz59+sStt94a5557bpx99tnRuXPnuOOOO2LzzTcvtfkstQAAAABATavIzKzvIlYVVVVV0bJly5g3b95/1Xh4Hc+8p06X99rFA+p0ef8t6vp9ivj098rnBgAAgFXZ58mJCjsGHgAAAAAgwAMAAACAQhPgAQAAAECBCfAAAAAAoMAEeAAAAABQYAI8AAAAACgwAR4AAAAAFJgADwAAAAAKTIAHAAAAAAUmwAMAAACAAhPgAQAAAECBCfAAAAAAoMAEeAAAAABQYAI8AAAAACgwAR4AAAAAFJgADwAAAAAKTIAHAAAAAAUmwAMAAACAAhPgAQAAAECBCfAAAAAAoMAEeAAAAABQYAI8AAAAACgwAR4AAAAAFJgADwAAAAAKTIAHAAAAAAUmwAMAAACAAhPgAQAAAECBCfAAAAAAoMAEeAAAAABQYAI8AAAAACgwAR4AAAAAFJgADwAAAAAKTIAHAAAAAAUmwAMAAACAAhPgAQAAAECBCfAAAAAAoMAEeAAAAABQYAI8AAAAACgwAR4AAAAAFJgADwAAAAAKTIAHAAAAAAUmwAMAAACAAhPgAQAAAECBCfAAAAAAoMAEeAAAAABQYAI8AAAAACgwAR4AAAAAFJgADwAAAAAKTIAHAAAAAAUmwAMAAACAAhPgAQAAAECBCfAAAAAAoMAEeAAAAABQYAI8AAAAACgwAR4AAAAAFJgADwAAAAAKTIAHAAAAAAUmwAMAAACAAhPgAQAAAECBCfAAAAAAoMAEeAAAAABQYAI8AAAAACgwAR4AAAAAFJgADwAAAAAKTIAHAAAAAAUmwAMAAACAAhPgAQAAAECBCfAAAAAAoMAEeAAAAABQYAI8AAAAACgwAR4AAAAAFJgADwAAAAAKTIAHAAAAAAUmwAMAAACAAhPgAQAAAECBCfAAAAAAoMAEeAAAAABQYAI8AAAAACgwAR4AAAAAFJgADwAAAAAKTIAHAAAAAAUmwAMAAACAAhPgAQAAAECBCfAAAAAAoMAEeAAAAABQYAI8AAAAACgwAR4AAAAAFJgADwAAAAAKTIAHAAAAAAVW6ABv6dKlcd5550WnTp2iadOmsdFGG8WPf/zjyMxSm8yM4cOHx7rrrhtNmzaNvn37xssvv1z2OO+8804cdNBBUVlZGa1atYohQ4bE/Pnzy9q88MILscMOO0STJk2iQ4cOMXLkyBXquf3222PTTTeNJk2aRPfu3ePee++tnScOAAAAAP9foQO8Sy65JG688ca47rrrYurUqXHJJZfEyJEj49prry21GTlyZFxzzTUxatSoGDduXDRr1iz69esXH374YanNQQcdFFOmTIkxY8bE3XffHWPHjo1jjjmmNL+qqip233332GCDDWL8+PFx6aWXxogRI+JnP/tZqc0TTzwRBxxwQAwZMiQmTJgQAwcOjIEDB8bkyZPr5sUAAAAAYJVUkct3ZyuYb33rW9G2bdv45S9/WZo2aNCgaNq0afzv//5vZGa0b98+vv/978cPfvCDiIiYN29etG3bNm666abYf//9Y+rUqdGtW7d45plnomfPnhERMXr06Ojfv3+8+eab0b59+7jxxhvjnHPOiVmzZkWjRo0iIuLMM8+MO+64I6ZNmxYREYMHD44FCxbE3XffXapl2223jR49esSoUaM+0/OpqqqKli1bxrx586KysrJGXqMi6HjmPXW6vNcuHlCny/tvUdfvU8Snv1c+NwAAAKzKPk9OVOgeeH369ImHHnooXnrppYiIeP755+Oxxx6LPffcMyIiZsyYEbNmzYq+ffuW7tOyZcvo1atXPPnkkxER8eSTT0arVq1K4V1ERN++faNBgwYxbty4Upsdd9yxFN5FRPTr1y+mT58e7777bqnN8supblO9nJVZuHBhVFVVlf0BAAAAwOfRsL4L+DRnnnlmVFVVxaabbhqrrbZaLF26NH7yk5/EQQcdFBERs2bNioiItm3blt2vbdu2pXmzZs2KNm3alM1v2LBhrLnmmmVtOnXqtMJjVM9r3bp1zJo161OXszIXXXRR/PCHP/y8TxsAAAAASgrdA+/3v/993HLLLXHrrbfGc889FzfffHNcdtllcfPNN9d3aZ/JWWedFfPmzSv9zZw5s75LAgAAAOArptA98E477bQ488wzY//994+IiO7du8frr78eF110URx22GHRrl27iIiYPXt2rLvuuqX7zZ49O3r06BEREe3atYs5c+aUPe6SJUvinXfeKd2/Xbt2MXv27LI21bf/U5vq+SvTuHHjaNy48ed92gAAAABQUugeeO+//340aFBe4mqrrRbLli2LiIhOnTpFu3bt4qGHHirNr6qqinHjxkXv3r0jIqJ3794xd+7cGD9+fKnNww8/HMuWLYtevXqV2owdOzYWL15cajNmzJjo0qVLtG7dutRm+eVUt6leDgAAAADUhkIHeHvttVf85Cc/iXvuuSdee+21+NOf/hRXXHFFfPvb346IiIqKijjllFPiggsuiDvvvDMmTZoUhx56aLRv3z4GDhwYERFdu3aNPfbYI44++uh4+umn4/HHH4+hQ4fG/vvvH+3bt4+IiAMPPDAaNWoUQ4YMiSlTpsRtt90WV199dQwbNqxUy8knnxyjR4+Oyy+/PKZNmxYjRoyIZ599NoYOHVrnrwsAAAAAq45Cn0J77bXXxnnnnRfHH398zJkzJ9q3bx/f+973Yvjw4aU2p59+eixYsCCOOeaYmDt3bmy//fYxevToaNKkSanNLbfcEkOHDo1dd901GjRoEIMGDYprrrmmNL9ly5bxwAMPxAknnBBbbbVVrL322jF8+PA45phjSm369OkTt956a5x77rlx9tlnR+fOneOOO+6IzTffvG5eDAAAAABWSRWZmfVdxKqiqqoqWrZsGfPmzYvKysr6LqfGdDzznjpd3msXD6jT5f23qOv3KeLT3yufGwAAAFZlnycnKvQptAAAAACwqhPgAQAAAECBCfAAAAAAoMAEeAAAAABQYAI8AAAAACgwAR4AAAAAFJgADwAAAAAKTIAHAAAAAAUmwAMAAACAAhPgAQAAAECBCfAAAAAAoMAEeAAAAABQYAI8AAAAACgwAR4AAAAAFJgADwAAAAAKTIAHAAAAAAUmwAMAAACAAhPgAQAAAECBCfAAAAAAoMAEeAAAAABQYAI8AAAAACgwAR4AAAAAFJgADwAAAAAKTIAHAAAAAAX2hQK85557LiZNmlS6/ec//zkGDhwYZ599dixatKjGigMAAACAVd0XCvC+973vxUsvvRQREa+++mrsv//+scYaa8Ttt98ep59+eo0WCAAAAACrsi8U4L300kvRo0ePiIi4/fbbY8cdd4xbb701brrppvi///u/mqwPAAAAAFZpXyjAy8xYtmxZREQ8+OCD0b9//4iI6NChQ/zrX/+queoAAAAAYBX3hQK8nj17xgUXXBD/8z//E48++mgMGDAgIiJmzJgRbdu2rdECAQAAAGBV9oUCvCuvvDKee+65GDp0aJxzzjmx8cYbR0TEH/7wh+jTp0+NFggAAAAAq7KGX+ROW2yxRdlVaKtdeuml0bDhF3pIAAAAAGAlvlAPvA033DDefvvtFaZ/+OGHsckmm3zpogAAAACAj3yhAO+1116LpUuXrjB94cKF8eabb37pogAAAACAj3yu813vvPPO0v/vv//+aNmyZen20qVL46GHHopOnTrVXHUAAF9CxzPvqfNlvnbxgDpfJgAA/90+V4A3cODAiIioqKiIww47rGze6quvHh07dozLL7+8xooDAAAAgFXd5wrwli1bFhERnTp1imeeeSbWXnvtWikKAAAAAPjIF7pk7IwZM2q6DgAAAABgJb5QgBcR8dBDD8VDDz0Uc+bMKfXMq/arX/3qSxcGAAAAAHzBAO+HP/xh/OhHP4qePXvGuuuuGxUVFTVdFwAAAAAQXzDAGzVqVNx0001xyCGH1HQ9AAAAAMByGnyROy1atCj69OlT07UAAAAAAB/zhQK8o446Km699daargUAAAAA+JgvdArthx9+GD/72c/iwQcfjK9//eux+uqrl82/4ooraqQ4AAAAAFjVfaEA74UXXogePXpERMTkyZPL5rmgBQAAAADUnC8U4P3lL3+p6ToAAAAAgJX4QmPgAQAAAAB14wv1wNt5550/9VTZhx9++AsXBAAAAAD82xcK8KrHv6u2ePHimDhxYkyePDkOO+ywmqgLAAAAAIgvGOBdeeWVK50+YsSImD9//pcqCAAAAAD4txodA+/ggw+OX/3qVzX5kAAAAACwSqvRAO/JJ5+MJk2a1ORDAgAAAMAq7QudQrvvvvuW3c7M+Mc//hHPPvtsnHfeeTVSGAAAAADwBQO8li1blt1u0KBBdOnSJX70ox/F7rvvXiOFAQAAAABfMMD79a9/XdN1AAAAAAAr8YUCvGrjx4+PqVOnRkTEZpttFt/4xjdqpCgAAAAA4CNfKMCbM2dO7L///vHII49Eq1atIiJi7ty5sfPOO8fvfve7WGeddWqyRgAAAABYZX2hq9CeeOKJ8d5778WUKVPinXfeiXfeeScmT54cVVVVcdJJJ9V0jQAAAACwyvpCPfBGjx4dDz74YHTt2rU0rVu3bnH99de7iAUAAAAA1KAv1ANv2bJlsfrqq68wffXVV49ly5Z96aIAAAAAgI98oQBvl112iZNPPjn+/ve/l6a99dZbceqpp8auu+5aY8UBAAAAwKruCwV41113XVRVVUXHjh1jo402io022ig6deoUVVVVce2119Z0jQAAAACwyvpCY+B16NAhnnvuuXjwwQdj2rRpERHRtWvX6Nu3b40WBwAAAACrus/VA+/hhx+Obt26RVVVVVRUVMRuu+0WJ554Ypx44omx9dZbx2abbRZ//etfa6tWAAAAAFjlfK4A76qrroqjjz46KisrV5jXsmXL+N73vhdXXHFFjRUHAAAAAKu6zxXgPf/887HHHnt84vzdd989xo8f/6WLAgAAAAA+8rkCvNmzZ8fqq6/+ifMbNmwY//znP790UQAAAADARz5XgPe1r30tJk+e/InzX3jhhVh33XW/dFEAAAAAwEc+V4DXv3//OO+88+LDDz9cYd4HH3wQ559/fnzrW9+qseIAAAAAYFXX8PM0Pvfcc+OPf/xjbLLJJjF06NDo0qVLRERMmzYtrr/++li6dGmcc845tVIoAAAAAKyKPleA17Zt23jiiSfiuOOOi7POOisyMyIiKioqol+/fnH99ddH27Zta6VQAAAAAFgVfa4ALyJigw02iHvvvTfefffdeOWVVyIzo3PnztG6devaqA8AAAAAVmmfO8Cr1rp169h6661rshYAAAAA4GM+10UsAAAAAIC6JcADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACqzwAd5bb70VBx98cKy11lrRtGnT6N69ezz77LOl+ZkZw4cPj3XXXTeaNm0affv2jZdffrnsMd5555046KCDorKyMlq1ahVDhgyJ+fPnl7V54YUXYocddogmTZpEhw4dYuTIkSvUcvvtt8emm24aTZo0ie7du8e9995bO08aAAAAAP6/Qgd47777bmy33Xax+uqrx3333RcvvvhiXH755dG6detSm5EjR8Y111wTo0aNinHjxkWzZs2iX79+8eGHH5baHHTQQTFlypQYM2ZM3H333TF27Ng45phjSvOrqqpi9913jw022CDGjx8fl156aYwYMSJ+9rOfldo88cQTccABB8SQIUNiwoQJMXDgwBg4cGBMnjy5bl4MAAAAAFZJDeu7gE9zySWXRIcOHeLXv/51aVqnTp1K/8/MuOqqq+Lcc8+NffbZJyIifvOb30Tbtm3jjjvuiP333z+mTp0ao0ePjmeeeSZ69uwZERHXXntt9O/fPy677LJo37593HLLLbFo0aL41a9+FY0aNYrNNtssJk6cGFdccUUp6Lv66qtjjz32iNNOOy0iIn784x/HmDFj4rrrrotRo0bV1UsCAAAAwCqm0D3w7rzzzujZs2d85zvfiTZt2sQ3vvGN+PnPf16aP2PGjJg1a1b07du3NK1ly5bRq1evePLJJyMi4sknn4xWrVqVwruIiL59+0aDBg1i3LhxpTY77rhjNGrUqNSmX79+MX369Hj33XdLbZZfTnWb6uUAAAAAQG0odID36quvxo033hidO3eO+++/P4477rg46aST4uabb46IiFmzZkVERNu2bcvu17Zt29K8WbNmRZs2bcrmN2zYMNZcc82yNit7jOWX8UltquevzMKFC6OqqqrsDwAAAAA+j0KfQrts2bLo2bNnXHjhhRER8Y1vfCMmT54co0aNisMOO6yeq/vPLrroovjhD39Y32UAAAAA8BVW6B546667bnTr1q1sWteuXeONN96IiIh27dpFRMTs2bPL2syePbs0r127djFnzpyy+UuWLIl33nmnrM3KHmP5ZXxSm+r5K3PWWWfFvHnzSn8zZ878z08aAAAAAJZT6ABvu+22i+nTp5dNe+mll2KDDTaIiI8uaNGuXbt46KGHSvOrqqpi3Lhx0bt374iI6N27d8ydOzfGjx9favPwww/HsmXLolevXqU2Y8eOjcWLF5fajBkzJrp06VK64m3v3r3LllPdpno5K9O4ceOorKws+wMAAACAz6PQAd6pp54aTz31VFx44YXxyiuvxK233ho/+9nP4oQTToiIiIqKijjllFPiggsuiDvvvDMmTZoUhx56aLRv3z4GDhwYER/12Ntjjz3i6KOPjqeffjoef/zxGDp0aOy///7Rvn37iIg48MADo1GjRjFkyJCYMmVK3HbbbXH11VfHsGHDSrWcfPLJMXr06Lj88stj2rRpMWLEiHj22Wdj6NChdf66AAAAALDqKPQYeFtvvXX86U9/irPOOit+9KMfRadOneKqq66Kgw46qNTm9NNPjwULFsQxxxwTc+fOje233z5Gjx4dTZo0KbW55ZZbYujQobHrrrtGgwYNYtCgQXHNNdeU5rds2TIeeOCBOOGEE2KrrbaKtddeO4YPHx7HHHNMqU2fPn3i1ltvjXPPPTfOPvvs6Ny5c9xxxx2x+eab182LAQAAAMAqqSIzs76LWFVUVVVFy5YtY968ef9Vp9N2PPOeOl3eaxcPqNPl/beo6/cp4tPfK58boC4Ubd0HAADVPk9OVOhTaAEAAABgVSfAAwAAAIACE+ABAAAAQIEJ8AAAAACgwAR4AAAAAFBgAjwAAAAAKDABHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIEJ8AAAAACgwAR4AAAAAFBgAjwAAAAAKDABHgAAAAAUmAAPAAAAAApMgAcAAAAABdawvgsAAP67dDzznjpd3msXD6jT5QEAQF3TAw8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAosIb1XQAA8OV0PPOeOl/maxcPqPNlAgDAqkoPPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDAvlIB3sUXXxwVFRVxyimnlKZ9+OGHccIJJ8Raa60VzZs3j0GDBsXs2bPL7vfGG2/EgAEDYo011og2bdrEaaedFkuWLClr88gjj8SWW24ZjRs3jo033jhuuummFZZ//fXXR8eOHaNJkybRq1evePrpp2vjaQIAAABAyVcmwHvmmWfipz/9aXz9618vm37qqafGXXfdFbfffns8+uij8fe//z323Xff0vylS5fGgAEDYtGiRfHEE0/EzTffHDfddFMMHz681GbGjBkxYMCA2HnnnWPixIlxyimnxFFHHRX3339/qc1tt90Ww4YNi/PPPz+ee+652GKLLaJfv34xZ86c2n/yAAAAAKyyvhIB3vz58+Oggw6Kn//859G6devS9Hnz5sUvf/nLuOKKK2KXXXaJrbbaKn7961/HE088EU899VRERDzwwAPx4osvxv/+7/9Gjx49Ys8994wf//jHcf3118eiRYsiImLUqFHRqVOnuPzyy6Nr164xdOjQ2G+//eLKK68sLeuKK66Io48+Oo444ojo1q1bjBo1KtZYY4341a9+VbcvBgAAAACrlK9EgHfCCSfEgAEDom/fvmXTx48fH4sXLy6bvummm8b6668fTz75ZEREPPnkk9G9e/do27ZtqU2/fv2iqqoqpkyZUmrz8cfu169f6TEWLVoU48ePL2vToEGD6Nu3b6nNyixcuDCqqqrK/gAAAADg82hY3wX8J7/73e/iueeei2eeeWaFebNmzYpGjRpFq1atyqa3bds2Zs2aVWqzfHhXPb963qe1qaqqig8++CDefffdWLp06UrbTJs27RNrv+iii+KHP/zhZ3uiAAAAALAShe6BN3PmzDj55JPjlltuiSZNmtR3OZ/bWWedFfPmzSv9zZw5s75LAgAAAOArptAB3vjx42POnDmx5ZZbRsOGDaNhw4bx6KOPxjXXXBMNGzaMtm3bxqJFi2Lu3Lll95s9e3a0a9cuIiLatWu3wlVpq2//pzaVlZXRtGnTWHvttWO11VZbaZvqx1iZxo0bR2VlZdkfAAAAAHwehQ7wdt1115g0aVJMnDix9NezZ8846KCDSv9fffXV46GHHirdZ/r06fHGG29E7969IyKid+/eMWnSpLKrxY4ZMyYqKyujW7dupTbLP0Z1m+rHaNSoUWy11VZlbZYtWxYPPfRQqQ0AAAAA1IZCj4HXokWL2HzzzcumNWvWLNZaa63S9CFDhsSwYcNizTXXjMrKyjjxxBOjd+/ese2220ZExO677x7dunWLQw45JEaOHBmzZs2Kc889N0444YRo3LhxREQce+yxcd1118Xpp58eRx55ZDz88MPx+9//Pu65557ScocNGxaHHXZY9OzZM7bZZpu46qqrYsGCBXHEEUfU0asBAAAAwKqo0AHeZ3HllVdGgwYNYtCgQbFw4cLo169f3HDDDaX5q622Wtx9991x3HHHRe/evaNZs2Zx2GGHxY9+9KNSm06dOsU999wTp556alx99dWx3nrrxS9+8Yvo169fqc3gwYPjn//8ZwwfPjxmzZoVPXr0iNGjR69wYQsAAAAAqElfuQDvkUceKbvdpEmTuP766+P666//xPtssMEGce+9937q4+60004xYcKET20zdOjQGDp06GeuFQAAAAC+rEKPgQcAAAAAqzoBHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIEJ8AAAAACgwAR4AAAAAFBgAjwAAAAAKDABHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIEJ8AAAAACgwAR4AAAAAFBgAjwAAAAAKDABHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIEJ8AAAAACgwAR4AAAAAFBgAjwAAAAAKDABHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIEJ8AAAAACgwAR4AAAAAFBgAjwAAAAAKDABHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIEJ8AAAAACgwAR4AAAAAFBgAjwAAAAAKDABHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIEJ8AAAAACgwAR4AAAAAFBgAjwAAAAAKDABHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIEJ8AAAAACgwAR4AAAAAFBgAjwAAAAAKDABHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIEJ8AAAAACgwAR4AAAAAFBgAjwAAAAAKDABHgAAAAAUWMP6LgAAgLrX8cx76nyZr108oM6XCQDw30APPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgRU6wLvoooti6623jhYtWkSbNm1i4MCBMX369LI2H374YZxwwgmx1lprRfPmzWPQoEExe/bssjZvvPFGDBgwINZYY41o06ZNnHbaabFkyZKyNo888khsueWW0bhx49h4443jpptuWqGe66+/Pjp27BhNmjSJXr16xdNPP13jzxkAAAAAllfoAO/RRx+NE044IZ566qkYM2ZMLF68OHbfffdYsGBBqc2pp54ad911V9x+++3x6KOPxt///vfYd999S/OXLl0aAwYMiEWLFsUTTzwRN998c9x0000xfPjwUpsZM2bEgAEDYuedd46JEyfGKaecEkcddVTcf//9pTa33XZbDBs2LM4///x47rnnYosttoh+/frFnDlz6ubFAAAAAGCV1LC+C/g0o0ePLrt90003RZs2bWL8+PGx4447xrx58+KXv/xl3HrrrbHLLrtERMSvf/3r6Nq1azz11FOx7bbbxgMPPBAvvvhiPPjgg9G2bdvo0aNH/PjHP44zzjgjRowYEY0aNYpRo0ZFp06d4vLLL4+IiK5du8Zjjz0WV155ZfTr1y8iIq644oo4+uij44gjjoiIiFGjRsU999wTv/rVr+LMM8+sw1cFAAAAgFVJoXvgfdy8efMiImLNNdeMiIjx48fH4sWLo2/fvqU2m266aay//vrx5JNPRkTEk08+Gd27d4+2bduW2vTr1y+qqqpiypQppTbLP0Z1m+rHWLRoUYwfP76sTYMGDaJv376lNiuzcOHCqKqqKvsDAAAAgM+j0D3wlrds2bI45ZRTYrvttovNN988IiJmzZoVjRo1ilatWpW1bdu2bcyaNavUZvnwrnp+9bxPa1NVVRUffPBBvPvuu7F06dKVtpk2bdon1nzRRRfFD3/4w8//ZAEovI5n3lOny3vt4gF1ujwAAKA4vjI98E444YSYPHly/O53v6vvUj6zs846K+bNm1f6mzlzZn2XBAAAAMBXzFeiB97QoUPj7rvvjrFjx8Z6661Xmt6uXbtYtGhRzJ07t6wX3uzZs6Ndu3alNh+/Wmz1VWqXb/PxK9fOnj07Kisro2nTprHaaqvFaqutttI21Y+xMo0bN47GjRt//icMAAAAAP9foXvgZWYMHTo0/vSnP8XDDz8cnTp1Kpu/1VZbxeqrrx4PPfRQadr06dPjjTfeiN69e0dERO/evWPSpEllV4sdM2ZMVFZWRrdu3Uptln+M6jbVj9GoUaPYaqutytosW7YsHnrooVIbAAAAAKgNhe6Bd8IJJ8Stt94af/7zn6NFixalMetatmwZTZs2jZYtW8aQIUNi2LBhseaaa0ZlZWWceOKJ0bt379h2220jImL33XePbt26xSGHHBIjR46MWbNmxbnnnhsnnHBCqXfcscceG9ddd12cfvrpceSRR8bDDz8cv//97+Oee/49vtGwYcPisMMOi549e8Y222wTV111VSxYsKB0VVoAAAAAqA2FDvBuvPHGiIjYaaedyqb/+te/jsMPPzwiIq688spo0KBBDBo0KBYuXBj9+vWLG264odR2tdVWi7vvvjuOO+646N27dzRr1iwOO+yw+NGPflRq06lTp7jnnnvi1FNPjauvvjrWW2+9+MUvfhH9+vUrtRk8eHD885//jOHDh8esWbOiR48eMXr06BUubAEAAAAANanQAV5m/sc2TZo0ieuvvz6uv/76T2yzwQYbxL333vupj7PTTjvFhAkTPrXN0KFDY+jQof+xJgAAAACoKYUeAw8AAAAAVnUCPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAJrWN8FANS3jmfeU6fLe+3iAXW6PAAAAL7a9MADAAAAgAIT4AEAAABAgQnwAAAAAKDAjIEHAADwX6Sux/eNMMYvQG3TAw8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABRYw/ouAGpSxzPvqfNlvnbxgDpfJgAAALDq0AMPAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIEJ8AAAAACgwAR4AAAAAFBgAjwAAAAAKDABHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIE1rO8CAOCz6HjmPXW+zNcuHlDnywQAAPg4PfAAAAAAoMAEeAAAAABQYE6hBSiYuj5V1GmiAAAAxaYHHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACcxELAADqnQv4AAB8Mj3wAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGAN67sAAADgk3U88546Xd5rFw+o0+UBAP+ZHngAAAAAUGACPAAAAAAoMAEeAAAAABSYAA8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgQnwAAAAAKDABHgAAAAAUGACPAAAAAAoMAEeAAAAABRYw/ouAAAAgP9eHc+8p06X99rFA+p0eQB1QQ88AAAAACgwAR4AAAAAFJhTaAEAgK8kp2YCsKoQ4AEA1BFhAwAAX4QADwAAllPXQWuEsBUA+HQCPKhFeloAAAAAX5aLWAAAAABAgQnwAAAAAKDABHgAAAAAUGDGwAMAAD4T4/sCQP3QAw8AAAAACkyABwAAAAAFJsADAAAAgAIT4AEAAABAgbmIBQAAwJfkAh8A1CY98AAAAACgwAR4AAAAAFBgAjwAAAAAKDABHgAAAAAUmAAPAAAAAApMgAcAAAAABSbAAwAAAIACE+ABAAAAQIE1rO8CAAAAgPrV8cx76nyZr108oM6XCV9VArzP6frrr49LL700Zs2aFVtssUVce+21sc0229R3WQAAAHzF1HVoJjCDry4B3udw2223xbBhw2LUqFHRq1evuOqqq6Jfv34xffr0aNOmTX2XBwAAwKfQy4wvomifG8HvqskYeJ/DFVdcEUcffXQcccQR0a1btxg1alSsscYa8atf/aq+SwMAAADgv5QeeJ/RokWLYvz48XHWWWeVpjVo0CD69u0bTz75ZD1WBlB7HN0DAACofwK8z+hf//pXLF26NNq2bVs2vW3btjFt2rSV3mfhwoWxcOHC0u158+ZFRERVVVXtFVoPli18v06X92mvX13XElGseopUS0Sx6vmq1BJRrHpW5VoiilVPkWqJKFY9Raololj1FKmWiGLVU6RaIopVz1ellohi1bMq1xJRrHqKVEtEseopUi0Rn17P5uffX4eVREz+Yb9PnFe016ZIn5u6fp8iPv29+qqpfm0z8z+2rcjP0or4+9//Hl/72tfiiSeeiN69e5emn3766fHoo4/GuHHjVrjPiBEj4oc//GFdlgkAAADAV8jMmTNjvfXW+9Q2euB9RmuvvXasttpqMXv27LLps2fPjnbt2q30PmeddVYMGzasdHvZsmXxzjvvxFprrRUVFRW1Wm/RVVVVRYcOHWLmzJlRWVmploLWU6RailZPkWopWj1FqqVo9RSplqLVo5avRj1FqqVo9RSplqLVU6RailZPkWopWj1FqqVo9RSplqLVo5avRj1FqqW+ZWa899570b59+//YVoD3GTVq1Ci22mqreOihh2LgwIER8VEg99BDD8XQoUNXep/GjRtH48aNy6a1atWqliv9aqmsrCzMF7ZItUQUq54i1RJRrHqKVEtEseopUi0RxaqnSLVEFKsetXyyItVTpFoiilVPkWqJKFY9Raololj1FKmWiGLVU6RaIopVT5FqiShWPWr5ZEWqp0i11KeWLVt+pnYCvM9h2LBhcdhhh0XPnj1jm222iauuuioWLFgQRxxxRH2XBgAAAMB/KQHe5zB48OD45z//GcOHD49Zs2ZFjx49YvTo0Stc2AIAAAAAaooA73MaOnToJ54yy2fXuHHjOP/881c4xXhVryWiWPUUqZaIYtVTpFoiilVPkWqJKFY9Raololj1qOWTFameItUSUax6ilRLRLHqKVItEcWqp0i1RBSrniLVElGseopUS0Sx6lHLJytSPUWq5avEVWgBAAAAoMAa1HcBAAAAAMAnE+ABAAAAQIEJ8AAAAACgwAR4AAAAAFBgAjy+Eop+rZVZs2bF66+/Xt9lRETEsmXL6rsE4L9M0dfBwH8P6xtgVbV06dL6LiEiIhYsWFDfJfAJBHgU1gcffBDvv/9+VFVVRUVFRX2X84kmTJgQXbt2jZdeeqle63j99ddjzpw50aCBr3URvP/++7Fo0aJ6rwG+qIULF8a7774bEVHodXB9K1rYMGPGjHjkkUfqu4wy1a+RA0wfKcK6+cMPP6zvElYwbdq0GDdunPXNV1iR1of1WcvcuXPrbdnLe/fdd+Odd96p7zK+Eorw2X322Wdjn332qffwbOLEiXH44YfHq6++Wq91sHL29Cmkl156KYYOHRpHHXVU/OY3vynESnVlnn/++dhhhx3iyCOPjN12263e6pg4cWJ84xvfiIcffrjeaiiKIuwgvvzyy/GDH/wgbrzxxnrbSXr55ZfjyCOPjJ///Of1svzPqqjf7VXdSy+9FEcddVTsueeeccMNN9R3OWXmzZsXM2fOjKlTp9ZbDVVVVfHKK6/Ee++9V6iwYeLEidGlS5eYOXNmfZdSMn369DjllFNi7ty50aBBg3pZR0+ZMiV+9KMfxeLFi+t82R83ceLEOOaYY+KNN96o1xp22GGHmD17dr3V8HHPP/98bLHFFvH444/Xdyl8CdXrw/rcFlu4cGFZLXW9nfHuu+9G586d46KLLqrT5X7cP//5z9hll13ihhtuiLfffrtea/kk//jHP2LSpEn1WsMbb7wRU6dOjYqKinr93D7//PPxzW9+MzbccMNo1qxZvdUxceLE6NmzZ3Tu3Dk23HDDeqtjefYVygnwKJxJkybFjjvuGK1atYp99903jj322ELtIFWbNm1a7LzzznHCCSfE5ZdfXm9dnp9//vno06dPHHvssbH//vvX6bLnzZsXf/vb32LOnDn19qP3zjvvxMsvvxzTpk2LiIgGDRrUa/fzSZMmxS677BLvv/9+dOzYMZo0aVIvNey0007RrFmzQgSay3vvvfdi5syZ8fTTT8dbb731lfhRLsrpDHX1XlZ/ftq1axfDhw+Pww8/vE6W+1lMmzYtjjzyyDj99NPjf//3fyOi7ncUp02bFgcddFAMGTIkLrjggjpd9qepPqA0bNiwOOSQQ+q7nIj46Ddi4MCBce2118bJJ58c77zzTp2HeFVVVdGzZ88YMWJEnH766fW+g9azZ89Yf/31Y/3116+3Gvr06RN9+/aNtm3b1ksNH/f8889H79694wc/+EEMGzasvsspqe/fz6/C7+Obb74Z1113Xeyzzz6x2267xamnnhqvv/56NGjQoE7rnzlzZtxwww0xYMCA2HvvveOkk06K6dOnx5IlS6KioqJOa2ncuHGcdNJJMWLEiLj66qvrbLkft84668S2224bN910U9x8883xr3/9q95qWZkPPvggBgwYEGeffXZMnDixXmp4//3344wzzoh99tknpkyZUm8HmaZOnRrbb799nHnmmXHNNdfU23d/2rRpsd1228WFF14YF154Yb3UMG/evJg1a1Y89thjMXv27Fi0aFG9h6uFk1Agr7/+em644Yb5gx/8oGz6smXL6qmilZs4cWK2aNEimzZtmoceemh++OGHmZm5ZMmSOq1j2rRp2apVqzzttNNK0+rqtZo8eXL26tUru3Tpkq1bt84rrrgi//nPf9bJsqtNmjQpt9122+zYsWNutNFGefLJJ9fp8j/ulVdeybZt2+ZZZ52V8+fPX2mb2n5/XnnllVxvvfXyrLPOysWLF39iu/r4Tk2ePDn79u2bXbp0ycrKymzRokUed9xx+de//rXOa/lPpk+fniNHjsy5c+fWWw2TJ0/OG2+8MadOnVpny3zjjTdyww03zGHDhpVNL8I6+IUXXsg2bdrk2WefnU888URp+muvvVanNbRt2zbPOeecfP7550vTZ8yYke+//36d1bGyupo2bZrnnntu2fRHHnmkztfLy5szZ07utNNO2b1799x3331z8ODB+c4772Rm5tKlS+usjuOPPz633nrrbNq0aR588MH18nmeMGFCNm3aNM8+++yy6R988EG911BVVVVnNXzcxIkTc4011sgzzzyzbPpdd92Vr7zySp3X8/rrr+ett95aul2Xn9Nq06dPz7Fjx2ZmMda9n2TSpEnZvXv33HPPPXO//fbL/v37Z/v27XOttdYq/a7Xxes3adKk3HzzzbN///65zz775AEHHJAtW7bMTTbZJH/3u9/lokWLar2GzMyZM2eW/j9//vy87LLLsqKiIq+66qo6WX61f/zjH2W/Tz/4wQ9ygw02yMsvv7xefw9WZsyYMdm5c+c88MAD87nnnquXGu66667ca6+9cptttskXXnghM+v2e//CCy/kmmuumV/72tfKXoO6/u5X11FRUVH6na6r7061yZMn5y677JKbbLJJVlRU5DrrrJP9+vXLt956KzOLvT6sSwI8CuXnP/957rTTTvnGG2/8xy9pfX2Jx48fny1atMjTTjstb7nlluzZs2d+97vfrfMQb+LEidm8efOsqKjIk046qU53HqsDzJNOOikffPDBPPDAA7NFixZ5//3312kNzZs3z2HDhuW9996bQ4YMycaNG+evfvWrOqthecuWLcsRI0bk/vvvX7qd+dGG1BNPPFFnQcwll1ySAwcOzPfff79Uw5tvvplPPfVU3njjjfnss8/mwoULa72Oj5s0aVJWVlbmSSedlPfdd1+OHTs2zzrrrGzVqlX27NkzR48eXec1fZIPPvggu3TpkhUVFdmjR48cPnx4Pvzww2Vtavt7Pn/+/Fx//fWzVatWeeyxx+Y+++yTL730Ur777ruZ+e/PV02vB3/6059mnz598h//+Md/bFuX6+DXXnstO3bsuEKweNlll2WDBg3KdrhrS/UBpu9///tl00eOHJkbbLBB3n777fXy3Xr11VezWbNmecghh5RN//GPf5zNmjXLl19+uc5rWt7tt9+e66yzTn7/+9/PnXfeOQ844IA6D/FuuOGG/OY3v5n33XdftmvXrs5DvMmTJ2fz5s1z+PDhZdMvvPDCvOyyyz71YEtNefHFF7NRo0Z58cUXl02/8sor89RTTy1tw9SlN998M1u1apUHHXRQZv57nXLBBRfkBhtskJMnT66zWpYtW5YLFy7M4447Lrt165Y333xzaV5d7swvW7YsTzzxxKyoqMi//OUvpWlFM2HChGzevHmefvrp+fe//700fcyYMfnNb34zKysrc+LEiZlZu69f9bbgGWecUVbH66+/nptttlmut956ee+992Zm7b6OH3zwQW6++ebZq1ev0rT6CPEWLFiQW2yxRe67775lYVARQ7zqz8Wjjz6aG2+8cR5yyCH57LPP1tnyl9+Oe+CBB3LQoEG57bbblrbV6+J7X30A49vf/nbuuOOO+Z3vfKfsoHZdffeff/75bNq0aR500EG555575sYbb5xvvPFGZtbdfu3kyZOzsrIyhw0blvfdd1++8sorefrpp2fHjh2zQ4cO+eKLL2Zm/RxUKRoBHoVy8MEHZ+/evVc6r3olNn/+/NLGf12bO3duNm3aNE855ZTM/OgH++c//3n27NkzBw8eXNp5q+2V3YQJE7JJkyY5YsSIHDt2bDZt2jSPOeaYOgnxJk2alC1atCg7Wv7iiy/m6quvnieeeGKtLz8z86WXXsomTZrk+eefX1bDaquttkIPlLpc0R9zzDG55557lt7/22+/vXQkeN11181GjRrl3XffXat1HXzwwbn77ruXbv/hD3/I73znO7nWWmtl06ZNc6ONNsrrrruuTncI3n333ezTp0/pe7O83/72t9mpU6fcaaed8m9/+1ud1fSf/PCHP8wbbrghb7/99jz11FOzefPmeeKJJ+b//M//lLWrzc/X8OHDc7vttsuxY8fm4MGDc8stt8x9990377zzzrJeOzX5Xh5yyCG54447rnRe9XLqMqSqXubll1+ee+65Z9kO2qWXXprNmjXLvffeO1u1alXrId4NN9yQO+64Y1kNI0aMyNatW+f222+frVu3zv/7v/+r8xBvwoQJ2bp167Kdn0suuSTXWWedvO+++1Z6n9r8/lc/dnUo9c9//jOPPPLI/J//+Z/8xS9+kdtss00eeOCBdR7i9ejRI88777x85JFHsmXLlnnYYYfVyXpw4cKFufnmm2eHDh3ypZdeKk2/+OKLs0mTJnVy4OuDDz7IgQMH5hprrFHaKcvMvOiii7JZs2aloKiu/eUvf8lvfvOb2adPn5wwYUJmfhRqrrXWWnV+UKf6c/jcc8/l4Ycfnn369Mlf//rXK8yvVhufner1+ttvv53f+973skmTJvnQQw/9x+XV9Q7t1KlTc/XVV89LL710pfPHjRuXW221VW6xxRa1ur3+wgsvZGVlZZ511lmZ+e/XoXrdM3fu3Nxkk00+8Tetpv3lL3/JDTbYoGwbrD5CvDvvvDM33HDDPPzwwwsb4i1durTswMWRRx6Zq622Wh588MG13hNv+YMVy/9eH3HEEdmoUaPs1atXnfTEe/nll7OioqL0+b333nuzV69e+Z3vfCcfe+yxUrva/p2aNGlSVlRUlPapxo0bl7vssktuvPHGpV6ltb1f++677+aOO+5YOgOv+jkvWrQo77333uzatWt27tw533vvvVqt46tCgEdhLF26NI844ojcZZddMvOTu+2OGDEif/7zn9dlaZmZOXv27JwxY0apN0P1yuyDDz7IX/ziF3UW4v3rX//Kr33ta6UVfmbmfffdVwrxavt0nOOOOy4rKiry4YcfLv34Dh8+PCsqKnLIkCF52WWX5XPPPZevvvpqjS972bJluXjx4hw2bFiuvfbaOWrUqNK8Cy+8MCsqKnLQoEF544035m233Vb2o1dbP4DVp8ouWbIkzzvvvOzVq1f+6Ec/yqFDh2bbtm3zuOOOyzvvvDPnz5+fhx12WHbq1OkTT6/9MjVUf+5uvvnm/NrXvpbnnntuHnfccbnOOuvkCSecUNoZ2nfffbNPnz65YMGCGq3h07z00ku5+eab55NPPpmZH70Xy38/fv3rX2dFRUX+/ve/r7Oa/pPqXkMzZszIzI+OTn7/+9/PZs2a5YABA/K3v/3tZ+ql9kVUbzA++eSTOWjQoHz99dcz86PTIa+88sqsqKjIwYMH50UXXZSZWaO9d4477rjcbLPNPvXzccABB+T1119fY8v8LPbZZ5/cc889S7dnz56dRx99dD766KM5a9asPP3007OysnKFgLUm7bfffrnzzjuXbs+fPz9PO+200g724YcfnpWVlfnb3/62TnaoFyxYUNoReeyxx0o7bEOHDs0111wzH3zwwRXus/xpVbVh+vTpOXTo0Jw0aVLZjtGJJ56YO+ywQ2Z+9H3fdttt88ADDyz1KK3J12vSpEl52mmn5bhx48o29m+66aY86KCD8sMPP8zRo0dnixYtaj3Emz17di5ZsiTvv//+3GijjfLwww/Pf/zjH3nppZfmmmuuWSfh3VtvvZVLly7NBx54IPfcc8/cbrvtct68eXndddd9ag21+Rle/pTdBx54IPfaa6/s3bt3Hn/88dmmTZuVhne1eYDn2WefzV69epUOgk6ZMiUPPvjgFUK85XcqL7vssvzDH/5QYzU899xzecABB5QOELz99tt51FFHlYV4y78nCxcuzFNPPTXHjRtXYzV8FgsXLsxTTjklKyoqctKkSZn57+3d5b9LV111VbZo0aJWe1Huv//+WVFRkePHjy+9NtU1VNf0xz/+MRs0aJCPPvpordVRbcmSJfn4449np06dcp999ilNXz7Eu+aaa2q1hurnf//992enTp3yyCOPLOvVVh3iXXnllTl79uxareXjZsyYkf369Sv9blXXeuGFF2abNm3ykksuyQ033LBWT6d99dVX84gjjihtj1arXidfeOGFpdNpqz/ftbEuXLJkSd533335i1/8omx6fYR4f/vb3/InP/lJ2bLqOsR78803s2vXrvnwww+XXu/qf5csWZK//e1vs2XLlqWDBkXslVyXBHjUu+W/hDfeeGNWVFTkI488kpkffXmXn//222/n4MGD889//nOd1jhv3rzs1atXHn744Tl9+vTS9OqV2YcfflgnId6bb76ZS5YsKXUjzvz36zd69OhaDfGW3xEaOHBgrrPOOjlu3Lj8yU9+kpWVlaUeS3vvvXf27Nkz119//TziiCPyT3/6U43VUB3qvvLKK3ncccfltttumzfddFNedtll2bp16zzjjDPylltuyf79++fXv/713HjjjXOPPfaotd4Fs2fPzo4dO5Z6/sybNy/322+/3H777bNr1675xz/+sWwD6YorrsitttqqRntKvvXWW7nxxhvnH/7wh1y6dGm++eabec455+TXv/71/MY3vpF33XVXzpkzp9T+pz/9aXbr1i3ffvvtGqvhP3nkkUeycePG+cwzz5RNX/673bt37zz00ENXmF5XPt5rKPOjI8LHHHNMaSNi0KBB2aVLl9xnn32yT58+2bx58/zNb35T47Us34ugT58+pdOyMz86QtymTZs8+eSTs2PHjrneeuvlyJEjv/QGZvX9L7zwwmzSpEn++c9/Lq2/ln/sqqqqPPTQQ+tsHVz9vvTt2ze/853vlE1bPgSYOXNm9u7dO/fZZ59a+fwsW7YsDzjggNxtt91yyZIlpdfm4+Hp17/+9Rw8eHCNL//jXnzxxfzWt76Vd9xxR2l9/9e//jU32mijrKioyOuuu66s9szMc889N/v06VNr3/233347u3btmhUVFdmxY8f8/ve/nzfeeGNmfvQbucsuu+RvfvObXLp0aV5//fW544475re+9a0aHWdy3rx52apVq6yoqMj+/ftnr1698sEHH8w5c+bkrFmzcp111snbb789Mz/awV1rrbVy3333rZXPzDvvvJMdOnQoHZi4++67c/31188ePXpkq1atSqflL//9+sUvflGjv5nvvvtubrnllqWdnjvvvDN333337NChQzZr1iyffvrpzCxf515wwQWl6bVh6tSp2aNHj7Id1/vvvz/32muvXG211Uqf3eW3n84444xs1qxZVlVV1fh7VX0K5sfH0J00aVIpxFt+aI4PPvggjzvuuGzSpElOmzatxmpYffXVVxj/+e23384hQ4ZkkyZNSoF89am+J598clZUVJR6LtaliRMn5oEHHpjrrLNOKQhZfmc786PPXkVFRa3/Vmy//fa54YYb5qOPPrrSbe6XXnopV1tttbznnntqfNnV26Qf70127LHHZkVFRQ4YMKA0bf78+XnFFVdkRUVFab1Y0z5exyGHHJKrr756HnHEETl+/PjS9DPOOCNbtGiR119/fZ313pw/f34+99xzuc466+QOO+xQeu0uvPDCXHPNNUuh/SOPPJKdOnXKQw45pMbXQ++//36+/PLL2apVqzzggANKwebIkSOzdevWpaB89OjR2b9//7KewTVpxowZ+Ytf/KLUyy+z/Hfgvvvuq5MQb8aMGTly5MjSqe4fr2NlIV5tDPewbNmyfPTRR7OioqLU+ePjw8QsXbo0t9hii9JwC6s6AR715oMPPsgPP/yw1MNl6dKlOXv27Nx6663LNgqWN3z48OzRo0fZQLF15dprr83NNtssTzzxxLKNto+HeNtuu23uueeeNX4a1fz583PnnXfOXr16lXpwfbyXYm2FeBMnTsy999677Hl/61vfyoqKimzZsuUKp2lNnjw5f/rTn+b2229fY4NQP/vss7npppuWdj5fe+21POaYY3KTTTbJhg0blh1dXbhwYS5atCgvueSSPOCAA3LKlCk1UsPHvfHGG3nooYfmWmutVdpJW7hwYS5YsGClr/8JJ5yQ3/nOd2r8VOfddtst27Rpk3fddVdp2gcffLDSXlTHHXdcDho0qNZPt17+szl+/PisqKjIm266KTNXHmzvsMMOefDBB9dqTZ/k9ddfz9NOO63UG6h6A+WXv/xl7rrrrpn5UXDWtm3b0ndgwoQJee6559ZY74KXXnopTz/99NLt6hqefvrp3G677XLatGl5+OGHZ7t27Urjs7z77rs5dOjQLzzGWfU6eNasWTlv3rzM/GhjqUePHtm5c+f8y1/+UvocV29EDR8+PL/+9a+XnYZXm6qXe9FFF2WLFi1KYxllfvQaVc//4IMP8rvf/W5efvnltVbLiBEjskmTJqVebMtv6C5ZsiTff//9POyww/KSSy6p1RB64cKFue2225ZCqnvvvbfUo+Hpp5/OjTbaKA844ICynZ/zzjsvGzZsWKvjC82aNSvPOuus3GmnnbJnz5559dVX5yabbJL9+/fPkSNH5pFHHplnnHFGZn70el1xxRXZr1+/fPPNN2u0jmuvvTYbNmyYRx11VJ5xxhm5xRZb5O67756/+c1v8qyzzsr+/fuXvuv33HNPrr/++qUBsmva7rvvntttt11pXTxmzJhcb731sm/fviuEP+ecc042bty4xkKhzI+2G6qD0mp33XVX7r777tmtW7fSBWCq18kjRoyo1VBo4cKFecghh2RFRUVus802ZUFGdYi3zTbblIUN5513XjZv3rxWepp90kU9qn+/Jk+evEJPvJNPPjmbNWtWVuOXMXHixGzatGmec845K52/sp54xx9/fDZt2rTeBv3P/Cjg/O53v5vrrLNOPvXUU5lZ/tv+pz/9KTfddNNa+24tr1evXrnhhhvm2LFjV+iJd++992b37t1r/GJHb7zxRh5xxBFlB9QzPxq+YO21184rrrgiO3ToUHY67XvvvZfXXnvtCvf5Mv72t7/lwIEDV3je1b3afvzjH+cGG2yQhx9++Arfq7oaH3X8+PG577775quvvprPPfdcdu7cOXfZZZf8yU9+kmuvvXZpH2L5MfEqKyvz6KOPrrGxOSdOnJg9evTIBQsW5BNPPPEfe6w/8MADuf322+euu+6aCxcurLHf9BdeeKH0u7j8NntmeUBXHeIdcMABtdIJ4YUXXsiNN944DzrooLzttts+sY7qEK9r1641/h1afj+kOli98MILV9h/rq5n3333LR3IXdUJ8KgXL774Yh5wwAG5+eab5zrrrJN9+vTJyy67LBctWpSPPPJIdu3aNSsrK/Paa6/Nxx57LP/v//4vhwwZki1btqzzo43LBxG/+MUvskuXLp8a4l177bW588471/hOyZIlS/J3v/td9u7dO3fZZZdSiPfxoyGjR4/OysrKPPDAA2skxJs4cWLZ2HLLb6AddNBBpSPDy69wlz/NpCZUXzSjegy16sefMWNGfu9738vu3buXndK3fC21fQWlv/3tb3nCCSdkZWVl2ek0yy/37bffzrPOOitbt25dI2HiysYiGzRoULZq1apsfLTlA4Z58+blmWeemWuvvXatDwz+yiuv5Le//e2y5ey99965zjrrlKZVvz5LlizJhQsX5j777JNXXnll2fOrK1dddVVuuummeeKJJ5aCrOo6Nt9882zWrFm2b9++7ChlTfvzn/9cuiDN8st/8803c9ddd8311lsvN95441Ivxi/7Gr344ou533775RZbbJFNmzbN7t27549//OPM/OhUrs022yzbt2+f5513Xk6aNCl/+9vf5nHHHVc2MHltWbBgQb799ttlG+6PPfZYdu7cObfffvsVLiiybNmyPOecc3KjjTaqsdPs3nvvvZw/f35ZD9rXXnstt9hii+zYsWPpwNPyzj777OzUqVOdjOX4v//7v7nJJptkhw4dcsstt8zRo0eXXq+//vWvueGGG+Z3v/vdfPHFF0vBY00FDh83c+bM0us0c+bM/PGPf5xbbbVVnnfeefnhhx/mBRdcUDrVraKiouygXXWQ9mW9+eabZY911VVX5WqrrZa33XZbTpgwIW+99dbceOONs3379tmyZcuyg4C1MZxA9ffz1ltvza5du5YdYHrggQdy/fXXz8MOO6wUBp9//vm5xhprrNBLuSZqePzxx7Np06ZlO2l333137r777tmnT5/SWQXnnnturX5Oql122WXZqlWrPOGEE7JPnz7505/+tDSvOsTr2bNnTp8+PS+77LJs0qRJrQTPkydPziZNmuSFF15YNv3yyy/Pk08+ubStU3067Q477JC9evXKNdZYo8ZeoxdeeCFbtmy5QoB4/vnn549+9KPS7eqeeM2bN8/dd989mzdvXuvv0/Lmzp2br776aj722GNl2+Avv/xyfuc731lpiHfyySfnoEGDyn5Tv6x33nknX3zxxbz77rtz6tSpOWvWrNK8bbbZphTiLb+detJJJ+Xee+9dY3VUf6/+53/+J7fYYoscPHhwaWzL6t5k1WHQY489ll/72teyf//+K9y/pjz77LPZrFmz3G233UrTPt6rbfTo0bn++uvnkCFD6vyU6wkTJmSjRo3KLkA1fvz47NGjR1ZUVJR6Rn78tMnHHnusxgLGiRMnZuPGjcuGHnr88cdX2mN9+c/Oww8/XKMHK6dOnZprrrlmnnnmmZ94+vLy+3SjR4/OLl265BFHHFGjB91ffPHF0llLy4/pu7zlX4enn346t9pqq9xqq61yyZIlNfIZfvPNN3PQoEFlwyXssMMOudFGG+VTTz21QiC9aNGi7N+/f+kiTE6hhTr2wgsvlK6seNVVV+Wtt96a2223XbZu3Tr33XffXLhwYT799NM5ePDgXG211bJp06a5ySabZL9+/UrjEdSFt956K+fOnbvCSnPUqFG5ySab5NChQz8xxKvJU4KWt3jx4vzjH/+YW2+9de66666fGOLdddddue66637pMbo+6ej0v/71r9L/99prr1xnnXXy3nvvLYUyNXmFzE+qofq5V/fE23bbbcsGCK7tq/ot/9xmzJhRCjf++Mc/lrW74oorSmPf1fTR8o8/x29/+9u55ppr5p133lk274YbbshBgwZlx44d6+SI/VtvvZWrrbZa9uvXr9Rb7J577smNN944119//bIdj8WLF+f555+f7dq1q7Hemp/X4sWL8+KLL85tt902jz/++LIN/d/+9re50UYblfX8qq0a/vCHP2TTpk3zhBNOKJt3yy23ZMOGDWvs1LrqdfBxxx2Xv/zlL/NXv/pVfvvb386Kioo85JBDcs6cOfnWW2/lHnvskWuttVZWVFTkhhtumP3796/1dfCUKVNyr732ym7duuWee+5ZFjrcfPPN2a5du9xss83yhhtuyFdffTX//Oc/51FHHZWVlZU19tmeMmVK7rnnntmjR4/s0aNH6Sj5kiVL8g9/+ENuuOGG2aFDh/zlL3+Zzz33XP7xj3/MY445pkZr+CTVG7XTp0/Pww8/PO+///7cbbfdsmvXriuEeF26dMn27dtns2bNaq3n3XvvvZff+ta3cuutty793lSHeJ07dy6N05j50VXm77zzzrLnURMWLlyYXbp0yR133LFswPxLLrkkGzRoUBpzau7cufmHP/yhVk6l+yQLFizILl26rHCF4Ood6mOPPbbUu6q23qPZs2fn7rvvnscff3zZTll1iLfLLrvk0UcfnU2bNq3VHprVv0mLFy/OHXfcMY888sj83ve+l1tuuWXZuMb3339/fvvb386mTZvWWq/RBQsW5N57752rr7562fSLLrooW7VqVerpVu3FF1/MffbZJ9dbb70aO4i8ePHi3HTTTbN58+ZlO9EXX3xxNm/efIXP6dtvv52HH354rrHGGnXa827y5Mm53XbbZffu3bNBgwbZsGHD3HvvvXPs2LGZ+dFBzI+HeOecc06uvfbaNXr2w6RJk3K77bbLTTfdNCsrK7Nx48b5rW99q3RKfOZHPfE6depUqm3EiBE1XsfyoeFvfvOb/OY3v5mHHnponnrqqSu9cNDjjz+eTZo0yX333bfGasjMnDNnTmlbfNy4cbn++uvngAED8qKLLirr1bb8mHjNmzfP448/vs6uOD116tRs3rz5CqHLsmXL8plnnsnNNtsst99++1I9Hw9taqqGZs2alTohLD8G87PPPlvqsb78wZPa2H/44IMPcr/99svjjjuubPrixYvzzTffzBkzZqz07KoHH3xwpQcMv0wdBx54YA4dOrRs+vvvv59vvvnmJ65vx48fX6M98B555JHcfvvtc4899iiNw/rKK6/kBhtskD169Mj777+/9DosXrw4zz333HrdTygaAR51avbs2dm9e/eyK5hmfrSyOvPMM3OdddbJIUOGlFaeL774Yj711FP5xhtvlI13VNteffXVrKioyHbt2uWAAQPy6quvzscff7w0//e//31uvPHGedJJJ5VtGNT0WBJz5swpG3Mv86OdlT/96U+55ZZbfmpPvC97oYQXX3wxmzZtWnYUOPOjgV5/8pOflI2Jt9dee2X79u3zjjvuqNEfvilTpmTjxo1LP/7VLrvsshwyZEjpR786xNt+++3Ldhhr2ptvvpm/+tWvcr/99svBgwfnmWeeWer18eabb5ZCvOqg5f33389TTjklTz/99Bo7kjhz5sz82c9+lgMHDsxdd901L7744rLTzZcP8RYtWpSzZ8/OK664Is8+++w6+eGr3jCaOXNmtmnTJnfeeefSmBa//e1v8+tf/3pWVFTk3nvvnQMHDsxBgwZlmzZt6rQ3wcosWrQoL7jgglKIVx3CT506Nb/2ta/ltddem5m1e9Rv4cKFedttt60Q4s2aNSsHDBhQ6iH3ZdYzs2fPzh49epROZaw2Z86cvP7667NJkyZ57LHHlqbPmDEjn3766fzXv/5V6+vgiRMnZsuWLXPIkCF51VVXZceOHXPTTTct21n9wx/+kP369cvVV189mzVrlhtttFHutttuZWPJfBkTJkzIFi1a5EknnZQ/+clPsm/fvtmoUaNSr4UlS5bkmDFjcsCAAdmgQYNs0KBBdu7cOXfbbbdaDTc//PDDFXoTH3jggaWB0nfZZZfs3r17WYg3duzY3HLLLWv1whXLli3Lm266KXfeeefcbbfdSiHeW2+9lT/+8Y9z0003XeGzVhuefvrp/NrXvpZ77bVXWYhXPXB8bZ5aXe3j38vq9+u2227LDh065F//+tfMLN+hbtasWY2eCrn8aeXLh3U33HBDNm7cuOwquJn/HjC9Jk8J/bjlt0WqL0R14YUX5mmnnZbTpk3LIUOGZI8ePcpCvLvuuisPOOCAWvtOLV68OO+99978xje+kX369MnMzGuuueZTL+oxbdq0Gj8ddNKkSbnmmmvm3nvvnfPnz8+LLrroU2uYO3dunV54YPLkyVlZWZmnnnpqPvbYY/nMM8/kqFGjsn379rnRRhuVAvmpU6fm4MGDc7311ssDDzwwmzZtWqOfpylTpmTLli1z2LBh+fjjj+ff/va3vPrqq3PTTTfN9u3bl11oZNttt82uXbvmwQcfXON1TJgwISsqKspOt7zpppuyT58+ufrqq5fGdfz4uuCpp55a4bv3ZYwfPz7btm1bWqdUL6Nbt25ZUVFR+vxUjyNevU546KGHarSOT/PCCy9k69ats1GjRqVQaOnSpWWvzfjx47Nz587Zp0+f0hklNbkfNXHixFxzzTWzoqKidDGwzI/WQ9XLefzxx0s91mvzAMaHH36Yffr0KVvPjR49Ok866aRs0aJFbrjhhtmvX7/S97u2xiZcvHhx9urVq+yiFffdd1+eeOKJ2apVq2zVqlV++9vfLh3Irs0LVzzwwAM5YMCA7Nu3b+k7Vd0zskWLFrn11lvn4MGDs3///oXYTygSAR516qmnnsqtt946X3rppdJKoXoj9/3338+jjz4627Rpk0888UR9lpkvvPBCrr322llRUZHHHXdcbrDBBrnxxhtnt27d8pxzzslJkyblRRddlNtss01+//vfr9HxLKq9+uqr2aJFi2zdunXuuOOOefXVV5edhnPfffdlnz598pvf/OYnhnhf1IIFC3KHHXbI9ddfv2zn7+KLL87GjRuXVrTLL2+HHXbIzp0719gVVt9///086KCDsqKioiw4uOiii7Jly5ZlAzpnfjSO2YEHHph9+/Yt24GrKZMmTcoePXpk3759s2/fvrnLLrvkGmuskd/4xjdK78sbb7yRxx9/fFlPvMWLF9fYeISTJk3KzTffPPfee+/cbbfdcvDgwdmgQYPccsstyzZg991331xzzTXz7rvvzsx/n6Zam1Y25t4bb7yRbdq0yZ122ql0BPGll17KCy+8MPfZZ5/s379/XnDBBXW2QVnt7bffzueffz6vvfbavOeee0obBYsWLcqf/OQnue222+Zxxx1XCmcvueSSXHPNNUu9CWuqhokTJ+YNN9yQv/71r/Of//xnad5tt92WTZo0yeOPP7407Qc/+EG2bdv2S59GMW7cuNxss83yxRdfLNuwz/yoN9VFF12UFRUVeccdd3yp5XxeU6ZMyRYtWuR5551XmvbHP/4xKyoq8n//93/L2v7jH//ISZMm5R133JEvvfRSjZ2GOXXq1GzUqFHZAYNbbrklV1tttdL4jct7+umnc+zYsfnGG2/U6GliHzd58uTcY4898pxzzin7DP7zn//MbbfdNh977LFctGhR9u7dO7/+9a/nAw88UArxaqunxeuvv162Dv7tb3+bO+yww0pDvK5du37i+F5fxt/+9reyAyMTJkzINm3arBDiXX755bV+9cfXXnste/bsmb/73e9WOOg2efLk3HjjjfOyyy7LzPKQ7fHHH6+x9d/f/va33GmnnfKyyy4ru3BR5r9/04877rhcuHBh2c7hAw88ULZzW5OmTZuWW221VZ5yyin56quvlr4nzz77bLZo0SIfeeSR/Pvf/55HHXVUbrnllvnLX/6yrOaaVn0qaPXv0ZgxY7J79+7Zrl27bNWqVakH2fKuuuqqGh2X8N13381XX321dECtet3XsWPHXGeddfKBBx7IzBUvblIdltWVefPmZd++fVe4uEfmRz1lqi/IUv19nzx5cg4cODBbtGhRozva7733Xu62224r9BjK/Kh30nbbbZddunQpG+x/yy23rPGxHKsvdlLdAWH5386bb745t99++xw8eHDp+19bAUx1Hd///vfLpi9dujSffPLJ0gGl5U9J/fhvfW2rHtdx//33z4MPPji32GKL0nbyx2sZP358du3aNbt161ajQ95Un71zyimn5C677JKdO3cu63Dx8RCvS5cuueeee9b4ME3V+7oLFy7MTTfdNA888MB8/fXX84ILLshNNtkk99133/zpT3+a1113Xfbs2TPPPvvsFS7gWJPmzJmTe+65Zx577LH55JNP5sUXX5ybbLJJHnDAAXn11Vfnfffdl2uuueYKF9SpCQsWLMh33nmnbH/k/vvvL4V41WP9zZ8/P88666z87ne/m3vvvXdecMEFdTZe41eFAI869Ytf/CKbNm26wimm1SvROXPmZOvWrXPkyJH1UV6ZCRMmZLt27XLIkCH56quv5tSpU3PYsGE5YMCAXGONNXKPPfYojelz2mmn1fhYa2PGjMm2bdtmt27dcsstt8y99torGzdunNtvv30ef/zx+eCDD+Y111yT3/zmN3Ovvfaq8Y3d3//+99m7d+884IAD8tVXX80rrrjiU48MZ2aNjhWxbNmyfOSRR3LHHXfMbt26ZWbmddddl2uuuWZpA/fjXn/99S992vDKVI/Bd8YZZ5R2dKpP9d50002zS5cupaDzlVdeKV0driavvlZdw2mnnVbWC+Chhx7Knj175uabb152KsngwYOzoqJihdM5akP1FTF79eqVO+64Y9lVLpcP8Zb/Aa6rUzg+7sUXX8w99tgjN99886ysrMymTZvmWmutlTfccENmfrRzvXyIN2/evHzuuedy2223rbHeFy+++GL27ds3e/bsmWussUY2a9Ys27Rpk9dee20peKjuiVfdG+4f//hHduvW7UvvaP/0pz/Nli1blm5/fCdj6tSp2apVq1KPw7pQfVGGNm3alB0wOPvss7OioiKvvPLKvO+++2r1whnvv/9+Hnroodm4ceOyGqoH9T/22GNLPVDq0vIXrPj617+eLVu2zB/96Ef5u9/9LjM/6oVXvUO5aNGi/OY3v5kdOnQonQJYGzsBH3zwQR5xxBHZpUuX0vg1nxbiXXjhhdmuXbsVenN/GUuXLs1vf/vb2axZs7LA7JNCvCuvvDIrKipy1KhRNVZDtffffz8feOCB3GeffXLttdfOLbfcMocPH56vv/566fUfMWJErrXWWmVX2KvJ92bevHk5c+bM3HnnnbNHjx65zjrr5MiRI8vOHDj33HNzk002KW0r1PYwE8sfhFtttdXysMMOyz322KN0gPayyy7Lo446KjMzn3/++Tz22GOzU6dOtXJl78yP1rt77bVX7rnnnjly5MjS1aQfeOCB3HHHHXOTTTYpta1+baq//zU1bmz170///v3ze9/7Xul3cNq0adm2bdvcaqutVhg/+dxzz82KiooVguHaNnPmzNxkk01Kp/IufzXIzI+Cl8aNG+eIESNK95kyZUqNb4PNmjUru3fvXnahg+V7Bd11113ZvHnzFfYbavL3ojoM+vjZQ8v3/L7pppvym9/8Zu63336lUL6m17/PP//8SoeUqX6uy5Yty6eeeio32GCD3GWXXWrllNT/ZPLkyVlRUVE6GPfXv/61NN5u9anNH1//jRs3LrfaaqsaO1V0+vTpudZaa5UuDPbuu+/mdtttl507dy7rcLF8iPfoo4/mN77xjRodv/zVV1/NCy64oBTWP/TQQ9miRYtcf/31s7KyMkeNGlW2XbzTTjvVylVWZ8yYkUceeWTp9s0335ydO3fO9ddfP1u3bp0///nPy8buHTx4cH7729+u0Rqqh0fZbLPNVrgwx/I98T5tH5N/E+BRp/7v//4vmzRpUtqA+/jOY/URiuqxCurax+t5+umns1WrVrnffvuVHTF48skn849//GN+61vfyi233LJWeuBlfjSwfZ8+ffJ73/tePvbYYzl9+vS8+uqrc6uttsott9wymzVrlp07d86KiooauYLnBx98kPPmzSv9sN5111259dZbZ48ePbJ58+al7vrLv07XXXddaWeyJixYsCD/9a9/lTZsx48fn7169crWrVtnixYtVroDffHFF6/0yHlNmDRpUjZv3ry0MfLx8f2mTZuWHTt2zD322KN0n1deeaV0elBNmDp1aq6xxhp5/vnnZ+a/j+hV//vII4/kBhtskAMHDizbaT3ssMNqtOfAykyYMCFbtmyZRx99dF566aXZo0eP3HHHHTPz35+T6hBv5513XqEXW10fFV5rrbXylFNOKW08jB49Og8++OCsqKjISy+9NDP/3RNv++23zyOPPDKXLFlSY728Jk6cmGuvvXaeeuqp+fjjj+d7772Xzz//fH73u9/NBg0alJ2e/vvf/z5btmyZhx56aGZmjZzCOnbs2GzYsOEKYzUur1u3bisc3a9t48aNy65du+agQYPyb3/7W44cOTKbN2+egwYNygsuuCDbtGmTffr0yR122CGvvPLKWum1ef/99+d+++2XW265Zc6YMSOvu+66bN68eR577LH5gx/8IHffffdcY401/l979x6X4/3/Afxz5VQpp9IKoQjpXDpJsqIyURFiDpljFmkmC6GlsDkMY7M5beNrc5hpm9OYw3LaJh1EGBKjk+PQSvX6/dHjun73dd93OV3X3Z29n4/HHt+vu5v7032dPtf7+rzfb4wYMQJRUVGy1TpV9scff6Br166IiIjABx98gPfffx8dO3bEhAkTMHXqVNSrV09Ioy8rK0NgYKDsjTQOHTqE8PBweHh4iGotqQvi5eXlYcmSJZKn8N++fRsBAQFo27at6DxXXRDv008/lfxa/ccff6BXr17CTd/hw4eRkJCAZs2awc7ODkOGDMHly5dx8uRJ9OnTR2i2JOXqnLNnz6J79+7Cze/Fixcxc+ZM2NjYwMjICOPGjcOpU6fwzz//wMrKSrTKVW4///wzxowZA0dHRyQnJyM5ORmtW7fG2LFj4efnB09PT6GmWEZGBqKjo4Ugp5SysrJgZGSEuLg4UdohUBWsO3DgABwcHODp6SnM9WbPni1pUw8+XfaDDz5AWlqacP3mg4UXLlxAkyZN0L9/f+H45ZubyJnaV53ffvsNBgYGwookxYBveXk5ysrK0KdPH4SHh8u22gyoCgg1bNhQ5WGk4txh0KBBePPNNwFA8myDy5cvQ0dHRwgQKnZGVw64btq0CX5+fggICJD8fMc3XVHsVg9UdYbv1KmTaI5y6tQpdOjQAS4uLhqdY/31118YNmyYqB41ULUvDR48uMYgnlQPda9evQpbW1tRYzvg+YJ4UmXL8D755BO0bNkS8fHxQuOk27dv4/jx46JV0nxpgaFDhyI+Pl7yBzzfffcdzMzMMGTIEOG1c+fOISsrS1TTHKg6tkNDQ0UNP14VXx5l4sSJWLFiBWxsbDB8+HDRe/bu3SsE8ZRrkP7XG1aoQwE8olH//PMP2rRpI9TtASBaKlxcXIwePXpg69atADR30Kqr78RPrn7//XcYGRkhJCRE5Sa+oqJClrpQik8Xv/32W7i6umLYsGGii86lS5ewceNGvPPOO3B0dHzlOkfnz59Hv379YG9vD2dnZ/zyyy8Aqibg9vb28PX1VanTM2fOHOjp6UlWIDg7OxtBQUHo0qULevXqhQ0bNgCouknq27cvWrduLdw0Kz8hl6MzZkVFhdA9sbpOTZWVlVi9ejUaNWqk0pxBCmVlZQgNDYWJiYmwTfixKVq3bh3q1aun0S7NmZmZ0NPTEwKLQFV3zIEDB+L27du4c+eOcHzk5eWhdevWcHFx0XjKLFB1c9i4cWO1qXzXrl3D5MmTwXGcKPU5Pj4evr6+oqLVryIzM1OUJqq8DUeNGgV9fX1hhWlpaSm++eYbmJqaSlb3KCcnB2ZmZhg4cKBoRQdfm4ZPy1RsHiEnxe/g1KlT6NixI6ytrVUKyefn5+PUqVMIDg6Gp6enLDf5QFVKVkhICFq1agVdXV3R8VRSUoJjx45h0qRJsLW1lTVIdu3aNezYsUMI5p48eRIdOnRAREQEjh07hvz8fIwdOxYhISHgOA4nT57U+CT3yJEjCAsLqzaI17dvX+HmVuo6Ovx+k5+fD19f32qDeKGhoSo3KFJJT09H48aNRZ2jeYWFhUhOToabmxtMTEwwYsQItGjRAr6+vpKPoUGDBmpvtrKzs7F161Z06dIFVlZW8PT0xJtvvglvb29ZVqrzrl69KkpX3rdvH4YOHQonJyfk5eUhJycHa9euhaWlpaheFyB98AWoumG2tbXFlClTRK8rzj352pYODg7o1asX4uLiJG3qkZ+fDwcHB8TExIheV65XeP78eTRp0gRDhw7FlClT0KhRo1oJ3gFVY1ZOpVM+x4SGhiIoKEi2MVRWVuLq1auiB5jqznNDhgxBv379ZBnDihUrVFLwk5OT0axZM+FarXh+++yzz9CvXz9JV3Lx/y7Hcdi4caOwinbhwoUwMTERyqUoSk1Nha2traSNB55l5syZQsYMIL6+P08QTwpLly5Fr169hD8r/vvVBfHUvVcqH330ETp37oxZs2ZVu8KwrKwMc+bMQevWrWWZHz9+/BhbtmyBjY2NaGWdukU0s2fPRqtWrSQbR0ZGBgwMDETXqO3bt8PHxweXL18WzUP37NmD4OBguLq6CvsIUY8CeERj+BPF559/jvr162Po0KEq9dJmz54NCwsLWVOllP31119o3rw5bGxs8NNPP6kths4H8QYNGiQEkKQ+0fM1sPjvSXEiu337dri4uGDEiBGihgX8OF510ss/HYmIiEBsbCw8PT1hbGyM33//HQDwww8/wM3NDeHh4cLnz507V9IJbnp6Opo0aYKRI0ciISEBTk5OsLCwENJCU1NT0aNHD1hbWwvBDKmfkKtTUFAAd3d32NjYqFzQ+H2ATxlQrFEopbS0NPj7+yMwMFBUm0yxm9bFixdhYGCgdiInh4KCAlhZWcHNzU30+tSpU9G0aVNYWlrCyMgI0dHRwgTy+vXr6NSpk0YnlEBVvTlLS0u4uroKrykXUz537hycnZ0RHBwsPAkuLy+X7Ob/zp074DhOWCXAU3z6W1FRAWdnZ/To0UOUrvSqDwmUz1UbNmwAx3GIiIhQCcrHx8fDwsJCtppYvIcPH6KwsBCnT5/GvXv3hEDV6dOn0blzZ3Tv3r3awv5S1Zt7+PAhCgoKcPbsWVEay7FjxxAcHIyOHTuKxsAH5cvKyiR/Uq9s4sSJaNWqFbZu3SpcJ0+ePImOHTsiODhYOBc9fvxYaLAhp+vXr+PMmTMqaeSHDx/GwIED4e7uLnRqrqysxHfffQd7e3uEhoaivLxckutlXl4eduzYgZKSElHJiuLiYiF1WDGIl56ejnr16mHYsGGSrxDiazwpp7Ip4n/nlStXYuzYsULJDamC8dWNQTk4V1RUhN27dwudpo2MjGQLapaXlyMhIQEtW7YUav4BVSVBBgwYAGdnZ+F6XVRUJKyolzP4vG/fPri5uVX7oFHx/HvkyBF06tQJ9erVkzRwdvjwYdjY2CAjI0Ptvqh4HeDnExzHabTbrLJ//vkHo0aNgrm5ufBQnVdRUYGysjIEBwfL2jiMN3HiROjr6ws39YpZCJWVlcLKJUC6fenixYs4ceIEysrKkJycjCZNmuCLL77AmjVr0KJFC6F8gDpSrs6+fPmyMLecOXMm6tevj2+//RaJiYnVlpThVx7LfZ1S9ssvv8De3l70uYrbgw/iubi4qKyyknIMzZs3rzZYxgfxunbtKmvjKUVJSUlCEE/5/varr77ClClTYGJiIukDeOXj4NGjR/j666/RtWtXhIWFqbx/586dmDhxIt544w3Jzjv37t2DiYkJfHx8RK9PmzYNpqameOONN2BhYSHq0Pzjjz9i6NChss9B6zoK4BFZXLt2rdoVEkVFRUhKSoKhoSFsbGwwbdo0xMfHY8SIEWjWrJnGJyx8Csw777yDkJAQuLi4YNasWbh48aJKjQYTExP06dNH8oLlV65cgY6ODiIjIzF37ly16Xrbtm1Dt27dMGLECEknlnyKqGLa8sOHD9GmTRsMGjRIeG337t1wdXXFmDFjMGLECOjq6ko2DuUUUQC4desW2rRpg5EjRwKouhidOHEC3t7ecHJyQnR0tKQBxJoUFRWhW7dusLGxUVtIdceOHbCxsVEpHC4Ffh9MT0+Hr68vAgMDRbX1+En/Dz/8ABsbG40Fx/Lz8zFq1Ch4e3sLN2pLliyBoaEh1q1bh4yMDMTExEBXVxdbtmwRxilnR6vqFBQUIC4uDkZGRqLUDuUbqbi4OLRv3x4lJSWyjHPKlCkwMDDA+vXrVR5e8J8XHx+Prl27vnLK7s2bN0UpR8pPuj/55BNwHAdra2tER0dj9uzZGDVqFJo3by77OfjcuXMICAhAp06d0KBBA7Rq1QrDhg0TJtx8oGrQoEGi4JSUNbvOnTsHX19f2NraguM4NG3aFMOGDRPSnlJTUxEcHAwnJydhDHIWluZlZmZi2bJlAKrSwuzs7LBlyxZhf+FXKYaGhoq+GznHdePGDSGgYGZmhujoaKxYsUIILOfk5GDw4MHw8vISHiBUVlZi586dkp2Pbt++DVNTU3Ach27dumHw4MHYtGmTEJQpKCjAoEGD0Lp1a9ET/czMTMlrhymXVeDNmjVLdCOiuE1KS0uRlpYmWSFu/po5d+5c0WclJSUhKipKCIgrn+P2798v241RWloavvrqK1y9ehWzZs1C586dRXXJ+NWtTk5OQsMBTRTXT0pKQqtWrdQ2AOI/+9GjR0Kdu0OHDkn+HS1cuBDGxsYqn6s8Bj4wf/XqVY2uVL9x4wZ27NiBWbNmITk5WchoyMrKQocOHWBjY4O1a9cK7y8rK8PcuXNhYmIi6Thv3LiBTZs2ISYmBvPnz8fu3btRXl6Oq1evwsXFBY0bN8aPP/4onHseP36MefPmwdjYWNJxlJWVYfjw4XB3d0dZWRkKCgqQlJSEZs2ageM4IQiteE2aPn063nvvPcnGwOvXrx86d+4s/HnmzJngOA4NGzZU29gkMTER06ZNq5U6wxcuXICenp5Kmrri/p6amoqAgAD06NEDT548kez45x9o5+TkqDzMUXb//n3Y2NjAxcVF0lW/2dnZiIyMxP79+1U+f9GiRejYsSNmzZolnF+ysrIQGhqKgQMHSlreIScnBzNnzsSXX36Je/fuCQHlJ0+e4H//+x+6dOkiulalpqaib9++CA8Pl3Qc9+/fR1xcHBo2bCjUNl24cCEMDQ2xZcsWHDlyBAsXLoSBgQESEhKEvydHA6PXDQXwiOQqKirQu3dvmJmZVVsD4v79+zh69Cj8/f1hZ2eHbt26YdKkSbLVkqvJ33//jTfffBM7d+5EaWkpUlJS4OLiguDgYLz99tu4cuWKUJQ/NTUVHTp0EGoZSOXYsWPgOA7Tp0/HmDFjYG5ujiVLloiKfALA1q1b4eHhgQEDBkjypEZdiig/IQkODsbo0aNFF7eUlBRYWFhIGmhVlyLKj2H06NEICQkRTUROnjwJV1dXUWt6TVAM4ilPFKdMmYLBgwcLN01S4yc4Z8+eha+vLwICAlSCeO+++y4GDRok2xjUjefmzZuIjIyEl5cX/P39YWRkhCNHjoje2759e4wfP17l72rCpUuXhOOkoKAACQkJMDQ0rDaIFxMTAy8vL8nHoFgjcvr06WjYsCHWr1+v9oYyJiYG7u7urxRALC0tRUhICDw9PfHjjz8Kryt/93v37kV4eDgsLS3h7u6OSZMmSdppV52srCw0bdoU06ZNw65du5CZmYkJEybA3Nwcbdq0EQIcfBBvyJAhou6CUo2hSZMmmDZtGvbv34/ffvsN8+fPh7GxsShg9+uvvyIkJARubm6Sj0Gds2fPolGjRkJQBqg6D1cXxJPju1Hnzp07cHZ2hpmZGUaNGoX+/fujS5cuMDc3R8+ePfHtt98iISEBERER8PLyqrbJ0Ks4f/48hg4dio4dO6Jnz5748MMPYWJiAgsLC/Ts2RNJSUk4cOAAnJycZK1Lq3jNVJwHLFq0CMbGxio31MqF/6WgeM1UTD9duHAhGjdurHZlkNRNtpSlp6cLDb2Aqq68M2fOrDaIp6ljCqh6sNSyZUvhplDdtkhOTsaCBQtkG8PGjRvRvHlzZGRkVHsNTE5Oxrhx42TfVsoyMjJgaWkJX19ftGvXDq1btwbHcRg/fjyKi4uRlpYGFxcX6OnpwcvLCwEBARgwYABMTU0lzX7IyMgQjudu3brB2NgYRkZG6N+/P8rLy5GZmQk/Pz9wHCesVA8MDISZmZksWRirVq2CqampcB/z999/46OPPkLTpk2xfPly0XvnzZuH+vXry1KLef/+/bC0tBTN+RYtWgSO47Bu3TrRnG/u3LkaXbl57949lQCYo6OjMO9QF6gGqq7vUqUY37lzR2XVsbe3t1ADr7rj7f79+5I1zQCqgmOdOnUCx3FwcHCAoaEhRo0ahblz5woP9z/77DPY29tj9uzZwvUjPz9f0hWbDx8+FB5M8s2vPD09sWbNGiGD6uuvv0b37t1FQbxr165JvjgFqHowwTfiCQkJgampqegaVVxcDGtra0yYMEHyz36dUQCPyKKgoAAeHh5qVywpn0z//fdf2Va8PK8NGzagQ4cOoicm9vb24DgOXbp0weDBg7Fx40YA8nXPjImJQVJSEoCqjnlRUVEwMTFBTEwMdu3aJbxv+/bt6NWrl2QdMRVTRPnVDLdu3YKenp7QmVNxmx06dEjy2k+KKaI7d+4UxqCrq6tSiJZfiSd1EFX5M3iKT1j5G9muXbsKQbz4+HiYmJhIdsOoLuVBcQyZmZkqK/H4lCWpahE+D8Ug3uTJk2FiYoKxY8cKP//333/x+PFj+Pj4YNGiRRobF4+/oeT3YaDmIN6DBw8QHh6usqLlVfABGeXOl++9954QxFN80vjgwQMMGTJEkiLzf/zxh3BMKQd7FX835TpQcrp79y66d+8uqqnE27JlCzp06AA7OzvhYcKff/4JIyMjjBo1SrJUoHv37qFnz54qTTqePn2KEydOoE2bNvD29ha+i8OHD8PX1xc+Pj4oKSmRLQCdlZUFfX19IR1S8ZgPCQmBnZ0dNm/eLATx+LIOUn436vDfQ2FhIVxdXRESEoKUlBSUlpZi+/btmDp1KmxsbGBhYYFGjRqB4zj07NlT0pUVvLNnz2Ly5Mmwt7fHgQMHUFJSgjNnzmDkyJHw8/ODoaEh7OzswHEcfHx8ZAuEKM5vHjx4gI8++qjaVDa5xqB4zUxNTcXHH3/8zC7xcjl37hz09fWRmJgoer26IN6hQ4c0ckzx+BWTkZGRwmuKQYeKigqMGTNGpfC+lP7880/Ur18fc+bMEQUQ+d+9rKwMkZGRwupbTeG7dc6aNQvFxcUoLy/HgwcPMHv2bNSrVw+DBw/GgwcPkJubiw0bNiAsLAxDhw7F4sWLJW3ScOnSJRgbG+ODDz4QVp8XFxcjPj4eTZo0Ec7JT58+xfr16xEZGYnhw4fj008/lbxZhOL+6OjoiKFDhwp/LigoENJpFy9eDKAqeCdnKZf8/HxYW1ur1NqcNWsW6tevj88++wxAVT1o5VrMciosLISxsbFQI3vmzJk4cOAAnJ2d1dYFBaR/gHv79m107twZxsbGGDhwIMaPH49169bBzs5OlNGjqQfH+/btwxtvvIHAwEBs3rwZkZGRaN++PTp27Ag7Ozts2LAB3bt3h6OjI6KjoyW7j1O2du1aODk5Ydy4cZgzZw4SExPRuXNnGBkZoU+fPhg1ahSmTp0KExMTtem0Unv06BESExNRr149YZVqeXm5cC7s16+f5CnwrzsK4BHZPCvt8MmTJ1i+fHm1zQE0qaCgAAMGDBCCR/wquL/++gtbtmwRUsukql2jzvLly+Hm5ia6cXN1dYWxsTGcnZ3h5eWF7777DqWlpZIvLy4qKoKrqyucnJxw+PBhtGvXDpMnTxa9R66TqnKKaHBwMNauXQtzc3NERUWJ3ifniV3dv89vi+vXr2Pz5s0A/n+/dnZ2RmRkJPT09CSbMN28eRODBw/Gr7/+KrzG3wDm5eUJgVzF7yo4OFj2OoCKlNPCgKoJZlRUFNzc3IROrkDV02AzMzPJ0saeV3p6OvT19dU2rLhx44YQxFN8ih4XF4cOHTpIFpzmC9x/8MEHan/OB/HWrVsnHM9z5syBpaXlK9+QPCvtmv95aWkpPv30Uxw8eFD0ulz47nDHjx8XJm6K57vVq1ejRYsWWLdunfDa2bNnJb1Bu3r1KqysrITVzepWJTZo0EBUvys1NVXWBwbnzp1Dy5YtRYW3AXHwR10Q78yZM5LfvCpS7pJ569YtuLq6wsvLS7Q6PC8vDxcuXMDs2bMxfPhwIRVRKor7SFpaGt555x107twZO3bsEL3v4MGD2LBhA/z9/WVpaqSoqKgIzs7OMDAwQPPmzdUeQytXrsSOHTskP66Uj29ra2vo6+sL20QxEL9kyRKV1UJSys7ORosWLdCtWzdhf1XcXtUF8Y4cOSLrMaXo4cOHGDt2LExMTDBz5kzRz/7991/Ex8fD0tJStsY4/PaaPXs2dHR0sHjxYlFd0/LycsyZMwdWVlayHs/KysvLERMTI5QpUX6As2DBAnAchxUrVsg+lunTp2Pw4MEqrz9+/BiffPIJmjVrhunTp2vs5p6/Pi1btgwODg6ih/t8gxojIyNhZaLU2SD8tuDHsWnTJjRt2lSlBvasWbOgr68PHx8fGBgYaCwrha9D+vPPP2PXrl2YOHEiQkND0aFDBxgYGMDW1lby0gXK+JWHx48fx86dO5GYmIiePXsiMDAQHMfB0dFRtNJLrn3n2rVronnjvn370KhRI8yZMwclJSV4+vQpDh8+jBkzZghd0zmOQ9u2bSUtu3Pz5k1RDexVq1bB09MTEydOxN27d/HkyRNkZ2dj+vTpCA4ORtOmTYVVelLdh6t7mMjvw3fv3kViYiI4jsOmTZuEn8+ePRsmJiYav0+o6yiAR2RVXdphaWkppkyZAo7jZD/JP6+YmBh4enpi5MiRaNWqlai+UEVFxSvXpHoe9vb2wlPs0aNHw9zcHMeOHcOZM2fg5+cHW1tb2QKeRUVFcHNzA8dxovbeUhf/Vkc5RdTAwAABAQHCz6WsfaXOxYsXERUVhdDQUOHGnf+9c3Nz0apVK0ydOlXULZlfoSllqsKVK1fg6emJfv36iWqIXL16FU2bNsX7778vjCs9PR0uLi4wNjaWPV2ipuDmtWvX8NNPP+HevXuIjIyEu7s7Vq9ejYSEBI0GFnl8t1nlwu7btm0T/n9+fr4QxPviiy+wbNkylY6jr4Lvzqu8km7v3r2iosl8Ou3WrVsxa9Ys6OnpSbYt1aVdKzZAKSkpQWRkJBo0aKCxm8aDBw+C47gaV2Xb2NggIiICgDwrAvfu3YtGjRoJT76VP+PevXtwdHQUrdaRE9+IwM3NDbq6uiqrNZVX4jk5Oams3JTa9evXUVxcLDr388GZ27dvw83NDd7e3tizZ49KipRU2+zmzZs4f/68yo0sUPWdjR07FtbW1qL0dE0rLCxEQEAAzMzMVGqm8alsUgczefz3zq/IdnNzE5qI8OLj49GwYUO1jbmkwO+7NjY28PT0REJCglBuRHF78UE8GxsbzJ8/X5axKFLcJxUfgIWFhaFx48bw9fXF119/jaSkJAwfPhwtWrSQ7Lyr7gaW34evX7+Od999FxzHoX///liyZAmSkpIwbNgwScfwIrp37y56UAqIt92gQYPQvn17lZqtUgZDKisr0atXL0ybNk30+fxnPH78GAMGDICNjY1sGTD37t1T+4D+xo0baNasmUp6dVFREebNm4cOHTpIut2Ux8B/B5cuXYKbm5uQpaP4PfA18aRsgFCTjIwM2Nraqm1CUVhYiNzcXHTs2BFubm7IyMiQZQw5OTnw8PDA559/rvaYO3ToEMLDw+Hj41NjGZFXpVgySnFe89NPP0FXVxfjxo0TfebDhw+Rm5uLlStXSjrvUiybojjPW716NRwcHDBhwgSVDJ309HSkpKRIdg9e0wIEPrh49+5dxMfHg+M47NixAx9//HGt3Ce8DiiARyTzrLRDfiVeeXk5oqKioK+vXysTFuUTOD9ZePjwITp06AAzMzONXQh5/ARv1apVGD16NN566y2YmpoKHdp4fKfaV1XdtuK7+XXt2lW2pyHPShE9f/68sGpIsZ6QXE/P0tPT0bJlS4SEhCA8PBwNGjQQVpEVFBSgXbt2GD9+vMrnFxUVydIt+dKlSwgMDERAQIBQT8XU1FQ0EeD/NycnR/ZOTc8KbpqZmQk3ADdv3hRSvzVdo5D/fI7jMGnSJNHrfL0YxUlCfn6+sMKA4zjJxpqXl4eWLVsiODhY9PqCBQvQtGlTnD9/XrQvzZgxAxzHvVLqy8ukXUdHR6Nx48Ya3Ubp6elCkEr5wQD/nQQGBgorQuSQm5uLxo0bi9L9lI9tf39/UdqUXDIzM9GoUSMh2JyUlAQdHZ0ag3i+vr7w8vKSpVYN8P8NKxwcHDB//nzRzQAvPz8fbm5u6NmzJ/bv3y/5ubmgoADNmjWDv78//P39kZ2drXLtS0tLE4J4/Mp5nhzXihcpq8B3aJfqpuRZx3dWVpbasgpyNnnKzMwEx3H48MMPAVQ9jHB2dkZCQoLQAVM5iBcVFQVXV1fcuXNHllWJNT1k2rt3L+7fv49PPvkEjo6OaN26Nbp27Yp33nlHsvIXNd3A/v3339izZw8KCgqwfv16dO3aFaamprC3t8c777wje+1Rde7duwdnZ2fExsYCEKcV89/dhg0b8MYbb8g6z6ioqICXl5fowbHyOA4ePIhGjRrhwoULspxv2rdvD29vb2zatEmlHtmCBQtgY2OjEugoKCgQAtZSjcPZ2RmdOnXCt99+qzIHj4mJwRtvvCEEU5XPQ5oSEhIiNDTig3iKnZz58bRv3x5eXl6S3+eVlZUhLCxMCISvXbtWOEcqPkD67bffEBISgt69e6tcI6TEl1RQvm/6+eefoauri0mTJtVYf1MqimVTvv/+e+H1zz77DE5OTpg4caKsdeZrWoDQpEkT4Tzz6NEj4QGXlHPv/xoK4JFX8qJph3Z2dhg+fDgaN26s0Yj7rVu3RJ+nbgJQWlqKSZMmITAwsMb3vaqaVihcuHABRkZGaNq0qai4qhSrGl50Wzk4OEg+qXzRFNGgoCDRyimpZWRkQE9PT7iBrqioQFRUFKZNm4bS0lJcuXJFSD/SdPOFwMBABAYG4ssvv8TevXtV6uZoYjzPG9xUHFteXh5mzJhRYwcwueTn56NLly5wd3cXjh+++x9fn0rxe8vPz8eyZcskHWtGRgYcHBwQGhqKn3/+GQDw0UcfoWXLlqKusIrH9KJFi176SfWLHlNvvfUW/Pz8JA0wvAhvb2907dpV9PSZ3yb//vsvAgMDsXTpUtHrUrpz5w569+4NOzs7Uc2yyspKVFRUoKSkBP7+/rLWw+I/b+LEiSorkp4niCdn6uE///wDDw8PDB06FMnJyWjRogXGjh2L9evXi95369YtdO/eHXZ2dkL6qFQuXLiAli1b4ssvv8TMmTPRrVs3BAUFYcuWLaKC7WfOnMH48eNhYmKiNtD4ql6lrIJUNyWvUlZBrhujp0+fYvHixaKugUBVEM/FxaXaIF5eXp4sZUie5yHTu+++K7y/vLwc165dw+PHjyXtQPm8N7BA1XHGNwCQO8NA0YULF0R1YceNG4cWLVoITQCUV79t2rQJ1tbWwvaUSlpaGrZv3y78eciQITWmMW/atAmdO3eWrUnXqlWrEBkZifr168PX1xdxcXFCHc+MjAy0atVKGK9c2+vhw4dISUnBkCFD0KFDB7Rr1w5Lly4VjuPi4mLY2tpi4cKFKs1xNDk/TUlJER4y6+npqdT+5L+fe/fuoUmTJujdu7ekxxkA7Ny5E02aNIGTkxN69uyJdevWCasSFedWx48fh6+vL/r37y9rgzf+GqAcxNuzZw90dXURFRWltmmZVNSVTVEM4q1ZswZOTk6YPHmybKvCgZoXIChul/v372PVqlW10rjydUEBPPLSXjTtsKioCI6Ojhpd6g1URft79+6NoKAg/P7778Lr6rojZWVloVGjRvjqq68kH8e1a9dQXFwMQH1Ajh/DihUr0L17d0lXd71MimjHjh3h6ekpaRHuF0kRzcjIgLOzM8LCwmS58Obl5cHY2Fil7srQoUPh4OAAKysrhIWFiWo1aNKlS5fw1ltvoW/fvqLvSlMTtZcJbvLbTtMNaRRXqOTn58Pe3h7u7u6IjY0VBe8U8el5Uj0RffDggTCBPH36NHx8fDBw4ECMGjUKRkZGatNNFLfry3rRY4pPu66tVcYnTpyAubk5XFxc8OeffwqT/YqKCsTHx8PMzEzyJjnKUlNThbRVxZvIiooKzJ07V9R5UE7V3dQ8TxBPDvzxMG/ePKGZy9mzZ/H+++8LaZqbN28WbgBu376NgIAASTv58ebPn49+/foBAI4ePYpPP/0UrVu3Rp8+fZCQkCDs01euXMHkyZMl3151vayC3Me3Yg03xf1SXRBPzuvBiz5kkvv6WdMNrCbKkdTk7NmzaNiwoagm4p49e9CqVSv07dtXbXmWyMhIDBw4UNJGORkZGaKOxUBVHdDmzZsjJCQERUVFKnOJ6OhoDBgwQCWV91Upb5M///wTkZGRaNu2LczNzfHee+/h+vXrmDRpEhwcHGRrGKR8jBw5cgQJCQlo0aIF7OzsMHr0aGRlZSEoKAhDhgyRZQzP69KlS+jUqRM2bNiA+Ph46Onp4ZdffhG9hz8n3L9/X9JsHsWMqcmTJ2PNmjUYNmwYHB0dsX79erVBvFOnTmmk3mZ1Qby9e/eC4ziheYNc1JVNUQzirV27FhYWFoiJiZG10/WzFiDwavt8WNdRAI+8FG1LO3yW//3vf/D09MSwYcNEbd6VuzECQFhYGEJDQyW9UJeUlKBfv34wNTUVipZWN6n99ddf0blzZ1Hdhlfxstvqzp07shR1fpEU0aysLOTm5ko+BqAqoOrq6ooBAwYgNTUVQNVqLb6b3pdffglra2tYWVnJXhC9OhcvXhS+q+PHj2vsc7U9uKkoMzMTNjY2ogDZ7du34eHhAY7j8PXXX6v8nRkzZsDJyUmyupY5OTno0aMHVq1aJTxlPXXqFHx8fKCvry+kmQH/f56Jj4+HlZWVJCtStC3tmqcu7fDp06f46aef0LFjRzRr1gx+fn4YMWIEBgwYgJYtW2os7fDIkSNo1aoVWrZsCX9/fwwbNgwDBw6EiYmJRlYmKp7/1QXm+CDeF198IftYlB04cACGhoY4evSo8Frfvn2hp6eHN998E+3atcOMGTNw69Yt2Ro0nDp1Cn369BGdewMCAmBhYQFLS0t07twZY8eOxfXr1yW/EdC2+Y22HN+KxxT/eYrHNY8P4i1YsEDW1L66uoK+NvCNndQ1VeIfnLi6uuLUqVP4+++/cfnyZXzwwQcwNDQU1W2VYhx6enpqG0x98cUXMDAwQO/evfH999/j4cOHOHfuHGbNmgVDQ0NZajmqa8r19OlTPHnyBO+99x569uyJhg0bCvMJqeblPOVjSnk/ycnJwZo1a2BpaSnUE+c4TtaUUGXq6g6uXLkSbm5uuHTpEiZMmAB9ff1qV+JJ4dGjR0IaKr/NYmJi8NZbbwEARo4cCRcXF1EQT84HXs9TUkExiHfgwAFJM5pepGyKYhBv/fr1sjXrUVTbCxD+CyiAR16Ytk6alJWUlODBgwfCGH788Ud069ZNJYjHKysrw65duzB37lzJT3CVlZU4evQoevbsCWtra+GmvbogXnBwMJycnF75+9PWbaUtKaL8OAYMGIBx48bBxMQE+/fvF35+/fp1cByHtWvXyj6WmsYYFBQEDw8PlQ5kcqkLwU2eunosQFWKH5/Wpriii69Ppdik5lWoq8fCB/HS0tLQq1cv9O/fXzTx54vLS5nipi3HVE1ph7m5ucKKt/z8fEybNg3BwcEIDAzEvHnzRI2OXkVNaYfXr18X0g7PnTuH5ORk+Pn5ISgoCHPnzpVsDOqo+274a0BeXp6Qksnjazdu3LhRtjFVJyoqClOnTgUAREREoE2bNsjOzsb58+exePFiWFpayh4I7t27N8LCwgBUNXXi69M+ffoUM2bMgL+/v+Qr7+iaqV5Nx9SNGzeQkpIiuqmMjY2FpaUlFi9eLEvwStsfMmnTDWxWVhYMDAxUmiodPHhQ2DZLly4VVpA2b94cjo6OsLW1lXQl54ULF6Cvry+s7uW/i59//hm5ubl48uQJdu3aBUtLSzRs2BD16tWDvb097O3tJR3Hs65Rig/9iouL8c0336B79+4wNzeXdLXxs9LjlcsCrFy5EhEREdDV1dVY87/MzEy0b98eixcvFs1hcnJy4OfnJ2Q1RUREQF9fX/JyCkDVfuPl5YW3334bmZmZwirMsrIy2Nvb49tvv8W///6LsLAwuLq6YuPGjbI0O3mZMkRylJJ5mVJEW7dulXwcz1JbCxD+KyiAR16Itk+aeOfPn0e/fv1gb28PZ2dnYXn37t274erqiuHDh4uCeP/++y/Gjx+P5s2b4+bNm5KN4/HjxyguLhYuJmfOnIGnp6coiKf41OTx48dYsWIFvv/++1eeKGj7ttKWCe7FixfRp08f6OnpCalSlZWVKCsrw82bN+Hg4CBKs6sNFy5cQFhYmMZWTgF1I7gJ1FyP5fbt27C3t4ejoyMKCwsxf/58NGrUSPLaUMr1WL788kshiHf69Gn06tULb731Fg4fPozExETZum7V9jH1PGmHU6ZMkX0cL5J2qCnPm5KpbNmyZbVSJ+arr76Ch4cH+vTpA3Nzc5WAt5z1fPjvJSsrC7169YKzszNMTU1Vjlup0+nomlm9Zx1TfDqk4nE1Z84c2VZ71IWHTNpwA1tRUYHw8HBwHCdKI0xMTBSaQvCKiorwww8/YP369Th69Cjy8/MlG0dZWRlCQ0NV5hEffvgh2rZtK+qQ+eTJE/zyyy/YvHkz0tLSJK2b+LznYeXrQ35+vuR1AJ/3mFLXKV0TKioq8Pbbb4PjOPj4+MDGxgZhYWHYv38/KioqEB0dDT8/PwBV9zETJ04Ex3E4fPiwZGPg9xuO44S05ujoaHz66acAqjrw8qmpT548QXh4ODp27IhvvvlGsjEAL1+GyMPDQ/J0VW0qRfQstbEA4b+CAnjkhdSFSVN6ejqaNm2KiIgIxMbGwtPTE8bGxsKTol27dsHV1VW0Ei8qKgoGBgYqXV9fRXZ2NoKCgtClSxf06tULGzZsAFBVY8PLywvW1tbCBKm8vBxlZWV49913oaurK8lTm7qwrbRhggsAf/31F/z9/dG3b18cO3ZMeD0+Ph4WFha1kvatTOoiwM+jLgQ3n1WP5fbt23B2dgbHcZJ3Xa2pHotyEK93794wMzOTtbg8UHvH1IumHdZmLSp1pRPkHNPLpmRqguJnKt8o9uzZE3p6empT16QYq7o0IMUxFBUVoVevXmjRooXowZpcwVe6ZtbseY8pTdVArQsPmbThBpbvkmljY4MHDx6oNFXS1HknLS1N6JKZmpqKJUuWwNjYWGj2BGhHvcTaWFn7rGOqtuTn5yMwMBBt27bFwYMHMWzYMPTr1w+Ojo5YtmwZLC0thfPQ48ePMXXqVMmb32VkZMDf3x8RERGYMmWKUA81IiJCCE7zQcOSkhJERERI+uBA28oQAdpTiuh51MYChP8CCuCRF6bNkyY+VWDOnDnCaw8fPkSbNm0waNAg4TXFlXgDBgyAvr6+pCti0tPT0aRJE4wcORIJCQlwcnKChYWFEOw4fvy40I2Rf7o4efJkyTtDavO2UhxjbU9w+XHwF8S0tDQsXrwYurq6khYkr4u0Lbj5MvVY/v77b4SGhkqWhvO89VgUg3gnT55EUFCQrB3AeJo+pupq2qEmaON3U1MqUF5enrDCbOvWrfD29haCVVKO71kpmbt37wZQlV5nYGAgWcr7s9A189mfXdvHlKK68JBJG25gi4qK4OzsDAMDAzRv3lyWVMeaKHfJtLa2hr6+vhB4kXsf0sbzME/bjillRUVFcHJygre3N86fP4/CwkLEx8ejW7du4DhONC+UGr8tzpw5I3TZ3rdvH+7fv48PP/wQI0aMAMdxsu3PdXm/0VTZlOdRGwsQXncUwCMvRRsnTYqpAnw3Lf6mJDg4GKNHjxadRFJSUmBhYYHmzZvLUudj3rx5wmu3bt1CmzZtMHLkSABV39WJEyfg7e0NW1tbjBo1Co0bN5YlWKSN20qZNkxwgf+/MTIxMUGDBg1kXSlVl2hLcPNF67EorsST6sn+i9ZjWb9+vRDsk6MuS03j1MQxRWmH1dPG7+ZFUnlzc3NhYWGBuLg4ycfxvGlARUVF6N+/P2JjYzV2/NA1s2a1naqvTNseMqmjDTewhYWFCAgIgJmZWa3sN/z+wRfYd3NzE1YBAvIF8bTxPKxM244pZXwQz9bWVqgpnJubK5R3kHOc/L+dlpYGX19f+Pn5idJ05ZqH0n5DtBkF8MhL08ZJU0FBAdzd3WFjYyPU1Lh16xb09PSwZs0aAOIT26FDh0QF7l+VYp0PPnjABxFHjx6NkJAQ0U3IyZMn4ebmhoYNG8ra+VAbt5UybZjgAlXBoAEDBmhkpVRdUtvBzbpcj4Uviq3pSZUmjilKO6yetn03z5sKVFFRIeyrSUlJaNu2LR49eiT5/ltTGpDizXxUVBQsLS2FQLgm0DWzZtpS/oKnLQ+ZtMWzumTa2NiIumRK7VldMrOysoQumfxqW0Cea6S2nYeroy3H1LP2nS5dukjePIj3vN1VAwIChGYNgDzBX9pviDajAB55Jdo4aSoqKoKrqyucnJxw+PBhtGvXDpMnTxa9R84bacU6H3yr91u3bkFXVxerV69WGceJEydExYXloo3bSltJXXT2dVHbwU2qx6KdKO2w5s/Vhu/mZVOBsrOzZQ1W1ZQGxP9veXl5ra00o2tm9bSl/IXyeP6rK+jrcpdMuVe0ast5+Flq65jShn2H9puXp23nYiI/CuCRV6aNk6aioiK4ubmB4zgMHz5ceF3uuhLKdT6Cg4Oxdu1amJubIyoqSvS+2ljirI3bitQttR3c/C/XY9FmlHZYvdr+bl4mFUjTtQrVpQFpQx0oumbWTFvKX/Bq+yFTbXnZLpmenp7/mS6ZtX0efl6aPqa0Zd+h/ebVaNu5mMiLAnhEErU5aapuuXdxcTF8fHzQtWtXWVMFqhvP2bNn4evrCwMDAwQEBKgdY234r05wyevjv1iPpS6gtMPq1eZ3UxdSgbQ5DYiumTXTlvIXvNp+yKRp1CXz+dWFaxSguWNK2/Yd2m9ejbadi4l8KIBHJKPJSdPLLPeWOpUOeHa9hvPnzwt1PlJSUkTjr03/tQkuqbuoHkvdQmmH1avN76YupAJpcxoQXTOJNqIumS8/rv/6NUpb9x3abwh5NgrgkTpHW5Z7v0y9hm3btkn2+YS8rqgeS91GaYfVq83vpi6kAlEaECHPh7pkvrz/+jVK2/cd2m8IqZkOI6QOycjIYD169GA3b95kjRo1YnFxcWzJkiVMR0eHFRYWMh8fH9avXz/2ySefMI7jGGOMGRkZsdOnT7MtW7awBg0aSDaW0tJSdvPmTbZ06VKWmprKGGOsQYMG7Nq1a8zOzo4dP36cVVZWMgcHB7Z8+XJ269Yttm3bNvbo0SPJxkDI6+bSpUts6tSpbNCgQWzp0qWMMcYqKytZ/fr12fXr15mnpyf7/fffGQBmbGzM9u3bxx4/fszGjBnDnj59Ktk4Xub4/u677+j4ZoxZWVmxJUuWMA8PD3b27Fnm4uJS20PSGrX53XTq1Il99tlnzNvbmx06dIj99ttvjOM41qBBA7Z27Vr28OFD5u7urrHxqNOlSxe2ZcsW1rZt21odByHarqKigllYWLDS0lJ2/PhxxhhjixYtYj/++CMLCwtjsbGxLDs7myUlJbGMjIxaGaOVlRVbvnw5A8AWLFjATpw4wRhjwvy8tvzXr1Havu/QfkNIzTgAqO1BEPI8MjMzmYeHB4uJiWFJSUmssrKSRUdHs/r167PFixezmzdvspSUFDZt2jQGQCMn+suXL7OpU6cyACwhIYG5u7szMzMzFhQUxL744gvGcZwwlnPnzjFDQ0PWrl072cdFSF2UkZHB+vTpw7y8vJiuri7buXMnS05OZu+//z4rLCxkbm5uzN/fn61du1Z0fN+9e5c9ePCAWVhYSDoeOr5fzdOnTyV9aPI6qc3vRnG/XrhwIfvll1/YvHnz2IkTJ5iTk1OtjIkQ8uL4Y7lhw4bMxMSEpaSksG+++Yb5+/szxhjLy8tj7du3Z59//jmbMGFCrY7zvffeY8XFxWz58uXMw8Oj1sai6L98jaoL+w7tN4SoRwE8UifcuHGDOTs7szfffJNt27ZNeD08PJzl5OSwJ0+eMAcHBxYUFMRGjx6t0bHxF0HGGBs0aBBr06YN8/f3Zzo6VQtc+UOstp8cEaLNtDFAzxgd3+T1xN8Y/f777+zevXvs5MmTtJqAkDro0qVLLCoqiqWmprLExEQ2ffp0BoCVl5ezwsJC1q9fPzZnzhwWFhZWq+PMyclh8fHxbOnSpbTCVkvUhX2H9htCVFEKLakTtHm5t5WVFVu5ciXT0dFh33//PTMwMBDd3HMcRzf3hNTgxo0bzM/PjwUFBbGkpCTGGGM6OjqsqKiIHT58mNna2rKZM2ey5s2bM8Y0Gyyj45u8jigViJDXQ11IjWeM0uO1UV3Yd2i/IUQVrcAjdYa2L/e+dOkSi46OZgDY3LlzWffu3TU+BkLqotzcXDZkyBBmZmbGYmNjmZeXF1u0aBFLTExkcXFxzNTUlC1btoyVl5ez7du3MwcHB42PkY5v8jqiVCBCXg+UGk9eFu07hNQtFMAjdYq2L/fW1noNhGg7bQ/Q82Ok45sQQog2otR48rJo3yGk7qAUWlKnaPtybysrK/bxxx+zNm3asFatWtXaOAipa6ysrNiKFStYSUkJ27JlC4uNjWX+/v4MAHv69CmrV68es7e3Zy1atKjVMdLxTQghRBtRajx5WbTvEFJ30Ao8Uidp+3LvsrIy1rBhw9oeBiF1zpUrV9jkyZNZvXr1WFxcHPP29maMMTZ37ly2efNmdvToUWZubl6rY6TjmxBCiLai1HjysmjfIUT7UQCP1Fm03JuQ15O2B+gJIYQQQgghRNMogEfqtIsXL7LY2FiWnJzMbGxsans4hBCJUICeEEIIIYQQQv4fBfBInUfLvQl5PVGAnhBCCCGEEEKqUACPEEKI1qIAPSGEEEIIIYRQAI8QQgghhBBCCCGEEK2mU9sDIIQQQgghhBBCCCGEVI8CeIQQQgghhBBCCCGEaDEK4BFCCCGEEEIIIYQQosUogEcIIYQQQgghhBBCiBajAB4hhBBCCCGEEEIIIVqMAniEEEIIIUQ2mzZtYs2aNXvlf4fjOPbDDz+88r9DCCGEEFIXUQCPEEIIIYTUKCIigoWEhNT2MAghhBBC/rMogEcIIYQQQgghhBBCiBajAB4hhBBCCHlpy5YtY3Z2dqxx48bM3NycTZ48mT169EjlfT/88AOzsrJiurq6LCAggN24cUP08927dzNnZ2emq6vLLC0tWUJCAisvL1f7mWVlZSwqKoqZmZkxXV1d1q5dO7Zw4UJZfj9CCCGEEG1AATxCCCGEEPLSdHR02MqVK1l2djb76quv2K+//spiY2NF73ny5AlLSkpiX3/9NTt+/Di7f/8+Cw8PF37+22+/sVGjRrHo6Gh2/vx5tnbtWrZp0yaWlJSk9jNXrlzJUlJS2LZt29jFixfZli1bWPv27eX8NQkhhBBCahUHALU9CEIIIYQQor0iIiLY/fv3n6uJxI4dO9ikSZNYcXExY6yqicWYMWPYqVOnmLu7O2OMsZycHGZtbc1Onz7N3NzcWO/evZmfnx+Li4sT/p3Nmzez2NhYduvWLcZYVROLXbt2sZCQEDZ16lSWnZ3NDh48yDiOk/4XJoQQQgjRMrQCjxBCCCGEvLSDBw8yPz8/1rp1a2ZoaMhGjhzJ7ty5w548eSK8p379+szV1VX4c5cuXVizZs3YhQsXGGOMZWRksA8//JAZGBgI/40fP57dvn1b9O/wIiIiWHp6OuvcuTObOnUqO3DggPy/KCGEEEJILaIAHiGEEEIIeSm5ubksKCiI2dvbs507d7IzZ86w1atXM8aq6tQ9r0ePHrGEhASWnp4u/JeVlcUuX77MdHV1Vd7v7OzMrl27xhITE1lJSQkbMmQICwsLk+z3IoQQQgjRNvVrewCEEEIIIaRuOnPmDKusrGRLly5lOjpVz4W3bdum8r7y8nL2559/Mjc3N8YYYxcvXmT3799n1tbWjLGqgNzFixdZx44dn/uzmzRpwoYOHcqGDh3KwsLCWGBgILt79y5r0aKFBL8ZIYQQQoh2oQAeIYQQQgh5pgcPHrD09HTRa8bGxuzp06ds1apVrH///uz48ePs888/V/m7DRo0YFOmTGErV65k9evXZ1FRUczDw0MI6M2dO5cFBQWxtm3bsrCwMKajo8MyMjLYuXPn2IIFC1T+vWXLljEzMzPm5OTEdHR02Pbt25mpqSlr1qyZHL86IYQQQkitoxRaQgghhBDyTEeOHGFOTk6i/7755hu2bNkytnjxYmZra8u2bNnCFi5cqPJ39fX12cyZM9nw4cOZl5cXMzAwYN99953w84CAAPbTTz+xAwcOMFdXV+bh4cGWL1/O2rVrp3YshoaG7KOPPmLdunVjrq6uLDc3l+3Zs0dYBUgIIYQQ8rqhLrSEEEIIIYQQQgghhGgxekxJCCGEEEIIIYQQQogWowAeIYQQQgghhBBCCCFajAJ4hBBCCCGEEEIIIYRoMQrgEUIIIYQQQgghhBCixSiARwghhBBCCCGEEEKIFqMAHiGEEEIIIYQQQgghWowCeIQQQgghhBBCCCGEaDEK4BFCCCGEEEIIIYQQosUogEcIIYQQQgghhBBCiBajAB4hhBBCCCGEEEIIIVqMAniEEEIIIYQQQgghhGgxCuARQgghhBBCCCGEEKLF/g/BSVAq6dkSOQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1500x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bar plots\n",
    "\n",
    "labels = [itos[x] for x in list(nercounts.keys())]\n",
    "counts = nercounts.values()\n",
    "\n",
    "plt.figure(figsize=(15, 9))\n",
    "plt.bar(labels, counts)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Counts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "19a50a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.0370e+04, 6.6838e+04, 7.2900e+04, 5.4448e+04, 2.3796e+04,\n",
       "        1.2156e+04, 6.2960e+03, 2.6360e+03, 1.4500e+03, 7.1200e+02,\n",
       "        4.1000e+02, 2.1800e+02, 1.3600e+02, 7.0000e+01, 2.6000e+01,\n",
       "        2.2000e+01, 2.2000e+01, 1.6000e+01, 8.0000e+00, 6.0000e+00,\n",
       "        6.0000e+00, 4.0000e+00, 0.0000e+00, 2.0000e+00, 2.0000e+00,\n",
       "        2.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e+00, 0.0000e+00,\n",
       "        2.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0000e+00]),\n",
       " array([  2. ,  10.3,  18.6,  26.9,  35.2,  43.5,  51.8,  60.1,  68.4,\n",
       "         76.7,  85. ,  93.3, 101.6, 109.9, 118.2, 126.5, 134.8, 143.1,\n",
       "        151.4, 159.7, 168. , 176.3, 184.6, 192.9, 201.2, 209.5, 217.8,\n",
       "        226.1, 234.4, 242.7, 251. , 259.3, 267.6, 275.9, 284.2, 292.5,\n",
       "        300.8, 309.1, 317.4, 325.7, 334. ]),\n",
       " <BarContainer object of 40 artists>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Most texts are short, can get away with low max token length\n",
    "\n",
    "tokenized_sent_lengths = [len(tokenizer.tokenize(\" \".join(x))) for x in dataset_eng[\"train\"][\"tokens\"]]\n",
    "\n",
    "plt.hist(tokenized_sent_lengths, bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0602afff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9890158439975625"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Around 99 % of sentences are less than 70 tokens\n",
    "len([x for x in tokenized_sent_lengths if x < 70]) / len(tokenized_sent_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007952fa",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37470f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Taken from HF, to align the ner labels with tokenized words\n",
    "def align_labels_with_tokens(labels, word_ids, max_length):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    \n",
    "    # Word ids are mappings between original words and tokens, ie [hej jag heter] => [hej jag het #er] => [0,1,2,2]\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # New word\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]   #-100 to not take into account during loss function\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)                                #-100 for special tokens\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels[:max_length]    # Truncates labels to match max length\n",
    "\n",
    "\n",
    "\n",
    "def to_tensor_dataset(data, labels):\n",
    "    # Effectively zips the data and labels\n",
    "    inp_ids = data[\"input_ids\"]\n",
    "    atmask = data[\"attention_mask\"]\n",
    "    return TensorDataset(inp_ids, atmask, labels)\n",
    "\n",
    "\n",
    "\n",
    "def label_fix(labels):\n",
    "    # Input is a nested list of labels\n",
    "    return [[x if x in [1,2,3,4,5,6,7,8,13,14] else 0 for x in sublist] for sublist in labels]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(dataset, splitname, all_labels, max_length=70):\n",
    "    # Dataset, split, all_labels or subset => Returns torch tensors\n",
    "    \n",
    "    splitname = splitname\n",
    "    sents = dataset[splitname][\"tokens\"]\n",
    "    labels = dataset[splitname][\"ner_tags\"]\n",
    "    \n",
    "    # Change labels if all_labels = False\n",
    "    # A bit slow - would be faster with tensor operations\n",
    "    if not all_labels:\n",
    "        labels = label_fix(labels) \n",
    "    \n",
    "    assert len(sents) == len(labels) # Sanity check\n",
    "    \n",
    "    # Tokenize and align labels, currently pad everything to the same length\n",
    "    tokenized_sents = tokenizer(sents, \n",
    "                                is_split_into_words=True,   # Input came as lists\n",
    "                                add_special_tokens=True,    # CLS and SEP\n",
    "                                padding=\"max_length\",       # Pad shorter sequences\n",
    "                                truncation=True,            # Truncates longer sequences\n",
    "                                max_length=max_length,      # Set to 70 by default\n",
    "                                return_tensors=\"pt\")        # Pytorch tensors\n",
    "    \n",
    "    # Stack to turn list of torch tensors into one tensor\n",
    "    aligned_labels = torch.stack([torch.tensor(\n",
    "                                 align_labels_with_tokens(labels[i], \n",
    "                                 tokenized_sents[i].word_ids, \n",
    "                                 max_length)) \n",
    "                                 for i in range(len(labels))])\n",
    "    \n",
    "    dataset = to_tensor_dataset(tokenized_sents, aligned_labels)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_labels = True\n",
    "\n",
    "# Tokenizes, aligns labels and turns to torch tensors\n",
    "\n",
    "training_dataset = prepare_data(dataset_eng, \"train\", all_labels)\n",
    "evaluation_dataset = prepare_data(dataset_eng, \"validation\", all_labels)\n",
    "testing_dataset = prepare_data(dataset_eng, \"test\", all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24f35e5",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9aa605b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If not all labels, num labels = 11, 2*5 for B and I, + 1 for O\n",
    "# Else its the length of the label dataset\n",
    "\n",
    "if not all_labels:\n",
    "    num_labels = 11\n",
    "else:\n",
    "    num_labels = len(itos)\n",
    "    \n",
    "\n",
    "\n",
    "# The model probably doesnt need to be more complicated than this, the context is usually very short\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", \n",
    "                                                   num_labels=num_labels, \n",
    "                                                   vocab_size=tokenizer.vocab_size, \n",
    "                                                   ignore_mismatched_sizes=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dcb9165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 12:56:32,506\tINFO worker.py:1673 -- Started a local Ray instance.\n",
      "2023-12-14 12:56:38,046\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
      "2023-12-14 12:56:38,062\tINFO tune.py:586 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-12-14 16:50:17</td></tr>\n",
       "<tr><td>Running for: </td><td>03:53:39.79        </td></tr>\n",
       "<tr><td>Memory:      </td><td>26.2/255.7 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/16 CPUs, 0.2/1 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_d634a_00000</td><td>TERMINATED</td><td>127.0.0.1:7284 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">2e-05</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">         4166.43</td></tr>\n",
       "<tr><td>train_model_d634a_00001</td><td>TERMINATED</td><td>127.0.0.1:11644</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">       3</td><td style=\"text-align: right;\">3e-05</td><td style=\"text-align: right;\">   555</td><td style=\"text-align: right;\">        10512.7 </td></tr>\n",
       "<tr><td>train_model_d634a_00002</td><td>TERMINATED</td><td>127.0.0.1:8128 </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">       2</td><td style=\"text-align: right;\">2e-05</td><td style=\"text-align: right;\">  1478</td><td style=\"text-align: right;\">        11198   </td></tr>\n",
       "<tr><td>train_model_d634a_00003</td><td>TERMINATED</td><td>127.0.0.1:14808</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">3e-05</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">         4166.82</td></tr>\n",
       "<tr><td>train_model_d634a_00004</td><td>TERMINATED</td><td>127.0.0.1:6300 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">         4166.02</td></tr>\n",
       "<tr><td>train_model_d634a_00005</td><td>TERMINATED</td><td>127.0.0.1:18772</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">       3</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">   555</td><td style=\"text-align: right;\">         9151.91</td></tr>\n",
       "<tr><td>train_model_d634a_00006</td><td>TERMINATED</td><td>127.0.0.1:13060</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">3e-05</td><td style=\"text-align: right;\">   739</td><td style=\"text-align: right;\">         5608.25</td></tr>\n",
       "<tr><td>train_model_d634a_00007</td><td>TERMINATED</td><td>127.0.0.1:19352</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">       2</td><td style=\"text-align: right;\">2e-05</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">         7023.52</td></tr>\n",
       "<tr><td>train_model_d634a_00008</td><td>TERMINATED</td><td>127.0.0.1:9460 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">       2</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">         4220.34</td></tr>\n",
       "<tr><td>train_model_d634a_00009</td><td>TERMINATED</td><td>127.0.0.1:13424</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">5e-05</td><td style=\"text-align: right;\">   739</td><td style=\"text-align: right;\">         3356.03</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=6300)\u001b[0m C:\\Users\\NicHer\\AppData\\Local\\anaconda3\\envs\\envname2\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th style=\"text-align: right;\">     loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_d634a_00000</td><td style=\"text-align: right;\">0.0624402</td></tr>\n",
       "<tr><td>train_model_d634a_00001</td><td style=\"text-align: right;\">0.0834087</td></tr>\n",
       "<tr><td>train_model_d634a_00002</td><td style=\"text-align: right;\">0.0691316</td></tr>\n",
       "<tr><td>train_model_d634a_00003</td><td style=\"text-align: right;\">0.0608448</td></tr>\n",
       "<tr><td>train_model_d634a_00004</td><td style=\"text-align: right;\">0.0600416</td></tr>\n",
       "<tr><td>train_model_d634a_00005</td><td style=\"text-align: right;\">0.0910375</td></tr>\n",
       "<tr><td>train_model_d634a_00006</td><td style=\"text-align: right;\">0.0601074</td></tr>\n",
       "<tr><td>train_model_d634a_00007</td><td style=\"text-align: right;\">0.0666176</td></tr>\n",
       "<tr><td>train_model_d634a_00008</td><td style=\"text-align: right;\">0.0730264</td></tr>\n",
       "<tr><td>train_model_d634a_00009</td><td style=\"text-align: right;\">0.0605143</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 25: 3.3263437080383302\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 50: 3.2458693885803225\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 50: 3.0285105419158938\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 100: 3.138875758647919\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 125: 3.0900346546173094\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 100: 2.346327890753746\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 175: 2.9646165711539134\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 75: 3.021599639256795\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 150: 1.808817921479543\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 250: 2.633182162284851\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 275: 2.487296177040447\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 200: 1.4916460160911083\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 300: 2.357885938286781\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 125: 2.574951626777649\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 225: 1.3608422869443892\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 250: 1.2505053565502167\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 400: 1.9869853165745734\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 425: 1.9121352778462803\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 175: 2.1156929867608207\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 300: 1.0725803581376872\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 475: 1.7877343503424996\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 500: 1.7289447911381721\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 350: 0.944155499700989\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 550: 1.615414909720421\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 225: 1.8123280681504144\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 400: 0.8419264120329172\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 625: 1.473828919816017\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 650: 1.4337264869534052\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 450: 0.7615259428073963\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 675: 1.3930046690614135\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 275: 1.5683269252560355\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 475: 0.7279551300720165\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 500: 0.6987472316771746\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 775: 1.2501766363074702\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 800: 1.2179130306653678\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 550: 0.6474899208884347\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 850: 1.1596808592361563\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 875: 1.131725573352405\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 600: 0.6036069634060065\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 925: 1.082355768898049\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 950: 1.0592029128733433\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 650: 0.5660671681796129\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 650: 0.6876809134563574\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 400: 1.170042451992631\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1025: 0.9962194143053962\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 700: 0.5340823924754348\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 700: 0.6475426584535412\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1075: 0.9584770205859529\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1100: 0.9416801566529003\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 750: 0.5072498089969159\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 750: 0.6135038143942754\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1150: 0.9087220852660096\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1175: 0.8931705206822842\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 800: 0.4830413058795966\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1225: 0.8633864599755224\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 500: 0.971111496925354\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 850: 0.4598819997315021\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1300: 0.823139352249698\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1325: 0.8106093467617372\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 900: 0.44053655232613287\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 900: 0.5297057931207948\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 550: 0.8926141228492964\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1400: 0.7755480707223926\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 950: 0.42203016416022654\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 950: 0.5064631937208929\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1450: 0.7544147298639191\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1475: 0.7445219292554814\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 1000: 0.4855181127423421\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1525: 0.7247140502074703\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1550: 0.7155184120829067\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 1050: 0.4651108585519805\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1600: 0.6973017130815424\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 650: 0.7701874076661009\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1100: 0.374270351733589\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1125: 0.3670826430796749\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1700: 0.6629995576478541\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1725: 0.6552962166103332\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 700: 0.7204160643475396\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1175: 0.3549995407196594\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1775: 0.6407061423329821\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1800: 0.6332344754480033\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1225: 0.34292714074919267\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1850: 0.6187789339234901\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1875: 0.6119057175949216\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1275: 0.33205011317670785\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 1275: 0.39471529216889073\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1925: 0.5983620605849987\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1950: 0.5917674954894644\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1325: 0.3216437983026131\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 1325: 0.3820256531990643\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 800: 0.6419902076013386\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1350: 0.3167610906769901\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2025: 0.5733029227400268\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1375: 0.3118456937872212\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 1375: 0.3701263637527485\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2075: 0.5607782781292426\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2100: 0.555084949442008\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1425: 0.30252933154160383\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2150: 0.543977477860442\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 875: 0.5940374278724193\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1475: 0.2946055057021332\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2225: 0.5279361311867414\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2250: 0.5226879761755052\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1525: 0.2875959395319529\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 1525: 0.34011462602474285\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 925: 0.5663506812904332\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2325: 0.5083815304131838\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1575: 0.28074787390329653\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 1575: 0.331615763790462\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 950: 0.5534949241911894\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2375: 0.49944571520270487\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1600: 0.27734412101111955\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2400: 0.49465733534210204\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1625: 0.2738886276430522\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 1625: 0.3232342646289617\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2450: 0.4859108905294644\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2475: 0.481566238928812\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 1675: 0.3153944975333705\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2525: 0.47347711958728805\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1025: 0.5195391978759591\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1725: 0.26144834981569887\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2600: 0.46190003961754533\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2625: 0.4581315047340911\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2650: 0.4542647808662809\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1075: 0.49866889013107435\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1800: 0.2533807766068882\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2700: 0.4470933488205386\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2725: 0.44350663059134754\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1850: 0.24814928689619173\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2775: 0.43630639181514314\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1125: 0.4792132043325239\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1900: 0.24316659367153126\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2850: 0.42625360587088945\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2875: 0.4231647254941256\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1950: 0.23880083835803165\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 1950: 0.2801204085241382\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1175: 0.46192368751352136\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 1975: 0.23677460950056586\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2950: 0.41432571804498214\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 2000: 0.23499850019300356\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 2000: 0.275297930963221\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3000: 0.40851851275206236\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3025: 0.40593779332273877\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 2050: 0.23061429648364826\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3075: 0.40101385491029395\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3100: 0.3983445085389089\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 2100: 0.22650426700866472\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3150: 0.39306833756278226\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3175: 0.39050650994157726\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 2150: 0.2226016228973086\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 2150: 0.2600711493749648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3225: 0.38529538127481794\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3250: 0.38286450090685575\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 2200: 0.21855289805071978\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 2200: 0.25526996691668913\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1325: 0.41873900881775145\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 2225: 0.21670538681679605\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3325: 0.3758134165752497\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 2250: 0.21493244421549348\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 2250: 0.25086036229133607\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3375: 0.3711182627831413\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3400: 0.3690476607701481\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 2300: 0.24663289414111364\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3450: 0.364456321957177\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1400: 0.4007859971174704\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 2350: 0.20811384025787777\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3525: 0.35805383438954513\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3550: 0.3560125189358172\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average T loss at step 2400: 0.27261234989447986\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1450: 0.39015475070810524\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3625: 0.35003329152848317\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3650: 0.3481686053744339\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 2475: 0.20079652892420952\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3700: 0.3442547472842302\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1500: 0.3797800918196638\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 2525: 0.1981634518678881\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3775: 0.3388248896721245\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3800: 0.3370411712773402\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 2575: 0.19562378501044858\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 2575: 0.22695756487833174\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1550: 0.3701997019987433\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3875: 0.33192203561724315\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 2625: 0.1932073687885755\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 2625: 0.223952007498193\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3925: 0.32869279036734395\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3950: 0.3271170204578121\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 2675: 0.19059800743599745\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1600: 0.36136950679472646\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4000: 0.32411983380335735\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4025: 0.3226005523127463\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 2725: 0.188342642525698\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4075: 0.3192564793035925\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1650: 0.3530994564847964\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 2775: 0.1862451655779722\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4150: 0.31445674661423517\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4175: 0.31293265313300456\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 2825: 0.18442856514673944\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 2825: 0.21300411593214305\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1700: 0.34515089453471937\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4250: 0.30842462186268804\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 2875: 0.18255411710942407\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 2875: 0.2106162148674788\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4300: 0.30558612641929866\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4325: 0.3041034867774663\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 2925: 0.18063858955203055\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 2925: 0.208235140656137\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4375: 0.3013333598339398\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4400: 0.2998070693170567\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 2975: 0.2059437801920175\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4450: 0.29709552821263757\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1800: 0.33004631753079594\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 3025: 0.17694529067097647\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4525: 0.2930912285071453\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4550: 0.2917250730156909\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average T loss at step 3075: 0.22926336547541545\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1850: 0.3234315637146702\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4625: 0.28792753389691683\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4650: 0.2866800681719937\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 3150: 0.17293221783236645\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4700: 0.28417415826633435\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1900: 0.3173298500458661\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 3200: 0.17131241608341952\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4775: 0.2802797537945767\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4800: 0.2790670407240638\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 3250: 0.169869302111642\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4825: 0.27788127708439403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1950: 0.31157034020011243\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 3275: 0.16910689604266027\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 3300: 0.16842890193618862\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4925: 0.27409837133050213\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4950: 0.27313195133490037\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2000: 0.30575827667396516\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 3350: 0.16698682022729278\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5000: 0.27113266887401694\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5025: 0.2701252801164253\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 3400: 0.16559150061559835\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5075: 0.26831608780606975\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2050: 0.300401207598426\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 3450: 0.16407920474338386\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5150: 0.26547024281096454\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5175: 0.2644947747943095\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 3500: 0.16270152673461208\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 3500: 0.18598504680957245\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2100: 0.29474270675686143\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 3525: 0.18512092230488142\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5250: 0.2618132130685706\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 3550: 0.16113869150710278\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 3550: 0.18415259439553935\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5300: 0.2598625580630186\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5325: 0.2588734009429515\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2150: 0.28976024985659954\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 3600: 0.15982539809984803\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5375: 0.25704464439190056\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5400: 0.25612814554353935\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 3650: 0.18109956810994024\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5450: 0.2545400152043995\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5475: 0.2537474385040842\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 3700: 0.17959942355729966\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5525: 0.2522573041614162\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5550: 0.251416650142034\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5575: 0.2507634886111145\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 3775: 0.15606099037132812\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5625: 0.24918130457711715\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5650: 0.24848866711214052\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 3825: 0.1551852681524321\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5700: 0.24708523581466196\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5725: 0.24632013669286298\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 3875: 0.15420380664280345\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 3875: 0.1752698358721851\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2325: 0.27237439905503585\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5800: 0.244132949520898\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 3925: 0.15321243190206588\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 3925: 0.17403354824849873\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5850: 0.2426270793635064\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5875: 0.2419530103416281\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 3975: 0.15216985056906024\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 3975: 0.17276318488704076\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5925: 0.2405177029479641\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5950: 0.2397590380036781\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 4025: 0.15111003035034476\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6000: 0.23835043394720803\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2425: 0.26410475338684375\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 4075: 0.15029643990136568\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6075: 0.23631165237001187\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6100: 0.2357370712394162\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 4125: 0.14931936904689241\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 4125: 0.16917121311912145\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2475: 0.26012941503219983\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6175: 0.23382990290518277\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 4175: 0.14820418281818065\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 4175: 0.16778733655032396\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2500: 0.2583444980246946\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 4200: 0.14760890656297782\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6250: 0.23187923149162903\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 4225: 0.14710313187474\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 4225: 0.16646836021664338\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6300: 0.2307015462281249\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6325: 0.23002804541211253\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 4275: 0.16553778021898644\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6375: 0.22878629802633077\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2575: 0.252382065135425\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 4325: 0.14538193260058438\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6450: 0.22709597314272956\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6475: 0.22649499460110895\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average T loss at step 4375: 0.1832454818806478\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2625: 0.2483347085555572\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6550: 0.22480752488120126\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6575: 0.22428487930232624\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 4450: 0.14258535456783505\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6625: 0.22319635409993593\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2675: 0.24453798062585852\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 4500: 0.14144208339604342\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6700: 0.22148829504755438\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6725: 0.22093435293241823\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 4550: 0.14039863227224386\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 4550: 0.15843255012866703\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2725: 0.24080642143033718\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 4575: 0.13992041245492207\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6800: 0.21935468013739035\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 4600: 0.1394475680622829\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 4600: 0.15731187938310146\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6850: 0.21826951333517072\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6875: 0.21769292131951926\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 4650: 0.13858368936503027\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6925: 0.2165919574757446\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6950: 0.21609088636757204\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 4700: 0.13767615615853426\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7000: 0.2150162745153731\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7025: 0.21447133774473084\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 4750: 0.1367652621113107\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 4750: 0.15415122950246166\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7075: 0.21332592712601733\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7100: 0.21276292255984514\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 4800: 0.13584823956312903\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 4800: 0.15307619951173063\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2875: 0.2304038820790694\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 4825: 0.15263687940065898\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7175: 0.21126128777095768\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 4850: 0.1522240579400221\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7225: 0.2103035466081002\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7250: 0.20982141464614662\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 4900: 0.1513066014175408\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7300: 0.2090157751088375\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2950: 0.22573376792033156\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 4950: 0.15027348443821092\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7375: 0.20763263750322541\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7400: 0.20719675693478484\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7425: 0.20678061193901018\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3000: 0.22265409152155433\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5025: 0.1323126096816488\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7475: 0.20600416672174635\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7500: 0.20562103157600697\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5075: 0.13139505871288304\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7550: 0.2047906891543523\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3050: 0.21986699827267314\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5125: 0.13043923265093982\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7625: 0.20359023622040195\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7650: 0.20331668950564052\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5175: 0.12951605571739844\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 5175: 0.14572652267062686\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3100: 0.21701803770704917\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7725: 0.20206994037666814\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5225: 0.1285084761023459\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 5225: 0.14458341629585278\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7775: 0.2013591693482628\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7800: 0.20098182333664646\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5275: 0.12760741760486394\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3150: 0.21412947479118075\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7850: 0.20015555033924437\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7875: 0.19976078057648347\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5325: 0.1266537947867704\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7925: 0.19891383392862647\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3200: 0.21138115688620018\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5375: 0.125769694907829\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8000: 0.197786492971165\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8025: 0.1973536778311898\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5425: 0.12488899401140137\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 5425: 0.14046304636408494\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3250: 0.20860217962714916\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8100: 0.1961567905656979\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 5475: 0.13942751715064994\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8150: 0.19555629831552362\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8175: 0.19521347254522525\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average T loss at step 5525: 0.15518907786651442\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5550: 0.12267122356815742\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8250: 0.19409434286605878\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8275: 0.1936837789202123\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5600: 0.12184730662869697\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8300: 0.19327187914184593\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8325: 0.1928366143794733\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8350: 0.19248019183480494\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5650: 0.1210960214255085\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8375: 0.19206197953448673\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8400: 0.19163236126120042\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8425: 0.19128763126230797\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5700: 0.12030204846933198\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8475: 0.19052923021211005\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8500: 0.19019453178772786\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5750: 0.11952936971933661\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8550: 0.18967992258328287\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8575: 0.18932497852126887\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5800: 0.11880400245777403\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 5800: 0.13351027180950542\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3475: 0.19838601353404064\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8650: 0.188190757607916\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5850: 0.11803948995403539\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 5850: 0.1326565338187636\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3500: 0.197345820015762\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5875: 0.11766418846873963\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8725: 0.18698230757590859\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5900: 0.11731173110256658\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 5900: 0.13184164645014157\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8775: 0.18616620077754895\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8800: 0.18576217722026658\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 5950: 0.11656588588740496\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8850: 0.18509291720178667\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3575: 0.19428996011347777\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 6000: 0.11588356578731328\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8925: 0.18396728253353997\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 8950: 0.18361038572391022\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 6050: 0.11521180718559926\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 6050: 0.129419660529522\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3625: 0.19233966697129454\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9025: 0.18247741636511847\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 6100: 0.12874010371097716\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3650: 0.19135135945870402\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 6125: 0.11434157261628733\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9100: 0.1813558933778556\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 6150: 0.12798039548049825\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9150: 0.18070590585403404\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9175: 0.18037361010593775\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average T loss at step 6200: 0.14249802975258188\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 6225: 0.11299153407953351\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3725: 0.1886127964885368\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 6250: 0.12646893429666758\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 6275: 0.1123217817657976\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9325: 0.17852103621094056\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9350: 0.17819284910656746\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3775: 0.18676154323545166\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 6325: 0.11160516917842657\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9400: 0.17759296303245292\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9425: 0.17727486842454582\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 6375: 0.11095210817738381\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9475: 0.17666469675861668\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3825: 0.18488294272573993\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 6425: 0.11027323018487702\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9550: 0.1757440792124277\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9575: 0.1754639241583925\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 6475: 0.10959157372302103\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 6475: 0.1229740258554288\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3875: 0.18319763126374491\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9650: 0.17464483500574715\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 6525: 0.10894450252314127\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 6525: 0.12223714066664884\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9700: 0.17415177856980626\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9725: 0.17388637198815793\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 6575: 0.10847616342907797\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3925: 0.1816251958833687\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9775: 0.17336918370522525\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9800: 0.17309963412113055\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 6625: 0.10795681731822408\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9850: 0.17250114106885772\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3975: 0.18012947625316586\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 6675: 0.10741412324476411\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9925: 0.171616024036032\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9950: 0.17138190268661158\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 6725: 0.11986005749597585\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 4025: 0.17859740971043844\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10025: 0.17068764980937565\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 6775: 0.11919799441881512\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10075: 0.17007508082133915\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10100: 0.1697789531314197\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average T loss at step 6825: 0.13288152761640531\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 6850: 0.10557053212224282\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10175: 0.16890633215585435\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10200: 0.1686486845533934\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 25: 0.06420639142394066\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 6900: 0.10513415223350275\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10225: 0.1683209476776109\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 50: 0.06748742565512657\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10250: 0.16792878428973804\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 75: 0.06634173880020777\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 100: 0.06648357982747256\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average T loss at step 6925: 0.13176969502184252\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 125: 0.06627893414348364\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10275: 0.16762316645366943\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 6950: 0.10472656828553999\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 150: 0.06763988098750512\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average T loss at step 6950: 0.1314969375024285\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 6975: 0.10447731928354109\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 175: 0.06729223703699452\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 200: 0.0666189344250597\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10350: 0.1667088540251494\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 225: 0.06589873641522394\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average T loss at step 7000: 0.13090182318660665\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10400: 0.16604281808275398\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 250: 0.06488660988025367\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7025: 0.10400245019624149\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 275: 0.0645359526489946\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10425: 0.16573709376327184\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7050: 0.10379293982251646\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 300: 0.06442903003500154\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 7050: 0.11631917082161332\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 325: 0.06476484034640284\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average T loss at step 7050: 0.13031649890080826\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 7075: 0.11604357556329692\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 350: 0.06489509618042835\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 375: 0.06493103327229618\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10500: 0.16475895374154373\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 400: 0.06552976680803113\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average T loss at step 7100: 0.1297208826462525\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 425: 0.06562166400144205\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10550: 0.16421763834528352\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7125: 0.10309995544677306\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 450: 0.06518369791718821\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10575: 0.163915143068745\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7150: 0.10290467924036255\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 7150: 0.11529671662170407\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 475: 0.0653354540065323\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10625: 0.16326234231712022\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 500: 0.06462288975249976\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m epoch:  1\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10650: 0.16296182341906984\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7200: 0.10245748447147586\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 7200: 0.11479759656102236\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10700: 0.1623905140837267\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10725: 0.16209210220091522\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7250: 0.10204361941890212\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 7250: 0.1143356036729067\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10775: 0.16154146150195065\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10800: 0.1612290325956861\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7300: 0.10159866776448126\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 7300: 0.11383312610446074\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10850: 0.16068861188734496\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10875: 0.16038271922642236\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7350: 0.10120183996048433\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10925: 0.1598192528883492\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10950: 0.15950516431111858\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7400: 0.10078342962947145\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11000: 0.15888719077637176\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11025: 0.15859755285785643\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7450: 0.10038019341972951\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 7450: 0.11244802440453365\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11075: 0.15804367926678123\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11100: 0.15775754375297518\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7500: 0.09998537677383672\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 7500: 0.11199886829312114\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 200: 0.04247921847971156\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7525: 0.09977466932688206\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11175: 0.15694665854873857\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7550: 0.0995486768790126\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 7550: 0.11150808059643577\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11225: 0.15643752083995946\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11250: 0.15618257816088799\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7600: 0.0991411738377524\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11300: 0.15571100860527115\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 275: 0.04255657322196798\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7650: 0.09867652546515493\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11375: 0.15492768065962725\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11400: 0.154669158176303\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7700: 0.09825206982093566\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 7700: 0.1100748230383033\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 325: 0.04292878475470038\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11475: 0.15389634377891473\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 7750: 0.10969706493764064\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 350: 0.042866460086245624\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7775: 0.09775810812124563\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11550: 0.15311373142289447\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 7800: 0.10929307079579209\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11600: 0.1526564275875215\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11625: 0.15238749925416184\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 7850: 0.10896242176953727\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7875: 0.09711105203009136\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 425: 0.04482240337988033\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 7900: 0.108593776609378\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7925: 0.09683254299328768\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11775: 0.15087793049577033\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11800: 0.1506392676621563\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 475: 0.045402225194204796\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 7975: 0.09653171726375845\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11850: 0.1501577302701609\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11875: 0.14990040884768394\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 8025: 0.09623032217625184\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11925: 0.1493749190822724\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 525: 0.044338953629513046\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 8075: 0.09590450773422171\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12000: 0.14871164575143\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12025: 0.14848814691584644\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 8125: 0.0955383986369635\n",
      "\u001b[36m(train_model pid=14808)\u001b[0m Average T loss at step 8125: 0.10701375926590095\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 575: 0.04271564127806493\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average T loss at step 8150: 0.11977309973654766\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average T loss at step 8175: 0.09527353888158245\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12150: 0.14739050717401814\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12175: 0.1471865196927767\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average E loss at step 25: 0.06216090119909495\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average E loss at step 50: 0.0651776879793033\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 625: 0.04159766099816188\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average E loss at step 100: 0.06344548602355644\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12200: 0.146986838155438\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12225: 0.14678068943892353\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average E loss at step 150: 0.062179529566007356\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12250: 0.1465678935032718\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average E loss at step 175: 0.06761831939619567\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 650: 0.04109372626049802\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12275: 0.14633435203471884\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average E loss at step 200: 0.06454351857420988\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average E loss at step 250: 0.06491821264009923\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12300: 0.14610009054490555\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average E loss at step 300: 0.06367997034724492\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 675: 0.04061910667965465\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average E loss at step 325: 0.06729981873685924\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12325: 0.14587295850896623\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average E loss at step 375: 0.06324833551328629\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12350: 0.14563407802300363\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average E loss at step 400: 0.0652976736711571\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12375: 0.14539635400695378\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average E loss at step 450: 0.06205691840422029\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 700: 0.03985593904533224\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12400: 0.14519123198313597\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average E loss at step 475: 0.06388677901245261\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average E loss at step 525: 0.05984450868541552\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12425: 0.14499292349283324\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average E loss at step 550: 0.0630183519914069\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 725: 0.03939966280968731\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average E loss at step 600: 0.060417716575611846\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12450: 0.14475702126181533\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12475: 0.14454154347609177\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average E loss at step 625: 0.06309802272431553\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average E loss at step 675: 0.060349889269219366\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12500: 0.14434443483010634\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average E loss at step 700: 0.06337163729293804\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 750: 0.03907021232810803\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12525: 0.14409186310106015\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average E loss at step 750: 0.06085384823021013\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12550: 0.1438901170822133\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average E loss at step 775: 0.06334166075020367\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 775: 0.03885564273275856\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average E loss at step 825: 0.061694268803359154\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12575: 0.1436653519271911\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average E loss at step 850: 0.06423374287297895\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12600: 0.14342315249885332\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average E loss at step 900: 0.06139376192182277\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12625: 0.14320180309727779\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average E loss at step 925: 0.06438700379885583\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 800: 0.038472267746765286\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m Average E loss at step 975: 0.060979554255594474\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12650: 0.14297208643308795\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average E loss at step 1000: 0.06291978828428546\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12675: 0.14277279011098323\n",
      "\u001b[36m(train_model pid=6300)\u001b[0m epoch:  1\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m Average E loss at step 1025: 0.06250109576693987\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12725: 0.142329119067936\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=7284)\u001b[0m epoch:  1\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=13060)\u001b[0m C:\\Users\\NicHer\\AppData\\Local\\anaconda3\\envs\\envname2\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m   warnings.warn(\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12775: 0.14190403984148564\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12800: 0.14168348447004064\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 875: 0.03773284972146419\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 75: 3.0890517298380535\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 100: 2.969348123073578\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 900: 0.037523889579460956\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12900: 0.1407872379710424\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12925: 0.14054733013762818\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 925: 0.03719862752726201\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 200: 2.231892506480217\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 225: 2.0668403768539427\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 950: 0.03715277807204984\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13025: 0.13966184511191065\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13050: 0.1394802975043154\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 975: 0.03700046532530672\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 325: 1.6313471352137052\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 350: 1.548555161016328\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1000: 0.03718058001348982\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13150: 0.13882762038624338\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13175: 0.1386422024838418\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1025: 0.03696744512068108\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 450: 1.2843250455458959\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 475: 1.2309094209733762\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1050: 0.036803940688315336\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13275: 0.13791457130250973\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13300: 0.137732908045396\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1075: 0.036548434592794284\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 575: 1.0467591888749082\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 600: 1.0114375957970818\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1100: 0.0361690300921592\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13400: 0.13701154010622751\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13425: 0.13683721367338694\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1125: 0.035940560344037496\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 700: 0.8903965254527118\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 725: 0.8642846488053428\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1150: 0.03569316723905301\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13525: 0.1360598922457138\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13550: 0.13587528938067583\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1175: 0.03534078696935854\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 825: 0.7737111475025162\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 850: 0.7544335321994389\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1200: 0.03499690764404174\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13650: 0.13520086336869194\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13675: 0.13503916856918144\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1225: 0.03510294595291382\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 950: 0.687909481605809\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 975: 0.6732481193466064\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1250: 0.035127264483040196\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13775: 0.13445246530230878\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13800: 0.13427948322154934\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1275: 0.03509675058075135\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 1075: 0.6237863446962695\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 1100: 0.6128865881094878\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1300: 0.035179125779048685\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13900: 0.13369764045610766\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13925: 0.1335238890296318\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1325: 0.03499904970721922\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 1200: 0.5720802192001914\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 1225: 0.5623283918205725\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1350: 0.03483948083020988\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14025: 0.13290925462781278\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14050: 0.1327407732130703\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1375: 0.03500305350476199\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 1325: 0.5283887722951202\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 1350: 0.5211149283777923\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1400: 0.035154677810870845\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14150: 0.1321356696543381\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14175: 0.13197108573458902\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 575: 0.8589327881938738\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 1450: 0.4934737644399163\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 1475: 0.48740823648396436\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1450: 0.03540761216555658\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14275: 0.13134367007321407\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14300: 0.13121070357817438\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 625: 0.7979757864177227\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 1575: 0.46394789441266937\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 1600: 0.45826782942545835\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1500: 0.03535807598712078\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14400: 0.13062033553672436\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14425: 0.13046101708365426\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 675: 0.745214069844396\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 1700: 0.4366535891715766\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 1725: 0.43174694457806756\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1550: 0.03544689612481893\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14525: 0.12989078528492026\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14550: 0.12973044428157682\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 725: 0.6987190579545909\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 1825: 0.4141321990956642\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 1850: 0.4095148622960708\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 750: 0.6788219829797745\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14650: 0.12916316839424716\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14675: 0.12902206982574205\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 775: 0.6601335175599783\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 1950: 0.39242422243627983\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 1975: 0.3892072923792691\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 800: 0.6422480664332397\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14775: 0.12846428646718455\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14800: 0.12831326610358185\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 825: 0.6252783814569315\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 2075: 0.3738339799436101\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 2100: 0.37019916918261775\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 850: 0.609500549974687\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14900: 0.12776056219796117\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14925: 0.1276220803138495\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 875: 0.5942916373397623\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 2200: 0.356280997908272\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 2225: 0.3529983108167359\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 900: 0.5802632584174474\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15025: 0.12708852040731525\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15050: 0.12695582622935048\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 925: 0.5665578860970768\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 2325: 0.3405237017462771\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 2350: 0.3378678347910457\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 950: 0.5536205881657569\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15150: 0.12637604145339318\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15175: 0.12622746689102798\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 975: 0.5416982365724368\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 2450: 0.32631796366440097\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 2475: 0.32354194625704125\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1000: 0.5308259936869144\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15275: 0.12563356510413248\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15300: 0.125489378272108\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1025: 0.5195786772959116\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 2575: 0.3135020374390975\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 2600: 0.31100638431599903\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1050: 0.5088596430669228\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15400: 0.12492387346297619\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15425: 0.1247887587563623\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1075: 0.49861255282926004\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 2700: 0.30151068024781397\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 2725: 0.2992676042703433\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1100: 0.48865509832447224\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15525: 0.12433357441728865\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15550: 0.1242188908021382\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1125: 0.47922669556736947\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 2825: 0.29015941334196027\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 2850: 0.28806685407106813\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1150: 0.4703447353402558\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15650: 0.12374569986976822\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15675: 0.12364957188495414\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1175: 0.4616457638413982\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 2950: 0.2804951855047385\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 2975: 0.27864358154505087\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1200: 0.45288014319259673\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15775: 0.12321601992524849\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15800: 0.12310831764406603\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1225: 0.44569015896898145\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3075: 0.2721736588730757\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3100: 0.2704998613053918\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1250: 0.43864424809366465\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15900: 0.12269649537169461\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15925: 0.12258608937363456\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1275: 0.43175390727233653\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3200: 0.26394021460891964\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3225: 0.26223073945182207\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1300: 0.425147738720362\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16025: 0.12215975231840638\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16050: 0.12204845733800068\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1325: 0.4183704117081075\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3325: 0.2561330099145819\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3350: 0.2545368503729827\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1350: 0.4118814833282872\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16150: 0.1215967267585865\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16175: 0.12147119135495817\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1375: 0.40619828914647754\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3450: 0.24890518480689144\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3475: 0.24752018746852927\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1400: 0.4005102605598846\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16275: 0.12100194086650094\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16300: 0.12089862827039138\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1425: 0.3952012466482426\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3575: 0.24237010962717312\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3600: 0.24120954664660127\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1450: 0.3899950469375171\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16400: 0.1205250913010223\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 25: 0.05585102633864153\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3650: 0.23875472864534783\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 50: 0.062304681456880645\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 75: 0.06821363020998736\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2325: 0.03556902291355855\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 100: 0.06384244739310815\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 125: 0.06443645242752973\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 150: 0.06494875332777156\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1475: 0.3206504509983174\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 175: 0.0645288549151571\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 200: 0.06410910442536988\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3700: 0.2362808531647032\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 225: 0.06478708343224651\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3725: 0.23504110188726585\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 250: 0.06553467698971507\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 275: 0.06502170857703525\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1500: 0.37969341305891674\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 300: 0.06394764301662993\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 325: 0.06280451974397203\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 350: 0.06665053940307449\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3750: 0.23401867758175357\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 375: 0.06560074298545562\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 400: 0.0647497526189909\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3775: 0.23288830217328663\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 425: 0.06550754621505107\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 450: 0.06506454767158074\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2375: 0.035498920710879914\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 475: 0.0654354436851511\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 500: 0.06494394814738189\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 525: 0.06525271881551903\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1525: 0.3127363914916994\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 550: 0.06622797241140771\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 575: 0.06638022598043672\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3825: 0.23073956008525748\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 600: 0.06623368029232855\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 625: 0.06706089424325619\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3850: 0.2296506995400535\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 650: 0.06688069364765677\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 675: 0.06655648266596943\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1550: 0.3702184491724737\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 700: 0.06605249721766865\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 725: 0.06557183482063983\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 750: 0.06535162677179324\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3875: 0.22863409489465336\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 775: 0.06512356109344476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 800: 0.06496654059550565\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3900: 0.22782123847088467\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 825: 0.06655975217516463\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 850: 0.06591932335784591\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2425: 0.03555652589222762\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 875: 0.0654793121414758\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 900: 0.06458937959640429\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 925: 0.06451600746813269\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1575: 0.3054158636566902\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 950: 0.06409985526830975\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 975: 0.06348675442392376\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3950: 0.2258063721832454\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1000: 0.06335262844814861\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 3975: 0.2249140087177731\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1025: 0.06272968545009484\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1050: 0.06262891940793322\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1600: 0.36146726919105276\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1075: 0.06278432083378416\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1100: 0.06321046559915307\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1125: 0.06295657166479052\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4000: 0.2240506387312198\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1150: 0.0630194443064671\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1175: 0.06315008697488861\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4025: 0.22308382322687817\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1200: 0.06311595155436711\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1225: 0.06311267161709778\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2475: 0.03552442969869578\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1250: 0.0633020535549731\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1275: 0.0634633233205429\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1300: 0.0633634669741038\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1625: 0.29860099794257144\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1325: 0.06314305063081194\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1350: 0.06289742671111728\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4075: 0.22095878632122112\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1375: 0.06386714992489115\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1400: 0.06363264923926701\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4100: 0.21993612897758394\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1425: 0.06343361966478385\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1450: 0.06370123350894448\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1650: 0.35317465877668425\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1475: 0.06357676986362233\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1500: 0.06368963346089003\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1525: 0.0636177320066996\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4125: 0.21895457668879043\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1550: 0.06374977686263118\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1575: 0.06409023540258031\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4150: 0.2179505595296105\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1600: 0.06423255828828588\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1625: 0.06419375134801242\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2525: 0.03558150984017215\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1650: 0.06454210862034938\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1675: 0.06451321465836372\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1700: 0.06441235268129865\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1675: 0.2920752935673096\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1725: 0.06421972669928463\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1750: 0.06406104295369004\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4200: 0.2161023500499626\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1775: 0.06400580510452823\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4225: 0.21508891588567408\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1800: 0.06393588938389763\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1825: 0.06385123988961935\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1700: 0.3452706487082383\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1850: 0.06461401953181799\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1875: 0.06432503196490773\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1900: 0.0641801383652905\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4250: 0.21411055667386117\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1925: 0.06376783868115753\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1950: 0.06372541825540023\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1975: 0.06346252506248848\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4275: 0.21328704972702422\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 2000: 0.06325488712934749\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2575: 0.035379139443212956\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 2025: 0.06319878902628351\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 2050: 0.06290025014541022\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m epoch:  1\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4325: 0.21134112648857015\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4350: 0.21049995447937184\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1750: 0.33767790588310787\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 75: 0.03380245239939541\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 100: 0.031032110695377924\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4425: 0.20769813495767067\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4450: 0.2068576855091064\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4475: 0.20603647325796662\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1800: 0.33022480309423474\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 200: 0.030134790394222365\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 225: 0.03269509742346903\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4550: 0.20338572175671848\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4575: 0.2025940837592772\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4600: 0.20182304027042372\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1850: 0.32365979962655017\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 325: 0.040081965704806724\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4650: 0.20022386573109857\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 350: 0.041329284357572244\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4675: 0.1994483826732273\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4700: 0.19864499126885463\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4725: 0.1977408591930868\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1900: 0.3175894356920923\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 450: 0.04106679420827681\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4775: 0.19604144375347068\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 475: 0.04019067072971283\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4800: 0.19520720703085923\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4825: 0.19441548442998852\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4850: 0.19376750689926414\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1950: 0.31191710020200564\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 575: 0.03805495188639844\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4900: 0.19272915452999081\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 600: 0.037667640225142046\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4925: 0.19218396615388278\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4950: 0.19161200661767236\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 4975: 0.1909728495675565\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 2000: 0.3061640147930011\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 700: 0.0376543541375057\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5025: 0.18972569713658602\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 725: 0.03775335365446718\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5050: 0.18919169798255944\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5075: 0.18860290194180304\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5100: 0.18803390042863183\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 2050: 0.3008526263231548\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 825: 0.036760589046876485\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5150: 0.1868290060552208\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 850: 0.036533764648792225\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5175: 0.18621898710410503\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5200: 0.185728950699357\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5225: 0.1851430565402197\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 2100: 0.2951801711720015\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 950: 0.03649776079526514\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5275: 0.183930535661807\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 975: 0.03650834544740904\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5300: 0.18328280294801652\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5325: 0.1826343284373307\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5350: 0.18213742917753914\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 2150: 0.2902569205669123\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1075: 0.03701539997117009\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5400: 0.18091890436851557\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1100: 0.03698780078943607\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5425: 0.18046299147496883\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5450: 0.1799883266980119\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5475: 0.17946530848216333\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 2200: 0.28483195202827283\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1200: 0.03722021347804305\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5525: 0.17858991852959374\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1225: 0.03713191160457019\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5550: 0.17805356970616942\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5575: 0.17769867828698419\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5600: 0.17721124336412425\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2250: 0.23629930227829352\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1325: 0.037226493991296385\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5650: 0.17637166084315775\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1350: 0.03732613096867377\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5675: 0.175959480779865\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5700: 0.17554926953452082\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5725: 0.17507599017271716\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2300: 0.23248088911094744\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1450: 0.037926297347605276\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5775: 0.17414563277915718\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1475: 0.03844714379769758\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5800: 0.1737702357954675\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5825: 0.17332178547452784\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5850: 0.17286506995972659\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2350: 0.2289833109421616\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1575: 0.03907033038468817\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5900: 0.172049697760682\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1600: 0.03909355091202997\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5925: 0.17162271593870274\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5950: 0.1711552017018152\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 5975: 0.17076714666026438\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2400: 0.22552870505178968\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1700: 0.038940459167771384\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6025: 0.16992883189967775\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1725: 0.03909015822191419\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6050: 0.16949656757916717\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6075: 0.16908024006958192\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6100: 0.16876676982520605\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2450: 0.22252313680566696\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1825: 0.03981713846307535\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6150: 0.16801816820212434\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1850: 0.039621223296751454\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6175: 0.16766075457872348\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6200: 0.16723413937357104\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6225: 0.16687523993584943\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2500: 0.21945055808499456\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1950: 0.03923178481258219\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6275: 0.16616186131660146\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 1975: 0.039277026032818234\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6300: 0.16580035482333708\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6325: 0.16537715228108815\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6350: 0.16498228818315674\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2550: 0.2163295491982032\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2075: 0.03841236931200319\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6400: 0.16426447594103594\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2100: 0.03834480264585831\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6425: 0.16391842507385804\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6450: 0.16365206256165898\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6475: 0.1632807534767322\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2600: 0.21289950494582838\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2200: 0.03747476308016551\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6525: 0.16259270326248673\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2225: 0.03736490852474183\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6550: 0.1622332125756079\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6575: 0.1619222835551402\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6600: 0.16157693173985038\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2650: 0.20961892344646227\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2325: 0.03651322633601345\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6650: 0.16094729794534204\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2350: 0.03642230380995898\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6675: 0.16064521430808465\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6700: 0.16026345954445162\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6725: 0.15992462866591656\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2700: 0.20652907552119965\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2450: 0.0357805976724283\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6775: 0.1593173279111339\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2475: 0.03570275806027554\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6800: 0.15895175706812606\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6825: 0.1586263962944751\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6850: 0.15829229039274306\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2750: 0.2033735302388668\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2575: 0.03526671614947738\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6900: 0.1575717019917531\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2600: 0.035150224480967156\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6925: 0.15723647028821375\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6950: 0.15695321625715\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 6975: 0.1565896820232913\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2800: 0.20046910852626232\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2700: 0.03472195536636045\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7025: 0.15595751990436565\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2725: 0.03456540567955267\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7050: 0.15561095132646052\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7075: 0.15521917492050888\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7100: 0.15484547696547735\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2850: 0.19777820904232693\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2825: 0.03386649311748869\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7150: 0.15426117919827487\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2850: 0.03375428276322374\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7175: 0.15394240327334383\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7200: 0.15359374519006652\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7225: 0.15332076591006916\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2900: 0.1951067680712982\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2950: 0.03337103432471079\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7275: 0.15283180008849603\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 2975: 0.03323089692337114\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7300: 0.1525686223846334\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7325: 0.15231281928958867\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7350: 0.15199279038684577\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2950: 0.1925229008459502\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3075: 0.03296528702954112\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7400: 0.15140936545429065\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3100: 0.03295597100508589\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7425: 0.15115706132912243\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7450: 0.1508733900877\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7475: 0.15068526389811618\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7500: 0.15046702984794197\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7525: 0.15019002734611517\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3875: 0.030858791463422557\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3250: 0.03240919337894257\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7575: 0.14977851422201602\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3275: 0.03236834925544077\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7600: 0.14954422778044796\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3900: 0.030856710836300268\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7625: 0.14932512990128036\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7650: 0.14920783369782917\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3925: 0.03090330313664274\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3375: 0.03203293962752515\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7700: 0.14875752870225523\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3400: 0.03208968392527929\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7725: 0.14848188317478048\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3950: 0.030899018390042284\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7750: 0.14828420196483363\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7775: 0.14807899728606805\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3975: 0.030929415832323448\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3500: 0.03172536861502782\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7825: 0.14761591496714466\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3525: 0.03160943523948064\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7850: 0.14736913678153998\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 4000: 0.03095349983799679\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7875: 0.147127828876584\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7900: 0.1468294730436431\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 4025: 0.030932544351928513\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3625: 0.03139211906049529\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7950: 0.14640056133691262\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3650: 0.03136006448599701\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 7975: 0.146155520875312\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 4050: 0.030934153267765923\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8000: 0.14593783587919462\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8025: 0.14566379538861268\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 4075: 0.030916397149026095\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3750: 0.031097102503631808\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8075: 0.14517569796827365\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3775: 0.031065125228424282\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8100: 0.1449051412720058\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 4100: 0.03096061843792374\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8125: 0.14478447545167203\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 25: 0.07346697986125945\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8150: 0.14460709648191833\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 50: 0.07680121004581451\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3275: 0.20799335134130342\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 75: 0.07503110279639562\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 100: 0.07560519557446241\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3875: 0.030880490831559478\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 125: 0.07606376174092293\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3900: 0.030901660834230522\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3300: 0.17638032493608383\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 150: 0.07785457337896029\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3950: 0.030867302218955704\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 175: 0.0774808586708137\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8275: 0.1434266346966105\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 3975: 0.030901343621576788\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 200: 0.07716443964047358\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8300: 0.14315319438166188\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 225: 0.07680073121148678\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8325: 0.14285415515430863\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 250: 0.07507104846276343\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8350: 0.142605174239934\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 275: 0.07478254888037389\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 300: 0.07456727037051071\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3350: 0.1744550078234691\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 325: 0.07489377521121732\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4075: 0.03087135416677506\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 350: 0.07491709158755838\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4100: 0.030808330768161727\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4125: 0.030767974936193435\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 375: 0.07492960796629389\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8450: 0.1415827707341905\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4150: 0.030668533308263544\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 400: 0.07582512691500597\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8475: 0.14135431712989183\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3400: 0.20213925507268868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 425: 0.07602500324630562\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8500: 0.14114278194430174\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 450: 0.07555905236241718\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8525: 0.1410499403485482\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 475: 0.07582738708410608\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3425: 0.20111126958990783\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 500: 0.07499902138020843\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m epoch:  2\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4250: 0.030498390653233988\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4275: 0.03049330272475749\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8600: 0.14047731826899487\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3450: 0.20008431752839975\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 25: 0.01753617630340159\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8650: 0.13994643452367342\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3475: 0.19910257680160698\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 50: 0.015777943676803262\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4400: 0.03003464805094402\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8725: 0.13907965190568206\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3500: 0.1980951564923328\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 75: 0.019243763202490907\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8775: 0.13851327174497394\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3525: 0.19707464645880915\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 100: 0.023494864668464288\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4525: 0.029742845612314713\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8850: 0.137804848229579\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3550: 0.19604788519881508\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 125: 0.02190654329303652\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8900: 0.1372798121586315\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3575: 0.1950946280771176\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 150: 0.020743294849526136\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4650: 0.029447066339973037\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 8975: 0.13652065417512377\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3600: 0.1941263109359554\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 175: 0.02076204674584525\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 9025: 0.1360289784389325\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3625: 0.19321903035115323\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 200: 0.019999854189518374\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4775: 0.02905302957978303\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 9100: 0.1352383585680298\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3650: 0.19226042110441022\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 225: 0.019994264429486874\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 9150: 0.1347906817375686\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3675: 0.19136992566103786\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 250: 0.019993459828430787\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 4900: 0.029040987009560624\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 9225: 0.13414125432677726\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3700: 0.1904514356290393\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 275: 0.020425069659144025\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 9275: 0.13375525397454097\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3725: 0.18955790613427909\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 300: 0.020878253615034432\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5025: 0.02897434227931486\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 9350: 0.13307197445743021\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3750: 0.188677504653049\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 325: 0.021055581697370283\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 9400: 0.1326587696751286\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3775: 0.1877609740121796\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 350: 0.020974562776308242\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5150: 0.028914526730779506\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 9475: 0.13201579899456792\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3800: 0.18687301724444536\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 375: 0.02208812710782513\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 9525: 0.1315979545263621\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3825: 0.1859117236653696\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 400: 0.022491156571923056\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5275: 0.028832344397820005\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 9600: 0.1309584684652479\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3850: 0.18502561043058946\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 425: 0.022604248004499825\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 9650: 0.1306266743039026\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3875: 0.1842377922456231\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 450: 0.023249331134397328\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5400: 0.028626484187920544\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 9725: 0.13012372431062474\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3900: 0.1834245823474768\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 475: 0.023302272881846876\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 9775: 0.1297750482809893\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3925: 0.1827096897224261\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 500: 0.02319823453866411\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5525: 0.02875360893847698\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 9850: 0.12914191544019005\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3950: 0.18195025612924343\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 525: 0.022747451046016068\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 9900: 0.12869126781173618\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3975: 0.1812533281184147\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 550: 0.02221992456809279\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5650: 0.02892164225119709\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 9975: 0.12817922551233035\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 4000: 0.18055990454304266\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 575: 0.021810681964574464\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10025: 0.12785698953591243\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 4025: 0.17977781490862416\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 600: 0.021407209167664407\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5775: 0.02894384720753672\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10100: 0.12720202386707452\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 4050: 0.1790499205554456\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 625: 0.02119337829486467\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10150: 0.12677080698784732\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 4075: 0.17828243933578072\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 650: 0.02088421896104522\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 5900: 0.028997661816867578\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10225: 0.12618266079969326\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 4100: 0.17765177469070229\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 675: 0.020678471513135635\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 25: 0.0684440348483622\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10275: 0.12565699498393174\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 25: 0.06778300270438194\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 50: 0.06948456746526062\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10300: 0.12545962416749726\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 50: 0.0699498687312007\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 75: 0.06778744087244074\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 700: 0.02028708163378594\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 100: 0.06804063040297478\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6025: 0.028964621532036723\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 125: 0.06787431774660944\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6050: 0.028952163071733608\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 150: 0.06897902749789257\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6075: 0.028967797261325753\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 175: 0.0688935844520373\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6100: 0.029007660760287792\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 175: 0.0695651884643095\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 200: 0.06811587938340381\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10425: 0.12429430509031342\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6125: 0.029007707805464245\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 200: 0.06920999031979591\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 225: 0.06717541209732493\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10450: 0.12403302884430059\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 225: 0.0681462685805228\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 250: 0.06593463898636401\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10475: 0.12379019834550468\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 250: 0.06675439141318203\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 275: 0.06582460346716372\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 775: 0.019595668002955556\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 275: 0.0664749180965803\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 300: 0.06582677533073972\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6200: 0.02897967053444107\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 325: 0.0659403933169177\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6225: 0.028972737863193498\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 350: 0.06610152893965798\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6250: 0.02899215531234746\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 350: 0.06665090933708208\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 375: 0.06606127044185996\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10575: 0.12297427201272873\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6275: 0.028997150560419134\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 375: 0.0667704802279671\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 400: 0.06672796575934627\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10600: 0.12271390821515936\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6300: 0.02898884419788027\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 400: 0.06733626928413287\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 425: 0.06693930666784154\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10625: 0.1225067666422603\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 425: 0.06757645833360798\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 450: 0.06645261857555144\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10650: 0.12228767068987219\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 450: 0.0673276001897951\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 475: 0.06657020080344457\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6350: 0.028948632850772087\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 500: 0.06573447422590106\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10675: 0.12208726474636865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=19352)\u001b[0m epoch:  1\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 500: 0.06654309184290469\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10700: 0.12186658568513824\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m epoch:  1\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10725: 0.12164411198661504\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 875: 0.018856588975958793\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6450: 0.028975467288490094\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10775: 0.12124312870846758\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6475: 0.028969121955326825\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10800: 0.12100784183107828\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 900: 0.01868326210251932\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10825: 0.12081872444718568\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10850: 0.12061124214895948\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 925: 0.01848280457908766\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10900: 0.12019135313758861\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10925: 0.11998701972085998\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10950: 0.11974659738996142\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 10975: 0.11951759956666912\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 975: 0.018284575646558106\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6700: 0.028972294187756844\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11025: 0.11908231690151949\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6725: 0.028966170491361392\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11050: 0.1188865039976576\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1000: 0.018357512367103482\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11075: 0.11868706341040831\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11100: 0.11848501125053132\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1025: 0.01824819432392648\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6825: 0.02895112709741295\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11150: 0.11810274804580666\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6850: 0.02893699733015591\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11175: 0.11791283812765704\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1050: 0.018120936344826727\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11200: 0.11770498760621324\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11225: 0.11753579140340942\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1075: 0.01797343017649837\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6950: 0.028901054453364994\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11275: 0.11718000513610266\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 6975: 0.02887816703186423\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11300: 0.11700904301761375\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1100: 0.017722505985743323\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11325: 0.11679808796521829\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11350: 0.11659858522101922\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1125: 0.017511191443152104\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7075: 0.02881670258611308\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11400: 0.11623974313827364\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7100: 0.02879115082957994\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11425: 0.11602099565483036\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1150: 0.017356527107388147\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11450: 0.1158492881074656\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11475: 0.11566738686481341\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1175: 0.01716993650052439\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7200: 0.028745182634236498\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11525: 0.11530479172204643\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 7225: 0.02875925421521328\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11550: 0.11510416610851189\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1200: 0.01695124859363811\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11575: 0.11491431207365038\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11600: 0.11479428237493391\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 375: 0.047506457559143504\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11650: 0.11440502962793395\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11675: 0.11424535940138386\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11700: 0.11407391721558728\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11725: 0.11387605879154837\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 425: 0.04803070457268725\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11775: 0.11351928450823034\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11800: 0.11335570888555686\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11825: 0.1131778074645358\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11850: 0.11301239662015204\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 475: 0.048565752650856185\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11900: 0.11265075785239856\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11925: 0.11245031241536803\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11950: 0.1123268262109177\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 11975: 0.1121546159841041\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 525: 0.04729538414782534\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12025: 0.11183977934676478\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12050: 0.11167907293476922\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12075: 0.11150220481500364\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12100: 0.11135949693224657\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 575: 0.04550121115986258\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12150: 0.11107548760806298\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12175: 0.11094688322671782\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12200: 0.11081217512211043\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12225: 0.1106679825414225\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 625: 0.0442294869845733\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12275: 0.1103480939256502\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12300: 0.11018037404926902\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12325: 0.11001307193202493\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12350: 0.10984716003641098\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 675: 0.043238705177791416\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12400: 0.10952780996385802\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12425: 0.10937290826053812\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12450: 0.10919697724918234\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12475: 0.10903840292004037\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 725: 0.04197795930912654\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12525: 0.1087040166262904\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12550: 0.1085590686744547\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12575: 0.10839206421412097\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12600: 0.10820027218978794\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 775: 0.041423473239035134\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12650: 0.10786084062500241\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12675: 0.1077204160945969\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12700: 0.10758031927564894\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12725: 0.1073986269942037\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 825: 0.04080780681039235\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12775: 0.1070990332264265\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12800: 0.10693221083948061\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12825: 0.10677950698930565\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12850: 0.10660774127183983\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 875: 0.0402547135687034\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12900: 0.10628469558432585\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12925: 0.10610326642332224\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12950: 0.10593515682930232\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 12975: 0.10577209513967539\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 925: 0.03978056110278074\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13025: 0.10544430724301353\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13050: 0.10530881192898459\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13075: 0.10518472625923175\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13100: 0.105085555181871\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 975: 0.0394929775241046\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13150: 0.1048558185834388\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13175: 0.1047189642115588\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13200: 0.10456467601901033\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13225: 0.1044288743267475\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1025: 0.039424628574832726\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13275: 0.10419042525600103\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13300: 0.10405505128386885\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13325: 0.10394999066888447\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13350: 0.10380882339384834\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1075: 0.038964810505357765\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9075: 0.02904191276514595\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13400: 0.1035660201738373\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9100: 0.029042145019620377\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13425: 0.103449415841005\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 1950: 0.017858289683503337\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13450: 0.10331947160683283\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13475: 0.10316844427139768\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1125: 0.0384065806614235\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13525: 0.1028849683668961\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13550: 0.10275275570595194\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13575: 0.1026075991819069\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13600: 0.10249040142254243\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1175: 0.037821413410847333\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9325: 0.028965615121508515\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13650: 0.10226799833349928\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9350: 0.028955631592553134\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13675: 0.10215847869453461\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2050: 0.018095112442132653\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13700: 0.1020562269075104\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13725: 0.10194103023704847\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1225: 0.03752057560881106\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9450: 0.02891934989924757\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13775: 0.10177734483102718\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9475: 0.028920912427915564\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13800: 0.10166609562288732\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2100: 0.017935991151344276\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13825: 0.10155673417040217\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13850: 0.10147063300432133\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1275: 0.03757159193238134\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9575: 0.02888651127999651\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13900: 0.10126536134721313\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9600: 0.028858805712901205\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13925: 0.10114299585317517\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2150: 0.018042005924981758\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13950: 0.10102625418588349\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 13975: 0.10091771464100964\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1325: 0.03753238921835667\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9700: 0.028978952745125106\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14025: 0.1007151795756163\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9725: 0.02900243717641091\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2200: 0.017871257738112368\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14050: 0.10059386409014576\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14075: 0.10050386367957927\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14100: 0.10040165665138394\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1375: 0.037574405562047934\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9825: 0.029019407170206193\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14150: 0.10017241320671726\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9850: 0.02901914929969917\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2250: 0.01773341697807579\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14175: 0.10005814569119596\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14200: 0.099945463411808\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14225: 0.09984642501896523\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1425: 0.03795230622901663\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9950: 0.02901462538878319\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14275: 0.09963625135526225\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 9975: 0.02904473731932954\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2300: 0.017635744821151173\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14300: 0.09954507511252089\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14325: 0.09945969999775661\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14350: 0.09934479292778753\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1475: 0.03806647103366647\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10075: 0.029028878004453952\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14400: 0.0991440505670198\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10100: 0.02902117578559561\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2350: 0.01765379143493656\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14425: 0.09902943295702371\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14450: 0.0989401857885377\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14475: 0.098840977724573\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1525: 0.038188776036602305\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10200: 0.02898272909030008\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14525: 0.09864732354657268\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10225: 0.028948345220387882\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2400: 0.01762658635708552\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14550: 0.09853411231320083\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14575: 0.098431281746573\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14600: 0.09832753992257046\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1575: 0.038360334336905486\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10325: 0.028807258643762414\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14650: 0.0981487693228795\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10350: 0.028761674730602937\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2450: 0.017745715257704582\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14675: 0.0980519580257788\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14700: 0.09795996403044742\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14725: 0.0978520052639658\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1625: 0.03853388595996568\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10450: 0.028617519468125335\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14775: 0.09765581388420928\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10475: 0.028563449422818157\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2500: 0.017809942650713492\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14800: 0.09755801931712356\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14825: 0.097467487977554\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14850: 0.09737756105951721\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1675: 0.0386022428082608\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10575: 0.028419872202386065\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14900: 0.09716902604301311\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10600: 0.028363704371086502\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2550: 0.017818416093177088\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14925: 0.09707197403678475\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14950: 0.09698451741725961\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 14975: 0.09687298324033451\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1725: 0.03861675119418921\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10700: 0.02823809387253719\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15025: 0.09668937495095402\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10725: 0.02820625782875064\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2600: 0.01763590189441367\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15050: 0.0965864188393682\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15075: 0.09648286606974078\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15100: 0.09637587338135233\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1775: 0.03851056845373356\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10825: 0.028063786575753123\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15150: 0.09618442087716146\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10850: 0.02803654036349802\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2650: 0.017468730826408575\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15175: 0.09608077381383777\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15200: 0.09599099319983954\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15225: 0.09588275154227982\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1825: 0.038691827573826255\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10950: 0.027902317948185026\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15275: 0.09566098262534843\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 10975: 0.027860241058716925\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2700: 0.01733582100860076\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15300: 0.09555928359248046\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15325: 0.09546390229760048\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15350: 0.09536796734024333\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1875: 0.039023544767871496\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11075: 0.027721998680769397\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15400: 0.095157275765902\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11100: 0.027682621207024388\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2750: 0.017161897166288163\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15425: 0.09506798249836071\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15450: 0.09500221840572459\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15475: 0.09493343366991096\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1925: 0.039433190757166836\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11200: 0.027567128283126945\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15525: 0.09477298278284042\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11225: 0.027532981294839262\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2800: 0.017020475847026057\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15550: 0.09469858539652792\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15575: 0.0946052324594913\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15600: 0.09452129970913889\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 1975: 0.03964553109063661\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11325: 0.02741820836347149\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15650: 0.09438026999024181\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11350: 0.02738384107496776\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2850: 0.0168972909273304\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15675: 0.09431792817668684\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15700: 0.09426798489044594\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15725: 0.09417722234523997\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 2025: 0.03976818540708427\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11450: 0.027264278089243428\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15775: 0.09404326026198118\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11475: 0.02724016366501439\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2900: 0.01679893728478607\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15800: 0.09397238414716273\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15825: 0.09389873541585272\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15850: 0.09385393371658377\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 2075: 0.039912934440289094\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11575: 0.02714432967993364\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15900: 0.09370672626008826\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11600: 0.027139535660168395\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 2950: 0.016649813810664316\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15925: 0.09364246212002482\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15950: 0.09357820709519889\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 15975: 0.09350799863138899\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 2125: 0.039804339348502894\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11700: 0.027006044606270235\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 16025: 0.09336297594651569\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11725: 0.026964920410732014\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3000: 0.016538855937498737\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 16050: 0.09328760056859071\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 16075: 0.09320618931713556\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 16100: 0.09311474754749104\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 2175: 0.039862296118497335\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11825: 0.026846317746232255\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 16150: 0.09297964472061632\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11850: 0.026829617711528007\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3050: 0.016452805568673075\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 16175: 0.09288848874220694\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 16200: 0.09281352126915195\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 16225: 0.09273294167692138\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 2225: 0.03969603292570774\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11950: 0.026699556969575634\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 16275: 0.09257328906020697\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 11975: 0.026674895930791878\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3100: 0.01634839770701928\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 16300: 0.09249506546520632\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 16325: 0.09244882080392879\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 16350: 0.09238330084692213\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 2275: 0.03945438780535299\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12075: 0.02657210254346119\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average T loss at step 16400: 0.0922509872800743\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12100: 0.02654894410831467\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 25: 0.05154673225653823\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 50: 0.062108254111080896\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3150: 0.01619947750471987\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 2300: 0.039376399918236646\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 75: 0.06519731460158558\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 100: 0.06062921100310632\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 125: 0.0612538998197997\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12125: 0.026527097878156748\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 150: 0.062405710273596926\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 175: 0.062133703784368534\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12150: 0.026513233326246492\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 200: 0.06212049217101594\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 225: 0.062396220508122095\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12175: 0.026499365488474484\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 250: 0.06335095035671838\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 275: 0.06294990494638279\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 300: 0.06198647533041367\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2325: 0.03212465736868551\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 325: 0.060864640952987915\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 350: 0.06378101481534292\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12200: 0.02649084601152846\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 375: 0.06229215622986279\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12225: 0.02648543937516064\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 400: 0.06127389851977569\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 425: 0.06191096880916602\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 2350: 0.039409267161180206\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 450: 0.061440119189970166\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 475: 0.06181023296221515\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 500: 0.06163467694060819\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12250: 0.0264599942252799\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 525: 0.06203359874751186\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 550: 0.06323978176032224\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 575: 0.06369300886876021\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12275: 0.026425433840724506\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 600: 0.06351725558137938\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12300: 0.026404329627773955\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 625: 0.06411910998903914\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 650: 0.06399644137140757\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 675: 0.06386114233796468\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2375: 0.03203954342217184\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 700: 0.06360618633852157\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 725: 0.06322027067712042\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12325: 0.026384560480698142\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 750: 0.06297934351019406\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12350: 0.026350126732048285\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 775: 0.06290222795661389\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 800: 0.062453390910050074\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 825: 0.06401886113012689\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3250: 0.015899673199285574\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 2400: 0.0393480860060663\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 850: 0.0633490272671128\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 875: 0.06278058689232317\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 900: 0.061865744437964165\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12375: 0.026320274220941815\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 925: 0.06173755356507074\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 950: 0.061249192262889426\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12400: 0.026299956739556717\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 975: 0.06089662551584367\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1000: 0.060724006837874184\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12425: 0.026272092794879316\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1025: 0.059999482465181046\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1050: 0.059848497034510645\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1075: 0.060162774923543154\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2425: 0.03207184778587634\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1100: 0.06044500857979901\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1125: 0.06015642914152058\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12450: 0.026240395785502933\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1150: 0.06022973854071334\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12475: 0.02623442947334481\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1175: 0.060415537858826474\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1200: 0.06042385288295312\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 2450: 0.03961326768308194\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1225: 0.06045956213345005\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1250: 0.06057497744126595\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1275: 0.06077942320605051\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12500: 0.02620909646365326\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1300: 0.06071541507856567\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1325: 0.06053253850377415\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12525: 0.026164101938617845\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1350: 0.060288072465105126\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1375: 0.06104512060709319\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12550: 0.026126558982527262\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1400: 0.06069460452931837\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1425: 0.06045462233746085\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1450: 0.06067402775722038\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2475: 0.03199577630962001\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1475: 0.06053315449006346\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1500: 0.06064278460646649\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12575: 0.026097595759303052\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1525: 0.06066654718118876\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12600: 0.026060409396875275\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1550: 0.060826201101116306\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1575: 0.06125020918588335\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1600: 0.06149701128819743\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3350: 0.015830954825718464\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 2500: 0.03969893666571006\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1625: 0.06143010146318165\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1650: 0.06171400365861094\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1675: 0.061702218084815894\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12625: 0.02602512670299707\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1700: 0.06168011381267313\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1725: 0.06158433601695899\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12650: 0.025993429941896868\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1750: 0.06147475237990147\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1775: 0.061415530598970806\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12675: 0.025965975037669967\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1800: 0.061424333921729864\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1825: 0.061184676836722386\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1850: 0.06195313319601229\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2525: 0.03204469193560348\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1875: 0.061663183099466064\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1900: 0.06145611217561006\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12700: 0.02594727618557066\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1925: 0.06103477814307364\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12725: 0.025908025459567434\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1950: 0.06096929524637138\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 1975: 0.06066644793812516\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 2000: 0.06059158376080449\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3400: 0.015734404065712145\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 2550: 0.03964653217151542\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 2025: 0.06050838194494308\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m Average E loss at step 2050: 0.06015434338315106\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12750: 0.025874909185917987\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13060)\u001b[0m epoch:  1\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12800: 0.0258204617934274\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=9460)\u001b[0m C:\\Users\\NicHer\\AppData\\Local\\anaconda3\\envs\\envname2\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m   warnings.warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12825: 0.025798796636807994\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12850: 0.025764430785732973\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3450: 0.015768231512962645\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12900: 0.02571492511542686\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12925: 0.02567451762315325\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3475: 0.015777122139760708\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12950: 0.025643642818195426\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 12975: 0.025611107941904878\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3500: 0.01576101081163506\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13025: 0.025552516361086155\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13050: 0.025548691401155857\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3525: 0.015744261701978537\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13075: 0.025536132994961873\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13100: 0.025541100502768973\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3550: 0.01572912113620116\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13150: 0.025526067461455377\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13175: 0.025511323840063452\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3575: 0.015753886911572753\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13200: 0.02548848001373258\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13225: 0.025472038347293433\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3600: 0.015743652910140453\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13275: 0.025442242171974255\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13300: 0.02542694631378664\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3625: 0.015738904251496242\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13325: 0.025422222446941305\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13350: 0.025398477627788404\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3650: 0.01571034174202383\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13400: 0.025374643699699967\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13425: 0.025372397666033948\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3675: 0.015691352113673535\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13450: 0.025356349791128573\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13475: 0.025324585191209597\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3700: 0.015668355218906464\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13525: 0.025269894389172154\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13550: 0.025249098007260294\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3725: 0.0156613120507963\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13575: 0.025220281245268608\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13600: 0.02521100990805387\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3750: 0.015645126665216714\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13650: 0.025194281927848724\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13675: 0.025190917699783148\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3775: 0.015621912550749349\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13700: 0.0251750123763895\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13725: 0.025164331829717987\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3800: 0.01560597289695579\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13775: 0.025188409521331303\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13800: 0.02518417040671815\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3825: 0.01557490678615579\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13825: 0.025177200687027144\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13850: 0.025171451743136664\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3850: 0.015559950850306037\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13900: 0.02517185657544105\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13925: 0.02515509983793198\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3875: 0.015570654814480804\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13950: 0.02513696952653441\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 13975: 0.025125585405838785\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3900: 0.015584281922444158\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14025: 0.02511260472026659\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14050: 0.025095880286307905\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3925: 0.0156273420906817\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14075: 0.025102954747790215\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14100: 0.025096345527600913\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3950: 0.015633724542246995\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14150: 0.02507193045599796\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14175: 0.025052328640829655\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 3975: 0.01568208199646397\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14200: 0.025040307660645238\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14225: 0.025037189469452697\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 4000: 0.015716077644920005\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14275: 0.02502855896829632\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14300: 0.025030230206095593\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 4025: 0.015712634630068147\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14325: 0.02502713507328767\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14350: 0.02500634012894978\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 4050: 0.01572780843186379\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14400: 0.02499467556714838\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14425: 0.024978811426716797\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 4075: 0.01573038967715168\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14450: 0.02498495967506387\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14475: 0.024978148381336652\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average T loss at step 4100: 0.015765977165233713\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 25: 0.08305139379575849\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14500: 0.024971117098066566\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 50: 0.08900018655695022\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14525: 0.02496019794650856\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14550: 0.02494611810540174\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 75: 0.08590762069448829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14575: 0.0249308816878677\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 100: 0.08634347418788821\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14600: 0.02492015626982583\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 125: 0.08630823213234544\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3300: 0.03583636431612611\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 150: 0.0881621622821937\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 175: 0.08717704773215311\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14625: 0.024907247852056454\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 200: 0.08680909996444824\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14650: 0.02491839086625478\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14675: 0.02490840569757789\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 225: 0.08619129962029143\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3325: 0.02846931558789482\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 250: 0.08422958666132763\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14700: 0.024894981236120037\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14725: 0.024882584024176826\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 275: 0.08401890519032763\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3350: 0.03577735328098147\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 775: 0.46437575364966066\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 300: 0.08403807129749717\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14775: 0.02486074042735013\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 325: 0.08458104088078611\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14800: 0.02484735828554541\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 350: 0.08445333243780104\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3375: 0.035691377543154414\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 375: 0.08441937426757068\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3375: 0.028311725065603647\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 400: 0.08547721157810884\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14825: 0.024846263622997005\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14850: 0.024843955821003325\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3400: 0.035606789900476166\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 425: 0.0854190586159444\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 825: 0.44080817382839144\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 450: 0.08496924777748063\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14875: 0.02483666500400658\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 475: 0.0849867234234453\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14900: 0.024820556969741468\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14925: 0.024809539265416045\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m Average E loss at step 500: 0.08411981739616022\n",
      "\u001b[36m(train_model pid=11644)\u001b[0m epoch:  3\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3425: 0.02818841385944869\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 14975: 0.024793853431082803\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=13424)\u001b[0m C:\\Users\\NicHer\\AppData\\Local\\anaconda3\\envs\\envname2\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 875: 0.4196419049517385\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15025: 0.024775457714039086\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15050: 0.024763862461620746\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15075: 0.024750362039735012\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15100: 0.024736986517784183\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3500: 0.0356711617896799\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15150: 0.0247179568326653\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15175: 0.02470650057969517\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15200: 0.024698326549466736\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15225: 0.024682255689469922\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 3550: 0.03561561175172781\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15275: 0.024647139646842658\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15300: 0.024636120191904424\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15325: 0.024624980835618752\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15350: 0.02462207733525789\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1025: 0.3691241588130048\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15400: 0.02459439922449109\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15425: 0.02458752409774924\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15450: 0.02459568440716738\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15475: 0.024599390393947717\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1075: 0.3549295054856948\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15525: 0.024597178970256612\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15550: 0.02461397208388703\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15575: 0.02460753120269739\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15600: 0.024605891143650624\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1125: 0.3418450583289895\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15650: 0.0246202029496749\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15675: 0.02463470793116401\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15700: 0.02465537786813361\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15725: 0.024646368912594772\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1175: 0.3299494837505862\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15775: 0.024654904862007172\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15800: 0.024658563672140338\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15825: 0.024661267374049174\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15850: 0.02467980032789068\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1225: 0.3192699534913563\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15900: 0.024681484234971327\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15925: 0.02468995051013879\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15950: 0.024698034742222402\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 15975: 0.02470266223023618\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1275: 0.30989535508458227\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16025: 0.024713652046242866\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16050: 0.02471466673411111\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16075: 0.024709102430829976\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16100: 0.024702349136784974\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1325: 0.3007989017964112\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16150: 0.0247124400521628\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16175: 0.0247057266518517\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16200: 0.02470803428022739\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16225: 0.024708009516608738\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1375: 0.29249488459019496\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16275: 0.02470651857449361\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16300: 0.024703564235967895\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16325: 0.02472790100528045\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16350: 0.02472691544114177\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1425: 0.28513973788897456\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average T loss at step 16400: 0.024739641642701784\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 25: 0.060582617860054595\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1425: 0.41553654823321523\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 50: 0.072313039181754\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 4025: 0.03577832966765262\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 75: 0.07628554104080346\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 100: 0.06866468498767063\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 4025: 0.027284633616984806\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 125: 0.07177163443420431\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 150: 0.07242343349219785\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1475: 0.4057905476660294\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 175: 0.07215530778491354\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 200: 0.07237859058712275\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1500: 0.4011107674706727\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 225: 0.07203840031335454\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 250: 0.07300571619354014\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1475: 0.2781087487699243\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 275: 0.07246674036814842\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 300: 0.07125493289937973\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 325: 0.06938390111381662\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1525: 0.3964298050368174\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 350: 0.0736359970801381\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 375: 0.07199260267561962\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1550: 0.39199186232722094\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 400: 0.07102874400027759\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 425: 0.07206736051740283\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average T loss at step 4075: 0.03580953810786817\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 450: 0.07138966271096757\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 475: 0.07151404958897681\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 500: 0.0712341302140485\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 4075: 0.027258924819761887\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 525: 0.07180728872081436\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 550: 0.0726666454851974\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1600: 0.3830249914334854\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 575: 0.07292779753563557\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 600: 0.07258472996853016\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1625: 0.37855318579135033\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 625: 0.07316534216116997\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 650: 0.07281737241009698\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 4100: 0.027291349665788514\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 675: 0.07245289531818815\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 700: 0.07204837885591717\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1650: 0.3742930768819695\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 750: 0.07142997763562016\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1675: 0.3699919650297779\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 50: 0.08325879540294409\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1700: 0.36587871408856965\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 875: 0.07156033901830752\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1550: 0.26833065657936517\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1725: 0.362015210494831\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 100: 0.07986746165901422\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1750: 0.35862084794576676\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1000: 0.06964574257107961\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1575: 0.26541657383923256\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 150: 0.07079065564864626\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1775: 0.35533128649940793\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 175: 0.07022763799210745\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1800: 0.3515947705926374\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1175: 0.06952818943365313\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1825: 0.34819825798324117\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1225: 0.06965943056648141\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1600: 0.2624683518652455\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1275: 0.06996444781046068\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1850: 0.3444702800811344\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1325: 0.06965882031780773\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1875: 0.3411841069916884\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 275: 0.06725346943532878\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1625: 0.25968156842916057\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1900: 0.3377224888554529\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1425: 0.06971374949015291\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1500: 0.0699246096604135\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1925: 0.33438436320511167\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1550: 0.0701415303025527\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1950: 0.33112554480799306\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1600: 0.07063916118398311\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1650: 0.2569705880447432\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 375: 0.06751003882288933\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 1975: 0.32875931252949414\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m Average E loss at step 400: 0.06831003270111978\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2000: 0.3261066050201189\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1775: 0.0702293504634667\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1675: 0.2542524246262637\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2025: 0.32291763546589164\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1825: 0.07010635425852554\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1875: 0.0707286097255225\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2050: 0.3191160729877287\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1925: 0.07017091174283581\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2075: 0.3163786480712693\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 1975: 0.06985682941118698\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=19352)\u001b[0m epoch:  2\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1700: 0.2516747081118143\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 2025: 0.06953687858345084\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2100: 0.3136181370412842\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2125: 0.3109104387834668\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m epoch:  2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=8128)\u001b[0m Average E loss at step 2050: 0.06919031881044226\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1725: 0.249005593004488\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2175: 0.30542591595015994\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1750: 0.24655276512274785\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2250: 0.2970219689567263\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2275: 0.29448777594475667\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2325: 0.28997763193828324\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2375: 0.28553141381870956\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 125: 0.01815882450621575\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2450: 0.2785932881392215\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2500: 0.2743579735694919\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 175: 0.017054845578303293\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2575: 0.2683489617566099\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2600: 0.2663947360138767\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1925: 0.23131369901869397\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2675: 0.2607492918589492\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2700: 0.2590070310742508\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1950: 0.22940957892518968\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2750: 0.2552024925199168\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 275: 0.016731127845758403\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2825: 0.24963106444608255\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2875: 0.24643298910072317\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2900: 0.24476364908796125\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 2925: 0.2433364801453108\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2050: 0.22205633709684197\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3000: 0.2390050504942968\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3025: 0.23772987667036297\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3050: 0.23657713773206152\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2100: 0.21816451239200044\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3125: 0.23258958171613514\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 425: 0.018464271584936583\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3200: 0.22862692871707624\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3250: 0.22599813529266977\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3275: 0.2247652157310233\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3325: 0.2223200669928797\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3375: 0.2198480180754085\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3400: 0.21885811581902945\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3450: 0.2163735305674482\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3500: 0.21421849514816754\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 575: 0.017727921039129242\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3575: 0.2112377954506921\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3625: 0.20918477667932367\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 625: 0.017152340122312308\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3700: 0.20614090033658983\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3750: 0.20429016754444068\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 675: 0.01671911952635532\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3825: 0.20157031368485537\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3875: 0.19987358840942504\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3900: 0.19924375494945054\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3950: 0.19769671936982158\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 3975: 0.19697305081286262\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2475: 0.19450587553640056\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 4050: 0.1947132321091277\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2500: 0.1933145123174414\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 4125: 0.1922234012384645\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 825: 0.015612128550331391\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 4200: 0.1898633118305311\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 4225: 0.18901965773014143\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2575: 0.18909077205717817\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 4300: 0.18677380667621712\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2600: 0.18762354269350628\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 4375: 0.18456365788822462\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 925: 0.0149786507295023\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 4450: 0.18226594221028708\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 4475: 0.18158957577460086\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 4500: 0.18091189451353987\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2675: 0.18344900135695064\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 4550: 0.179372742831423\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2700: 0.18213257515815914\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 4625: 0.1775433624824635\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1025: 0.014729550354106056\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 4700: 0.17553964106087386\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 4725: 0.17476129761515136\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2775: 0.1780946806124789\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 4800: 0.17264737623444185\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2800: 0.17686597259295272\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 4875: 0.17105611889968173\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1125: 0.014215473453708303\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 4950: 0.16986750705709747\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5000: 0.16901879090495642\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1175: 0.013954568464455116\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5075: 0.16769456703418847\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5125: 0.16685469226731078\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1225: 0.013788370002484025\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5200: 0.16537510332921207\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5250: 0.16444197411687733\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5275: 0.16386907494043207\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5325: 0.16286038616139817\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5375: 0.16197950200633643\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1325: 0.013699267797747557\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5450: 0.16081088836254645\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5500: 0.16010157888521312\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1375: 0.013639107529603114\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5575: 0.15908065008086197\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5625: 0.15827790035774103\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5650: 0.15799068142753714\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5700: 0.1574240809985694\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5750: 0.15667249764829025\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5775: 0.15628733356101276\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5800: 0.15600711253858052\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1500: 0.013676969602849568\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5875: 0.15493665739859236\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1525: 0.013738837419068211\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3250: 0.1574935677336314\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 5975: 0.1535258483895389\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3275: 0.15665150562731392\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 6050: 0.1524978384166698\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 6075: 0.15214667562415454\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 6100: 0.15193489259159476\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3325: 0.15497326950735196\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 6175: 0.1510327589044042\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1650: 0.013715380560209848\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 6250: 0.15009447038588114\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1675: 0.013690290895747647\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3400: 0.15238631404843866\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 6350: 0.14882947264489438\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 6375: 0.14857347180399422\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3425: 0.15167808340892985\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 6425: 0.14801837201600765\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1750: 0.013603562701385402\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 6500: 0.14720113666483667\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1775: 0.013548551702627611\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3500: 0.1495064556722729\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 6600: 0.14616641837762223\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 6625: 0.14596235870286153\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3525: 0.1487779377542555\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 6675: 0.14545371189648076\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 6700: 0.14511224057647892\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 6750: 0.14460486205637074\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1875: 0.01367026848118597\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3600: 0.14663821445043948\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 6850: 0.1435386141940669\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3625: 0.14596437154443742\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 6925: 0.14265532237269987\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 1950: 0.013833389710896135\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 7000: 0.141832850315184\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 7025: 0.14156904737830486\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3700: 0.14398374038581685\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 7100: 0.14062455111744807\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3725: 0.14332648483708177\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 7175: 0.1398537936801649\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 7200: 0.13955654561585915\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 7250: 0.13912161072839757\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2075: 0.013883148377167948\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3800: 0.14139980782408537\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 7350: 0.13830637168779109\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3825: 0.1406999134654301\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 7425: 0.1376403566436867\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 7450: 0.1374320015736118\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 7500: 0.13711317936972095\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2175: 0.01382571719283768\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3900: 0.1389437755106244\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 7600: 0.13635363881071833\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3925: 0.13844085388784266\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 7675: 0.1358574551371524\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2250: 0.01363612646423927\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 7750: 0.135333646146881\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2275: 0.013579805688862178\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 4000: 0.13694566467075492\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 7850: 0.13459647462326607\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 4025: 0.13638575485956664\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 7925: 0.13397504687962955\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 7950: 0.1337936513253148\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 7975: 0.1335914212775338\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 4075: 0.1353200532114019\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8050: 0.1329566474286015\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2400: 0.01349151412501063\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 25: 0.06328030217438936\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 50: 0.06627088801935316\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8100: 0.1325875900230402\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8125: 0.13251889756630678\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 75: 0.06506733788798252\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8150: 0.1323771960668743\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2425: 0.013548155231311313\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 100: 0.06701010929420591\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8175: 0.1322266762792731\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 125: 0.06692869009077548\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8200: 0.13207503754977085\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 150: 0.06850339964032173\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2450: 0.01358237203819397\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 175: 0.06841913726712977\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8250: 0.13164166275302486\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 200: 0.067540621929802\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 225: 0.06654835159579912\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8275: 0.13142223751586735\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2475: 0.013574288124506681\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 250: 0.06506416735937819\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 275: 0.06463081144600769\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8300: 0.1311935844164726\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8325: 0.1309256103293699\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 300: 0.06448240808482902\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 325: 0.06468577316998003\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8350: 0.13072153410635204\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 350: 0.06505002461872729\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8400: 0.13026117530746326\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 375: 0.06523284902144223\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 400: 0.06586542501085205\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8425: 0.13009456575309958\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 425: 0.06621833240257248\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2550: 0.013609552145598903\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 450: 0.06573609109735117\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 475: 0.065751770593373\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8475: 0.12969949260129454\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8500: 0.12952853994696703\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 500: 0.06491862848377787\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8525: 0.12947410386242109\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m epoch:  1\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2575: 0.013536409066336736\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8575: 0.12917808595744068\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8625: 0.12879043206519217\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2625: 0.013387216604952257\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8700: 0.12804044425728378\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8750: 0.12755364535302588\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2675: 0.013262435393840418\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8825: 0.12687544663531689\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8850: 0.12667660416284113\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8900: 0.12621217196281434\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8950: 0.12578926126869375\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 8975: 0.1255349601694178\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9025: 0.1250957718350308\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9050: 0.12487041544356996\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2800: 0.012953980550147598\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9125: 0.12421065340566206\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9150: 0.12404736957617761\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2825: 0.01291700622407087\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9200: 0.12369332630334691\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9250: 0.12337496363990184\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2875: 0.012819359894869748\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9325: 0.12276596491704457\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9350: 0.12256819996253193\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9400: 0.1222374484313533\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9425: 0.12203240999028048\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2950: 0.012657315717017085\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9500: 0.12149324069353507\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9525: 0.12129252566352212\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 2975: 0.012601996290904936\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9575: 0.12094511865836376\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9625: 0.12060525134890099\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3025: 0.0125027133451514\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9700: 0.12018408410361255\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9750: 0.11987335812529394\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3075: 0.012415236019289472\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9825: 0.11934477635106823\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9875: 0.1189575896428845\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9900: 0.11875526697418096\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3125: 0.012315884650864172\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9950: 0.1184695216351927\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 9975: 0.11834744247930874\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10025: 0.11806408983310092\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10075: 0.11766213819141055\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10125: 0.11727584944889927\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10150: 0.11709379051861026\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3225: 0.012093889716330847\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10200: 0.11678348366278121\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10250: 0.11629097407039045\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3275: 0.011999516767427016\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10325: 0.11571938632186626\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10350: 0.1155133521469599\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10400: 0.1150981269516118\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10450: 0.11465660907753963\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10475: 0.11443216311569195\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10500: 0.11426196417207497\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10525: 0.11409425825610062\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10575: 0.11374173280490721\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10625: 0.11332384179771111\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3425: 0.011834921225118127\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10700: 0.11274385821215686\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10725: 0.11254106533548378\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10750: 0.1123813079047913\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10775: 0.11219865264195875\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10825: 0.11182487471803876\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10875: 0.11143722236982209\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3525: 0.011772270018046191\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10950: 0.11086082985501212\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 10975: 0.1106497752555534\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11025: 0.11026222786302531\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11075: 0.10989919234442201\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11125: 0.1095513627478968\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3625: 0.011708723853785817\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11200: 0.1090093450313595\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11225: 0.10885149725951111\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11275: 0.10854767931987916\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11325: 0.10820616614640172\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11375: 0.10788809421557964\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3725: 0.01162878600189239\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11450: 0.10738130877602883\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11500: 0.10706070382863349\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3775: 0.011577886277873343\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11575: 0.10655558829340499\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11625: 0.10627236347592821\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3825: 0.011521878149621021\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11700: 0.1057982480315117\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11725: 0.1056303860885024\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11750: 0.1054957838840151\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11775: 0.10532112195130933\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11800: 0.10517373234363715\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3900: 0.011523887982761013\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11875: 0.1047158640814406\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11900: 0.10456810244886858\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3925: 0.01154280108720202\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 11950: 0.10429832696726289\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12000: 0.10400250279498262\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 3975: 0.011573331114931586\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12075: 0.10356056275807045\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12100: 0.10346464269705649\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12125: 0.10333195508482323\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12150: 0.10321220500087734\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12175: 0.10310146919779266\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12200: 0.1029790924306299\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 4050: 0.011573617192056032\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12250: 0.10273917946079628\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12275: 0.10258804691510058\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average T loss at step 4075: 0.011565195704937806\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12325: 0.10230596020456822\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 25: 0.08881085896864534\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12350: 0.10215765165904446\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 50: 0.09565663208253682\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12400: 0.10187598050487366\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 75: 0.092432502489537\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 100: 0.09165472148451954\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12425: 0.10173381069297383\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 125: 0.09268458421900869\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1575: 0.029691207001543797\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 150: 0.09494338157586753\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 175: 0.0939982746035925\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12475: 0.10142759684470594\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12500: 0.10129360645945999\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 200: 0.09409918067627586\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12525: 0.10113359587376138\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1600: 0.029655581203751353\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 225: 0.09391014420220421\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12550: 0.10100448859846604\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 250: 0.09183890921529382\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12575: 0.10084853914178094\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 275: 0.09163636315190657\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1625: 0.02966578947342574\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 300: 0.09134841656855618\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12625: 0.1005251165616296\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 325: 0.09191255610651122\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1650: 0.029723010328298462\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 350: 0.09141928103552865\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 375: 0.09151008343137801\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 400: 0.09265514790487941\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12675: 0.10026419310575545\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12700: 0.10013933779141389\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 425: 0.09260720492669326\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 450: 0.09235139562903592\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12725: 0.09997059156289229\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 475: 0.09274196408836073\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12775: 0.09969351536509924\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m Average E loss at step 500: 0.09179959428694565\n",
      "\u001b[36m(train_model pid=18772)\u001b[0m epoch:  3\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12825: 0.09941025418548806\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12875: 0.09911213033934219\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 12925: 0.09879726244442366\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1775: 0.02947197431631216\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 13025: 0.09819999686791338\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 13075: 0.09798824170103294\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 13125: 0.09780945438064324\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 13175: 0.09757486223242515\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1875: 0.029734074885879334\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 13275: 0.09714054365424007\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 13325: 0.09691810292295114\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 13375: 0.09665879504478463\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 13425: 0.09644343620715633\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 1975: 0.030149724334345752\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 13525: 0.09592924017707909\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 13575: 0.09568267554837334\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 13625: 0.09547809407404419\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 13675: 0.0952877784238735\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2075: 0.030303754369904994\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 13775: 0.09494182801203077\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 13825: 0.09474856997405602\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 13875: 0.0945878855433894\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 13925: 0.09438733071905162\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2175: 0.03016958509495578\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 14025: 0.09399055395937739\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 14075: 0.0937942938766439\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 14125: 0.09359410873079566\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 14175: 0.09338176802167426\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2275: 0.029809962271055167\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 14275: 0.09298100184105176\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 14325: 0.09282482725234412\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 14375: 0.09262530392621258\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 14425: 0.09243446653220456\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2375: 0.029665087628018994\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 14525: 0.09209882209539431\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 14575: 0.09191064909207662\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 14625: 0.09170954397468306\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 14675: 0.09157586552348965\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2475: 0.029610596044457076\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 14775: 0.09120399686091168\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 14825: 0.09102118622094356\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 14875: 0.09086170019353877\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 14925: 0.09065595679469987\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2575: 0.02942343919948241\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 15025: 0.09031573228566801\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 15075: 0.09013542134671261\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 15125: 0.08993785237397302\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 15175: 0.08976643312988647\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2675: 0.028835964759786736\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 15275: 0.08937843183748105\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 15325: 0.08919496628659022\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 15375: 0.08901980592313458\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 15425: 0.08883658032068859\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2775: 0.028267029969662488\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 15525: 0.08856487087241033\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 15575: 0.08840959070625799\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 15625: 0.08826389245405049\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 15675: 0.08814072852265431\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2875: 0.027757322634413394\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 15775: 0.08787835220525501\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 15825: 0.08774392440637219\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 15875: 0.08763377316459815\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 15925: 0.08751445896598237\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 2975: 0.02726454007793942\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 16025: 0.08726346203732063\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 16075: 0.08712514110378274\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 16125: 0.08697386179211036\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 16175: 0.0868290386898328\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3075: 0.02692880117632089\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 16275: 0.08653681414590801\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 16325: 0.08642856272825669\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average T loss at step 16375: 0.08632003795349338\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 25: 0.04454673373082187\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3150: 0.02657384542894775\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 50: 0.05859673143553664\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 75: 0.06416525919426931\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 100: 0.05933924452205247\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 125: 0.05995665956969606\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 150: 0.06049531847379209\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 175: 0.060406393314520496\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3175: 0.026455683806882693\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 200: 0.06111302873137902\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 225: 0.06162496789788646\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 250: 0.06248792589220102\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 275: 0.06177799505538794\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 300: 0.060915316336492345\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 325: 0.05963466907320813\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 350: 0.06314281232960639\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 375: 0.06191655106794011\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3200: 0.026340715769206328\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 400: 0.06080219096167639\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 425: 0.06179550602770178\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 450: 0.0614138511321087\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 475: 0.06167510199993175\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 500: 0.06141676231847668\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 525: 0.06211353991182044\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 550: 0.06324904362587734\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3225: 0.026213603051557104\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 575: 0.06368922710156767\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 600: 0.06363522261013714\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 625: 0.0643306213529897\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 650: 0.06395980039687577\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 675: 0.06395824791251302\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 700: 0.06374275555975535\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 725: 0.06326766672158807\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 750: 0.06319655868098684\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3250: 0.026076308098626145\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 775: 0.06296417440345976\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 800: 0.06264213550752175\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 825: 0.06436787075772317\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 850: 0.0636044644006795\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 875: 0.06310663709355452\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 900: 0.06231802943998547\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 925: 0.06229296218879156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 950: 0.06178595611854689\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3275: 0.026027134925998813\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 975: 0.06131116287846966\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1000: 0.06107075118301145\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1025: 0.06040668117074103\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1050: 0.06004540131281828\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1075: 0.06036604486145945\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1100: 0.06074113377851476\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1125: 0.06037683802439521\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1150: 0.06041645998702101\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3300: 0.025982460259414933\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1175: 0.06050227888562578\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1200: 0.060519534492050296\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1225: 0.060614296972029365\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1250: 0.06077669777944684\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1275: 0.06093542833531768\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1300: 0.060797763771865555\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1325: 0.06061191278182515\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1350: 0.06031954076802962\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3325: 0.025910942698413108\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1375: 0.06118839973319237\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1400: 0.0609007673652325\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1425: 0.060621301044067655\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1450: 0.06093605709081046\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1475: 0.06079377090394146\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1500: 0.06086769804879926\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1525: 0.06087110282449132\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1550: 0.06113019047057605\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3350: 0.025871524973881063\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1575: 0.061521476155284624\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1600: 0.06176431648208108\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1625: 0.06173895928579329\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1650: 0.06204108203112617\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1675: 0.061935246551219104\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1700: 0.061953529983425945\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1725: 0.06187151971097236\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1750: 0.06171734279022868\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3375: 0.02576327905000653\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1775: 0.06172326196728251\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1800: 0.061663170722527465\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1825: 0.06150785107107766\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1850: 0.062339183517926014\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1875: 0.0619988707451111\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1900: 0.06182473969130737\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1925: 0.06145524119398505\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1950: 0.061436497591303825\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3400: 0.02566840076713347\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 1975: 0.061130192205880896\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 2000: 0.060990022258713\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 2025: 0.06087987897152665\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m Average E loss at step 2050: 0.060563474811905306\n",
      "\u001b[36m(train_model pid=13424)\u001b[0m epoch:  1\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3425: 0.02562444265317053\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3450: 0.02562938542932397\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3475: 0.025599979637027384\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3500: 0.025535315145443226\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3525: 0.025479547373313913\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3550: 0.02541518263535617\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3575: 0.02542051760183885\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3600: 0.025362192391253806\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3625: 0.02533278002108624\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3650: 0.025272406323446912\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3675: 0.025230938044957164\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3700: 0.02518055404552551\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3725: 0.0251258223034143\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3750: 0.025083210901081718\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3775: 0.02502037961027101\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3800: 0.02497404266581151\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3825: 0.02490014452052385\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3850: 0.024850114425678298\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3875: 0.02485298383431584\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3900: 0.02484152765756918\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3925: 0.02488310796680644\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3950: 0.02487553342555346\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 3975: 0.02490840207167781\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 4000: 0.024930427615912777\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 4025: 0.02490677178615013\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 4050: 0.024895301111215684\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 4075: 0.024869642499537905\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average T loss at step 4100: 0.024904068745378115\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 25: 0.07034043001011014\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 50: 0.074283168585971\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 75: 0.07321448155368368\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 100: 0.07440165876876563\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 125: 0.0749100420884788\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 150: 0.07692606278695166\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 175: 0.07676728026941419\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 200: 0.07612131153000519\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 225: 0.07537637961407502\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 250: 0.07356626093387604\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 275: 0.07337065744129094\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 300: 0.0730400168367972\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 325: 0.07351535408829267\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 350: 0.07351967330489839\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 375: 0.07373127182821433\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 400: 0.07458024420309811\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 425: 0.07482795743819545\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 450: 0.07446838966467315\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 475: 0.07448942589622579\n",
      "\u001b[36m(train_model pid=9460)\u001b[0m Average E loss at step 500: 0.07362034407502506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 16:50:17,890\tINFO tune.py:1047 -- Total run time: 14019.83 seconds (14019.79 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=9460)\u001b[0m epoch:  2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def ret_dataloaders(training_dataset, evaluation_dataset, batch_size, shuffle=False):\n",
    "    train_loader = DataLoader(training_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=shuffle) #False for reproducibility for now\n",
    "    \n",
    "    eval_loader = DataLoader(evaluation_dataset,\n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=shuffle) #False for reproducibility for now\n",
    "    \n",
    "    return train_loader, eval_loader\n",
    "\n",
    "\n",
    "def train_epoch(train_loader, scheduler, optimizer, model, hyperparameter_search, i):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    t_lossi = []\n",
    "    for j,batch in enumerate(train_loader):\n",
    "        \n",
    "        _ids, at, lab = batch \n",
    "        out = model(input_ids=_ids.to(model.device), \n",
    "                    attention_mask=at.to(model.device), \n",
    "                    labels=lab.to(model.device)) #   logits = [256, 40, 31] B,T,C, loss is NLL\n",
    "        train_loss += out.loss.item()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(),\n",
    "                                       max_norm=1) #gradient clipping - safety net\n",
    "\n",
    "        optimizer.zero_grad() # Zero gradients between each update\n",
    "        out.loss.backward()   # Calculate gradients\n",
    "        optimizer.step()      # Step\n",
    "\n",
    "        if scheduler:         # Update learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "        if j % 25 == 0 and j > 0:\n",
    "            print(\"Average T loss at step {}: {}\".format(j, train_loss / j ))\n",
    "            if hyperparameter:\n",
    "                train.report({\"step\": j,\n",
    "                              \"epoch\": i,\n",
    "                              \"training_loss\": train_loss / j, \n",
    "                              \"loss\": None}) ##loss needs to be 0 here otherwise error\n",
    "            \n",
    "            t_lossi.append(train_loss / j)\n",
    "            \n",
    "    \n",
    "    return model, t_lossi\n",
    "    \n",
    "\n",
    "\n",
    "def eval_epoch(eval_loader, model, hyperparameter_search, i):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    e_lossi = []\n",
    "    with torch.no_grad():\n",
    "        for j,batch in enumerate(eval_loader):\n",
    "            \n",
    "            _ids, at, lab = batch\n",
    "            out = model(input_ids=_ids.to(model.device), \n",
    "                        attention_mask=at.to(model.device), \n",
    "                        labels=lab.to(model.device))\n",
    "            eval_loss += out.loss.item()\n",
    "\n",
    "            if j % 25 == 0 and j > 0:\n",
    "                print(\"Average E loss at step {}: {}\".format(j, eval_loss / j ))\n",
    "                if hyperparameter:\n",
    "                    train.report({\"step\": j, \n",
    "                                  \"epoch\": i, \n",
    "                                  \"evaluation_loss\": eval_loss / j, \n",
    "                                  \"loss\": None }) ##loss needs to be 0 here otherwise error\n",
    "                \n",
    "                e_lossi.append(eval_loss / j )\n",
    "    #report average evaluation loss to tune\n",
    "    avg_eval_loss = eval_loss / len(eval_loader)\n",
    "    if hyperparameter:\n",
    "        train.report({\"loss\" : avg_eval_loss})\n",
    "    return e_lossi\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(params, model, training_dataset, evaluation_dataset, hyperparameter_search=True):\n",
    "    train_loader, eval_loader = ret_dataloaders(training_dataset, evaluation_dataset, params[\"batch_size\"])\n",
    "    optimizer = AdamW(params = model.parameters(), lr=params[\"lr\"])\n",
    "    \n",
    "    total_steps = (len(training_dataset) // params[\"batch_size\"]) * params[\"epochs\"]\n",
    "    warmup_steps = int(total_steps * 0.1) #standard 10th\n",
    "    scheduler = get_scheduler(\"linear\", \n",
    "                              optimizer=optimizer, \n",
    "                              num_warmup_steps= warmup_steps, ##set warmup steps to 0.1 * total num steps \n",
    "                              num_training_steps=total_steps)\n",
    "    \n",
    "    \n",
    "    for i in range(1, params[\"epochs\"] + 1):\n",
    "        model, t_loss = train_epoch(train_loader, scheduler, optimizer, model, hyperparameter, i)\n",
    "        e_loss = eval_epoch(eval_loader, model, hyperparameter, i)\n",
    "        print(\"epoch: \", i)\n",
    "        \n",
    "    \n",
    "    if not hyperparameter:\n",
    "        return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dbd851",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "# Random search of some hyperparameters using ray.tune\n",
    "\n",
    "analysis = tune.run(\n",
    "            tune.with_parameters(\n",
    "                train_model,                               # Name of function - normally trainer.train\n",
    "                model=model,                               # Model input\n",
    "                training_dataset=training_dataset,         # Syntax\n",
    "                evaluation_dataset=evaluation_dataset,      # Syntax\n",
    "                hyperparameter_search=True,\n",
    "            ),\n",
    "            resources_per_trial={\n",
    "                \"gpu\": gpu_per_trial,                                 # Only gpu, Can set to decimals for simultaneous trials\n",
    "                \"cpu\": cpu_per_trial\n",
    "            },\n",
    "            config={\n",
    "                \"lr\": tune.choice([2e-5, 3e-5, 5e-5]),     # Per original bert paper\n",
    "                \"batch_size\": tune.choice([16, 32, 64]),   # Batch sizes to test\n",
    "                \"epochs\": tune.choice([1,2,3]),            # Epoch choices\n",
    "                    }, \n",
    "            num_samples=10,                                # Total number of trials\n",
    "            metric=\"loss\",                                 # The metric to optimize - loss on evaluation set\n",
    "            mode=\"min\"                                     # As low as possible\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe643efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'lr': 5e-05, 'batch_size': 32, 'epochs': 1}\n"
     ]
    }
   ],
   "source": [
    "best_config = analysis.get_best_config(metric=\"loss\", mode=\"min\")\n",
    "print(\"Best hyperparameters found were: \", best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8446388",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = analysis.trial_dataframes\n",
    "\n",
    "plt.figure(figsize=(15, 9))\n",
    "for trial_id, df in dfs.items():\n",
    "    plt.plot(df['step'], df['evaluation_loss'], label=f'Trial {trial_id}')\n",
    "    plt.plot(df['step'], df['training_loss'], label=f'Trial {trial_id}')\n",
    "    \n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Evaluation Loss')\n",
    "plt.title('Evaluation Loss Over Steps')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410ed7f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dfs \u001b[38;5;241m=\u001b[39m \u001b[43manalysis\u001b[49m\u001b[38;5;241m.\u001b[39mtrial_dataframes\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m9\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trial_id, df \u001b[38;5;129;01min\u001b[39;00m dfs\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'analysis' is not defined"
     ]
    }
   ],
   "source": [
    "model = train_model(best_config, model, training_dataset, evaluation_dataset, hyperparameter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb836c2",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a3cf89e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def test(test_loader):\n",
    "    model.eval()\n",
    "    predictions, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            _ids, at, lab = batch\n",
    "            out = model(input_ids=_ids.to(model.device), attention_mask=at.to(model.device)) \n",
    "\n",
    "            #print(out.logits.shape)  #dim 2 are predictions for each input\n",
    "            preds = torch.argmax(out.logits, dim=2)\n",
    "\n",
    "\n",
    "            # Only compare indicies where there is not padding or special tokens\n",
    "            mask = (lab != -100) #& (lab != 0)\n",
    "\n",
    "            # Also test with mask = lab != 0, since the class is so large\n",
    "\n",
    "            # Retrieve correct indices\n",
    "            preds = preds[mask]\n",
    "            labs = lab[mask]\n",
    "\n",
    "            # Tolist for comparison\n",
    "            predictions.extend(preds.tolist())\n",
    "            labels.extend(labs.tolist())\n",
    "    \n",
    "    return predictions, labels\n",
    "\n",
    "\n",
    "#accuracy is high, because most are just 0\n",
    "#sum([1 for x,y in zip(predictions, labels) if x==y]) / len(labels)\n",
    "\n",
    "test_loader = DataLoader(testing_dataset, batch_size=64, shuffle=False)\n",
    "preds, labels = test(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0f08a9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20498089101601538"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for x,y in zip(preds, labels) if x==y]) / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7312ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cm = metrics.confusion_matrix([itos[x] for x in labels], [itos[x] for x in predictions])\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(30, 30))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041cde54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## precison - how many of the tags were correctly predicted\n",
    "## recall - h\n",
    "## f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envname2",
   "language": "python",
   "name": "envname2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
